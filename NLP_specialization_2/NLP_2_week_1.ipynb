{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re # regular expression library; for tokenization of words\n",
    "from collections import Counter # collections library; counter: dict subclass for counting hashable objects\n",
    "import matplotlib.pyplot as plt # for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red pink pink blue blue yellow ORANGE BLUE BLUE PINK\n",
      "string length :  52\n"
     ]
    }
   ],
   "source": [
    "# the tiny corpus of text ! \n",
    "text = 'red pink pink blue blue yellow ORANGE BLUE BLUE PINK' # ðŸŒˆ\n",
    "print(text)\n",
    "print('string length : ',len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red pink pink blue blue yellow orange blue blue pink\n",
      "string length :  52\n"
     ]
    }
   ],
   "source": [
    "# convert all letters to lower case\n",
    "text_lowercase = text.lower()\n",
    "print(text_lowercase)\n",
    "print('string length : ',len(text_lowercase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'pink', 'pink', 'blue', 'blue', 'yellow', 'orange', 'blue', 'blue', 'pink']\n",
      "count :  10\n"
     ]
    }
   ],
   "source": [
    "# some regex to tokenize the string to words and return them in a list\n",
    "words = re.findall(r'\\w+', text_lowercase)\n",
    "print(words)\n",
    "print('count : ',len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blue', 'orange', 'red', 'yellow', 'pink'}\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "# create vocab\n",
    "vocab = set(words)\n",
    "print(vocab)\n",
    "print('count : ',len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red': 1, 'pink': 3, 'blue': 4, 'yellow': 1, 'orange': 1}\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "# create vocab including word count\n",
    "counts_a = dict()\n",
    "for w in words:\n",
    "    counts_a[w] = counts_a.get(w,0)+1\n",
    "print(counts_a)\n",
    "print('count : ',len(counts_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'blue': 4, 'pink': 3, 'red': 1, 'yellow': 1, 'orange': 1})\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "# create vocab including word count using collections.Counter\n",
    "counts_b = dict()\n",
    "counts_b = Counter(words)\n",
    "print(counts_b)\n",
    "print('count : ',len(counts_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# barchart of sorted word counts\n",
    "d = {'blue': counts_b['blue'], 'pink': counts_b['pink'], 'red': counts_b['red'], 'yellow': counts_b['yellow'], 'orange': counts_b['orange']}\n",
    "plt.bar(range(len(d)), list(d.values()), align='center', color=d.keys())\n",
    "_ = plt.xticks(range(len(d)), list(d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_b :  Counter({'blue': 4, 'pink': 3, 'red': 1, 'yellow': 1, 'orange': 1})\n",
      "count :  5\n"
     ]
    }
   ],
   "source": [
    "print('counts_b : ', counts_b)\n",
    "print('count : ', len(counts_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "word = 'dearz' # ðŸ¦Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'dearz']\n",
      "['d', 'earz']\n",
      "['de', 'arz']\n",
      "['dea', 'rz']\n",
      "['dear', 'z']\n",
      "['dearz', '']\n"
     ]
    }
   ],
   "source": [
    "# Find all the ways you can split a word into 2 parts !\n",
    "# splits with a loop\n",
    "splits_a = []\n",
    "for i in range(len(word)+1):\n",
    "    splits_a.append([word[:i],word[i:]])\n",
    "\n",
    "for i in splits_a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 'dearz')\n",
      "('d', 'earz')\n",
      "('de', 'arz')\n",
      "('dea', 'rz')\n",
      "('dear', 'z')\n",
      "('dearz', '')\n"
     ]
    }
   ],
   "source": [
    "# same splits, done using a list comprehension\n",
    "splits_b = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "\n",
    "for i in splits_b:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word :  dearz\n",
      "earz  <-- delete  d\n",
      "darz  <-- delete  e\n",
      "derz  <-- delete  a\n",
      "deaz  <-- delete  r\n",
      "dear  <-- delete  z\n"
     ]
    }
   ],
   "source": [
    "# Delete a letter from each string in the splits list\n",
    "# deletes with a loop\n",
    "splits = splits_a\n",
    "deletes = []\n",
    "\n",
    "print('word : ', word)\n",
    "for L,R in splits:\n",
    "    if R:\n",
    "        print(L + R[1:], ' <-- delete ', R[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word :  dearz\n",
      "first item from the splits list :  ['', 'dearz']\n",
      "L :  \n",
      "R :  dearz\n",
      "*** now implicit delete by excluding the leading letter ***\n",
      "L + R[1:] :  earz  <-- delete  d\n"
     ]
    }
   ],
   "source": [
    "# breaking it down\n",
    "print('word : ', word)\n",
    "one_split = splits[0]\n",
    "print('first item from the splits list : ', one_split)\n",
    "L = one_split[0]\n",
    "R = one_split[1]\n",
    "print('L : ', L)\n",
    "print('R : ', R)\n",
    "print('*** now implicit delete by excluding the leading letter ***')\n",
    "print('L + R[1:] : ',L + R[1:], ' <-- delete ', R[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earz', 'darz', 'derz', 'deaz', 'dear']\n",
      "*** which is the same as ***\n",
      "earz\n",
      "darz\n",
      "derz\n",
      "deaz\n",
      "dear\n"
     ]
    }
   ],
   "source": [
    "# deletes with a list comprehension\n",
    "splits = splits_a\n",
    "deletes = [L + R[1:] for L, R in splits if R]\n",
    "\n",
    "print(deletes)\n",
    "print('*** which is the same as ***')\n",
    "for i in deletes:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab :  ['dean', 'deer', 'dear', 'fries', 'and', 'coke']\n",
      "edits :  ['earz', 'darz', 'derz', 'deaz', 'dear']\n",
      "candidate words :  {'dear'}\n"
     ]
    }
   ],
   "source": [
    "vocab = ['dean','deer','dear','fries','and','coke']\n",
    "edits = list(deletes)\n",
    "\n",
    "print('vocab : ', vocab)\n",
    "print('edits : ', edits)\n",
    "\n",
    "candidates=[]\n",
    "\n",
    "### START CODE HERE ###\n",
    "candidates = set(vocab).intersection(set(edits))\n",
    "### END CODE HERE ###\n",
    "\n",
    "print('candidate words : ', candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: process_data\n",
    "def process_data(file_name):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        A file_name which is found in your current directory. You just have to read it in. \n",
    "    Output: \n",
    "        words: a list containing all the words in the corpus (text file you read) in lower case. \n",
    "    \"\"\"\n",
    "    words = [] # return this variable correctly\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    with open(file_name) as f:\n",
    "        file = f.read()\n",
    "    file = file.lower()\n",
    "    words = re.findall(r'\\w+', file)   \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT MODIFY THIS CELL\n",
    "word_l = process_data('shakespeare.txt')\n",
    "vocab = set(word_l)  # this will be your new vocabulary\n",
    "print(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\n",
    "print(f\"There are {len(vocab)} unique words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: get_count\n",
    "def get_count(word_l):\n",
    "    '''\n",
    "    Input:\n",
    "        word_l: a set of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    '''\n",
    "    \n",
    "    word_count_dict = {}  # fill this with word counts\n",
    "    ### START CODE HERE \n",
    "    word_count_dict = Counter(word_l)       \n",
    "    ### END CODE HERE ### \n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT MODIFY THIS CELL\n",
    "word_count_dict = get_count(word_l)\n",
    "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "print(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: get_probs\n",
    "def get_probs(word_count_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    Output:\n",
    "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    '''\n",
    "    probs = {}  # return this variable correctly\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    total_l = sum(word_count_dict.values())\n",
    "    for k,v in word_count_dict.items():\n",
    "        probs[k] = v/total_l\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT MODIFY THIS CELL\n",
    "probs = get_probs(word_count_dict)\n",
    "print(f\"Length of probs is {len(probs)}\")\n",
    "print(f\"P('thee') is {probs['thee']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: deletes\n",
    "def delete_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the string/word for which you will generate all possible words \n",
    "                in the vocabulary which have 1 missing character\n",
    "    Output:\n",
    "        delete_l: a list of all possible strings obtained by deleting 1 character from word\n",
    "    '''\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    split_l_alternate = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    delete_l = [L + R[1:] for L, R in split_l if R]\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "\n",
    "    return delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_word_l = delete_letter(word=\"cans\",\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test # 2\n",
    "print(f\"Number of outputs of delete_letter('at') is {len(delete_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: switches\n",
    "def switch_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: input string\n",
    "     Output:\n",
    "        switches: a list of all possible strings with one adjacent charater switched\n",
    "    ''' \n",
    "    \n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    for L,R in split_l:\n",
    "        if len(R) > 2:\n",
    "            switch =  L + R[1] + R[0] + R[2:]\n",
    "            switch_l.append(switch)\n",
    "        elif len(R) == 2:\n",
    "            switch = L + R[1] + R[0]\n",
    "            switch_l.append(switch)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "\n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_word_l = switch_letter(word=\"eta\",\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test # 2\n",
    "print(f\"Number of outputs of switch_letter('at') is {len(switch_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: replaces\n",
    "def replace_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
    "    ''' \n",
    "    \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    replace_set = set()\n",
    "    for p in letters:\n",
    "        replace = [L + p + R[1:] for L, R in split_l if R]\n",
    "        for r in replace:    \n",
    "            replace_set.add(r)\n",
    "    replace_set.discard(word)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # turn the set back into a list and sort it, for easier viewing\n",
    "    replace_l = sorted(list(replace_set))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "    \n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_l = replace_letter(word='can',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test # 2\n",
    "print(f\"Number of outputs of replace_letter('at') is {len(replace_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: inserts\n",
    "def insert_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for c in range(len(word)+1):\n",
    "        split_l.append((word[0:c],word[c:]))\n",
    "    insert_l = [ a + l + b for a,b in split_l for l in letters]\n",
    "    ## END CODE HERE ###\n",
    "\n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_l = insert_letter('at', True)\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test # 2\n",
    "print(f\"Number of outputs of insert_letter('at') is {len(insert_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: edit_one_letter\n",
    "def edit_one_letter(word, allow_switches = True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: the string/word for which we will generate all possible wordsthat are one edit away.\n",
    "    Output:\n",
    "        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_one_set = set()\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    delete_l = delete_letter(word)\n",
    "    replace_l = replace_letter(word)\n",
    "    insert_l = insert_letter(word)\n",
    "    switch_l = switch_letter(word)\n",
    "    total_l = delete_l + replace_l + insert_l + switch_l\n",
    "    edit_one_set = (set(total_l))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return edit_one_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_word = \"at\"\n",
    "tmp_edit_one_set = edit_one_letter(tmp_word)\n",
    "# turn this into a list to sort it, in order to view it\n",
    "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\n",
    "\n",
    "print(f\"input word {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\n",
    "print(f\"The type of the returned object should be a set {type(tmp_edit_one_set)}\")\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: edit_two_letters\n",
    "def edit_two_letters(word, allow_switches = True):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        edit_two_set: a set of strings with all possible two edits\n",
    "    '''\n",
    "    \n",
    "    edit_two_set = set()\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    edit_two_l = []\n",
    "    edit_one = edit_one_letter(word)\n",
    "    for w in edit_one:\n",
    "        if w:\n",
    "            edit_two = edit_one_letter(w)\n",
    "            edit_two_set.update(edit_two)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return edit_two_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_edit_two_set = edit_two_letters(\"a\")\n",
    "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\n",
    "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\n",
    "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\n",
    "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\n",
    "print(f\"The data type of the returned object should be a set {type(tmp_edit_two_set)}\")\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggest spelling suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will use your edit_two_letters function to get a set of all the possible 2 edits on your word. You will then use those strings to get the most probable word you meant to type aka your typing suggestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of logical operation on lists or sets\n",
    "print( [] and [\"a\",\"b\"] )\n",
    "print( [] or [\"a\",\"b\"] )\n",
    "#example of Short circuit behavior\n",
    "val1 =  [\"Most\",\"Likely\"] or [\"Less\",\"so\"] or [\"least\",\"of\",\"all\"]  # selects first, does not evalute remainder\n",
    "print(val1)\n",
    "val2 =  [] or [] or [\"least\",\"of\",\"all\"] # continues evaluation until there is a non-empty list\n",
    "print(val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: get_corrections\n",
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    if n == 1:\n",
    "        one_edit = edit_one_letter(word)\n",
    "        all_edits = one_edit\n",
    "    elif n == 2:\n",
    "        one_edit = edit_one_letter(word)\n",
    "        two_edits = edit_two_letters(word)\n",
    "        all_edits = two_edits.intersection(one_edit)\n",
    "    \n",
    "    suggestions = all_edits.intersection(set(probs.keys()))   \n",
    "    n_best = [(s, probs[s]) for s in suggestions]\n",
    "    n_best.sort(key=lambda tup: tup[1], reverse = True)  # sorts in place\n",
    "    suggestions = set([i[0] for i in n_best])\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation - feel free to try other words in my word\n",
    "my_word = 'dys' \n",
    "tmp_corrections = get_corrections(my_word, probs, vocab, 2, verbose=True) # keep verbose=True\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "\n",
    "# CODE REVIEW COMMENT: using \"tmp_corrections\" insteads of \"cors\". \"cors\" is not defined\n",
    "print(f\"data type of corrections {type(tmp_corrections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: min_edit_distance\n",
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    # use deletion and insert cost as  1\n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    \n",
    "    # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1): # Replace None with the proper range\n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "        \n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(1,n+1): # Replace None with the proper range\n",
    "        D[0,col] = D[0,col-1] + ins_cost\n",
    "        \n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1): \n",
    "        \n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            # Intialize r_cost to the 'replace' cost that is passed into this function\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column, \n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            # Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)\n",
    "            D[row,col] = min([D[row-1,col]+del_cost, D[row,col-1]+ins_cost, D[row-1,col-1]+r_cost])\n",
    "          \n",
    "    # Set the minimum edit distance with the cost found at row m, column n\n",
    "    med = D[m,n]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT MODIFY THIS CELL\n",
    "# testing your implementation \n",
    "source =  'play'\n",
    "target = 'stay'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT MODIFY THIS CELL\n",
    "# testing your implementation \n",
    "source =  'eer'\n",
    "target = 'near'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list(source)\n",
    "idx.insert(0, '#')\n",
    "cols = list(target)\n",
    "cols.insert(0, '#')\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"eer\"\n",
    "targets = edit_one_letter(source,allow_switches = False)  #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits != 1: print(source, t, min_edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"eer\"\n",
    "targets = edit_two_letters(source,allow_switches = False) #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits != 2 and min_edits != 1: print(source, t, min_edits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
