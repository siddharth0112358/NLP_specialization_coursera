{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trax==1.3.1\n",
      "  Downloading trax-1.3.1-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[K     |████████████████████████████████| 347 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jax\n",
      "  Downloading jax-0.2.7.tar.gz (520 kB)\n",
      "\u001b[K     |████████████████████████████████| 520 kB 27.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting funcsigs\n",
      "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: tensorflow-text in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from trax==1.3.1) (2.3.0)\n",
      "Collecting tensor2tensor\n",
      "  Downloading tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 14.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.1.0-py3-none-any.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from trax==1.3.1) (1.18.5)\n",
      "Requirement already satisfied: absl-py in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from trax==1.3.1) (0.10.0)\n",
      "Collecting gin-config\n",
      "  Downloading gin_config-0.4.0-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from trax==1.3.1) (1.4.1)\n",
      "Collecting jaxlib\n",
      "  Downloading jaxlib-0.1.57-cp38-none-macosx_10_9_x86_64.whl (42.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 42.7 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gym\n",
      "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting t5\n",
      "  Downloading t5-0.8.0-py3-none-any.whl (207 kB)\n",
      "\u001b[K     |████████████████████████████████| 207 kB 14.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from trax==1.3.1) (1.11.0)\n",
      "Requirement already satisfied: opt_einsum in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from jax->trax==1.3.1) (3.3.0)\n",
      "Requirement already satisfied: tensorflow<2.4,>=2.3.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-text->trax==1.3.1) (2.3.0)\n",
      "Collecting tensorflow-gan\n",
      "  Downloading tensorflow_gan-2.0.0-py2.py3-none-any.whl (365 kB)\n",
      "\u001b[K     |████████████████████████████████| 365 kB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: flask in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensor2tensor->trax==1.3.1) (1.1.2)\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.11.2-cp38-cp38-macosx_10_13_x86_64.whl (613 kB)\n",
      "\u001b[K     |████████████████████████████████| 613 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 19.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-probability==0.7.0\n",
      "  Downloading tensorflow_probability-0.7.0-py2.py3-none-any.whl (981 kB)\n",
      "\u001b[K     |████████████████████████████████| 981 kB 21.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gevent in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensor2tensor->trax==1.3.1) (20.6.2)\n",
      "Requirement already satisfied: tf-slim in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensor2tensor->trax==1.3.1) (1.1.0)\n",
      "Collecting dopamine-rl\n",
      "  Downloading dopamine_rl-3.1.8-py3-none-any.whl (117 kB)\n",
      "\u001b[K     |████████████████████████████████| 117 kB 49.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mesh-tensorflow\n",
      "  Downloading mesh_tensorflow-0.1.18-py3-none-any.whl (361 kB)\n",
      "\u001b[K     |████████████████████████████████| 361 kB 27.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sympy in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensor2tensor->trax==1.3.1) (1.6.1)\n",
      "Requirement already satisfied: requests in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensor2tensor->trax==1.3.1) (2.24.0)\n",
      "Requirement already satisfied: h5py in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensor2tensor->trax==1.3.1) (2.10.0)\n",
      "Requirement already satisfied: Pillow in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensor2tensor->trax==1.3.1) (7.2.0)\n",
      "Collecting pypng\n",
      "  Downloading pypng-0.0.20.tar.gz (649 kB)\n",
      "\u001b[K     |████████████████████████████████| 649 kB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-python-client\n",
      "  Downloading google_api_python_client-1.12.8-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 123 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gunicorn\n",
      "  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: opencv-python in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensor2tensor->trax==1.3.1) (4.4.0.42)\n",
      "Collecting kfac\n",
      "  Downloading kfac-0.2.3-py2.py3-none-any.whl (191 kB)\n",
      "\u001b[K     |████████████████████████████████| 191 kB 23.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensor2tensor->trax==1.3.1) (4.47.0)\n",
      "Requirement already satisfied: future in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensor2tensor->trax==1.3.1) (0.18.2)\n",
      "Collecting bz2file\n",
      "  Downloading bz2file-0.98.tar.gz (11 kB)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-3.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: termcolor in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax==1.3.1) (19.3.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.26.0-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax==1.3.1) (0.3.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-datasets->trax==1.3.1) (3.13.0)\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 18.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from gym->trax==1.3.1) (1.5.0)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-1.4.14-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from t5->trax==1.3.1) (1.0.5)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: transformers>=2.7.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from t5->trax==1.3.1) (3.3.0)\n",
      "Requirement already satisfied: babel in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from t5->trax==1.3.1) (2.8.0)\n",
      "Collecting tfds-nightly\n",
      "  Downloading tfds_nightly-4.1.0.dev202012180106-py3-none-any.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from t5->trax==1.3.1) (1.6.0)\n",
      "Requirement already satisfied: nltk in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from t5->trax==1.3.1) (3.2.5)\n",
      "Requirement already satisfied: sentencepiece in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from t5->trax==1.3.1) (0.1.91)\n",
      "Requirement already satisfied: scikit-learn in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from t5->trax==1.3.1) (0.23.1)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.1.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (2.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (0.3.3)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.6.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.31.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (2.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (0.34.2)\n",
      "Requirement already satisfied: tensorflow-hub>=0.2 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-gan->tensor2tensor->trax==1.3.1) (0.9.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from flask->tensor2tensor->trax==1.3.1) (2.11.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from flask->tensor2tensor->trax==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: click>=5.1 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from flask->tensor2tensor->trax==1.3.1) (7.1.2)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from flask->tensor2tensor->trax==1.3.1) (1.0.1)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.10.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from oauth2client->tensor2tensor->trax==1.3.1) (0.4.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from oauth2client->tensor2tensor->trax==1.3.1) (4.5)\n",
      "Collecting httplib2>=0.9.1\n",
      "  Downloading httplib2-0.18.1-py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 7.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.0.5 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from oauth2client->tensor2tensor->trax==1.3.1) (0.2.8)\n",
      "Requirement already satisfied: decorator in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-probability==0.7.0->tensor2tensor->trax==1.3.1) (4.2.1)\n",
      "Requirement already satisfied: setuptools in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from gevent->tensor2tensor->trax==1.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: greenlet>=0.4.16; platform_python_implementation == \"CPython\" in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from gevent->tensor2tensor->trax==1.3.1) (0.4.16)\n",
      "Requirement already satisfied: zope.event in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from gevent->tensor2tensor->trax==1.3.1) (4.4)\n",
      "Requirement already satisfied: zope.interface in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from gevent->tensor2tensor->trax==1.3.1) (4.7.1)\n",
      "Collecting pygame>=1.9.2\n",
      "  Downloading pygame-2.0.0-cp38-cp38-macosx_10_9_intel.whl (6.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.9 MB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flax>=0.2.0\n",
      "  Downloading flax-0.3.0-py3-none-any.whl (154 kB)\n",
      "\u001b[K     |████████████████████████████████| 154 kB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from sympy->tensor2tensor->trax==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from requests->tensor2tensor->trax==1.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from requests->tensor2tensor->trax==1.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from requests->tensor2tensor->trax==1.3.1) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from requests->tensor2tensor->trax==1.3.1) (2020.6.20)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.0.4-py2.py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: google-auth>=1.16.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from google-api-python-client->tensor2tensor->trax==1.3.1) (1.22.1)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from google-api-python-client->tensor2tensor->trax==1.3.1) (1.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow-datasets->trax==1.3.1) (1.52.0)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from pandas->t5->trax==1.3.1) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from pandas->t5->trax==1.3.1) (2020.1)\n",
      "Requirement already satisfied: packaging in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from transformers>=2.7.0->t5->trax==1.3.1) (20.4)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from transformers>=2.7.0->t5->trax==1.3.1) (0.8.1rc2)\n",
      "Requirement already satisfied: filelock in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from transformers>=2.7.0->t5->trax==1.3.1) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from transformers>=2.7.0->t5->trax==1.3.1) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from transformers>=2.7.0->t5->trax==1.3.1) (2020.6.8)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->t5->trax==1.3.1) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->t5->trax==1.3.1) (0.16.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from Jinja2>=2.10.1->flask->tensor2tensor->trax==1.3.1) (1.1.1)\n",
      "Requirement already satisfied: matplotlib in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor->trax==1.3.1) (3.2.2)\n",
      "Requirement already satisfied: msgpack in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor->trax==1.3.1) (1.0.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from google-auth>=1.16.0->google-api-python-client->tensor2tensor->trax==1.3.1) (4.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from packaging->transformers>=2.7.0->t5->trax==1.3.1) (2.4.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->flax>=0.2.0->dopamine-rl->tensor2tensor->trax==1.3.1) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->flax>=0.2.0->dopamine-rl->tensor2tensor->trax==1.3.1) (1.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (3.1.0)\n",
      "Building wheels for collected packages: jax, gym, pypng, bz2file, promise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for jax (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.2.7-py3-none-any.whl size=606843 sha256=30a20021263b5eb3b75babecf19f3057d485578446a8c270e192fd94d7420d7e\n",
      "  Stored in directory: /Users/sdeshpande/Library/Caches/pip/wheels/a5/3a/c2/f1de3b2efbfa6ca8229969acd6e3a028479b72d5d318f6ec78\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654654 sha256=8c0aa1e8e997db6c2ecce23a2ce546e46a0fd8b69f6aae0b096f6135540c7bd6\n",
      "  Stored in directory: /Users/sdeshpande/Library/Caches/pip/wheels/84/40/e7/14efb9870cfc92ac236d78cb721dce614ddec9666c8a5e0a35\n",
      "  Building wheel for pypng (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypng: filename=pypng-0.0.20-py3-none-any.whl size=67162 sha256=c6489b271124bd29789b8c5b1f727ae0987aae164a8b94bd112ff470b08ae4b1\n",
      "  Stored in directory: /Users/sdeshpande/Library/Caches/pip/wheels/3a/ad/91/4f6a5e9f3db79c28e71e7b59099dce8a75618a34ff415d44b1\n",
      "  Building wheel for bz2file (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bz2file: filename=bz2file-0.98-py3-none-any.whl size=6882 sha256=bdc3f612762edffea9981aca348f3f7eb39bcb4cd80ebed34adfa99fdab479e1\n",
      "  Stored in directory: /Users/sdeshpande/Library/Caches/pip/wheels/19/88/ce/c9430af242507ffff602cf86c5ff6a1ae5205cba5aaf21f6cc\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=15150bd1f041b5d045b2be22f55df75f768fc7224fe1df00115318fc63663ba6\n",
      "  Stored in directory: /Users/sdeshpande/Library/Caches/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built jax gym pypng bz2file promise\n",
      "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensor2tensor 1.15.7 has requirement six>=1.12.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: rouge-score 0.0.4 has requirement six>=1.14.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: t5 0.8.0 has requirement six>=1.14, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: jax, funcsigs, tensorflow-probability, tensorflow-gan, typeguard, tensorflow-addons, httplib2, oauth2client, importlib-resources, promise, tensorflow-metadata, tensorflow-datasets, pygame, flax, gin-config, pyglet, gym, flatbuffers, jaxlib, dopamine-rl, mesh-tensorflow, pypng, uritemplate, google-auth-httplib2, google-api-python-client, gunicorn, kfac, bz2file, tensor2tensor, portalocker, sacrebleu, rouge-score, tfds-nightly, t5, trax\n",
      "Successfully installed bz2file-0.98 dopamine-rl-3.1.8 flatbuffers-1.12 flax-0.3.0 funcsigs-1.0.2 gin-config-0.4.0 google-api-python-client-1.12.8 google-auth-httplib2-0.0.4 gunicorn-20.0.4 gym-0.17.3 httplib2-0.18.1 importlib-resources-3.3.0 jax-0.2.7 jaxlib-0.1.57 kfac-0.2.3 mesh-tensorflow-0.1.18 oauth2client-4.1.3 portalocker-2.0.0 promise-2.3 pygame-2.0.0 pyglet-1.5.0 pypng-0.0.20 rouge-score-0.0.4 sacrebleu-1.4.14 t5-0.8.0 tensor2tensor-1.15.7 tensorflow-addons-0.11.2 tensorflow-datasets-4.1.0 tensorflow-gan-2.0.0 tensorflow-metadata-0.26.0 tensorflow-probability-0.7.0 tfds-nightly-4.1.0.dev202012180106 trax-1.3.1 typeguard-2.10.0 uritemplate-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install trax==1.3.1 #Use this version for this notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # regular ol' numpy\n",
    "\n",
    "from trax import layers as tl  # core building block\n",
    "from trax import shapes  # data signatures: dimensionality and type\n",
    "from trax import fastmath  # uses jax, offers numpy on steroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trax                               1.3.1\r\n"
     ]
    }
   ],
   "source": [
    "# Trax version 1.3.1 or better \n",
    "!pip list | grep trax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relu Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Relu\n",
      "expected inputs : 1\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : [-2 -1  0  1  2] \n",
      "\n",
      "-- Outputs --\n",
      "y : [0 0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Layers\n",
    "# Create a relu trax layer\n",
    "relu = tl.Relu()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", relu.name)\n",
    "print(\"expected inputs :\", relu.n_in)\n",
    "print(\"promised outputs :\", relu.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = relu(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Concatenate\n",
      "expected inputs : 2\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x1 : [-10 -20 -30]\n",
      "x2 : [1. 2. 3.] \n",
      "\n",
      "-- Outputs --\n",
      "y : [-10. -20. -30.   1.   2.   3.]\n"
     ]
    }
   ],
   "source": [
    "# Create a concatenate trax layer\n",
    "concat = tl.Concatenate()\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", concat.name)\n",
    "print(\"expected inputs :\", concat.n_in)\n",
    "print(\"promised outputs :\", concat.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x1 :\", x1)\n",
    "print(\"x2 :\", x2, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = concat([x1, x2])\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers are Configurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Concatenate\n",
      "expected inputs : 3\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x1 : [-10 -20 -30]\n",
      "x2 : [1. 2. 3.]\n",
      "x3 : [0.99 1.98 2.97] \n",
      "\n",
      "-- Outputs --\n",
      "y : [-10.   -20.   -30.     1.     2.     3.     0.99   1.98   2.97]\n"
     ]
    }
   ],
   "source": [
    "# Configure a concatenate layer\n",
    "concat_3 = tl.Concatenate(n_items=3)  # configure the layer's expected inputs\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", concat_3.name)\n",
    "print(\"expected inputs :\", concat_3.n_in)\n",
    "print(\"promised outputs :\", concat_3.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x1 = np.array([-10, -20, -30])\n",
    "x2 = x1 / -10\n",
    "x3 = x2 * 0.99\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x1 :\", x1)\n",
    "print(\"x2 :\", x2)\n",
    "print(\"x3 :\", x3, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = concat_3([x1, x2, x3])\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(tl.Concatenate) #Uncomment this to see the function docstring with explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers can have Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment any of them to see information regarding the function\n",
    "# help(tl.LayerNorm)\n",
    "# help(shapes.signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal shape: (4,) Data Type: <class 'tuple'>\n",
      "Shapes Trax: ShapeDtype{shape:(4,), dtype:float64} Data Type: <class 'trax.shapes.ShapeDtype'>\n",
      "-- Properties --\n",
      "name : LayerNorm\n",
      "expected inputs : 1\n",
      "promised outputs : 1\n",
      "weights : [1. 1. 1. 1.]\n",
      "biases : [0. 0. 0. 0.] \n",
      "\n",
      "-- Inputs --\n",
      "x : [0. 1. 2. 3.]\n",
      "-- Outputs --\n",
      "y : [-1.3416404  -0.44721344  0.44721344  1.3416404 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages/jax/_src/lax/lax.py:6341: UserWarning: Explicitly requested dtype float64 requested in ones is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n",
      "/Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages/jax/_src/lax/lax.py:6341: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
     ]
    }
   ],
   "source": [
    "# Layer initialization\n",
    "norm = tl.LayerNorm()\n",
    "# You first must know what the input data will look like\n",
    "x = np.array([0, 1, 2, 3], dtype=\"float\")\n",
    "\n",
    "# Use the input data signature to get shape and type for initializing weights and biases\n",
    "norm.init(shapes.signature(x)) # We need to convert the input datatype from usual tuple to trax ShapeDtype\n",
    "\n",
    "print(\"Normal shape:\",x.shape, \"Data Type:\",type(x.shape))\n",
    "print(\"Shapes Trax:\",shapes.signature(x),\"Data Type:\",type(shapes.signature(x)))\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", norm.name)\n",
    "print(\"expected inputs :\", norm.n_in)\n",
    "print(\"promised outputs :\", norm.n_out)\n",
    "# Weights and biases\n",
    "print(\"weights :\", norm.weights[0])\n",
    "print(\"biases :\", norm.weights[1], \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x)\n",
    "\n",
    "# Outputs\n",
    "y = norm(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tl.Fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : TimesTwo\n",
      "expected inputs : 1\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : [1 2 3] \n",
      "\n",
      "-- Outputs --\n",
      "y : [2 4 6]\n"
     ]
    }
   ],
   "source": [
    "# Define a custom layer\n",
    "# In this example you will create a layer to calculate the input times 2\n",
    "\n",
    "def TimesTwo():\n",
    "    layer_name = \"TimesTwo\" #don't forget to give your custom layer a name to identify\n",
    "\n",
    "    # Custom function for the custom layer\n",
    "    def func(x):\n",
    "        return x * 2\n",
    "\n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "\n",
    "# Test it\n",
    "times_two = TimesTwo()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", times_two.name)\n",
    "print(\"expected inputs :\", times_two.n_in)\n",
    "print(\"promised outputs :\", times_two.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([1, 2, 3])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = times_two(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tl.Serial)\n",
    "# help(tl.Parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Serial Model --\n",
      "Serial[\n",
      "  LayerNorm\n",
      "  Relu\n",
      "  TimesTwo\n",
      "] \n",
      "\n",
      "-- Properties --\n",
      "name : Serial\n",
      "sublayers : [LayerNorm, Relu, TimesTwo]\n",
      "expected inputs : 1\n",
      "promised outputs : 1\n",
      "weights & biases: [(DeviceArray([1, 1, 1, 1, 1], dtype=int32), DeviceArray([0, 0, 0, 0, 0], dtype=int32)), (), ()] \n",
      "\n",
      "-- Inputs --\n",
      "x : [-2 -1  0  1  2] \n",
      "\n",
      "-- Outputs --\n",
      "y : [0.        0.        0.        1.4142132 2.8284264]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages/jax/_src/lax/lax.py:6341: UserWarning: Explicitly requested dtype int64 requested in ones is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n",
      "/Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages/jax/_src/lax/lax.py:6341: UserWarning: Explicitly requested dtype int64 requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  warnings.warn(msg.format(dtype, fun_name , truncated_dtype))\n"
     ]
    }
   ],
   "source": [
    "# Serial combinator\n",
    "serial = tl.Serial(\n",
    "    tl.LayerNorm(),         # normalize input\n",
    "    tl.Relu(),              # convert negative values to zero\n",
    "    times_two,              # the custom layer you created above, multiplies the input recieved from above by 2\n",
    "    \n",
    "    ### START CODE HERE\n",
    "#     tl.Dense(n_units=2),  # try adding more layers. eg uncomment these lines\n",
    "#     tl.Dense(n_units=1),  # Binary classification, maybe? uncomment at your own peril\n",
    "#     tl.LogSoftmax()       # Yes, LogSoftmax is also a layer\n",
    "    ### END CODE HERE\n",
    ")\n",
    "\n",
    "# Initialization\n",
    "x = np.array([-2, -1, 0, 1, 2]) #input\n",
    "serial.init(shapes.signature(x)) #initialising serial instance\n",
    "\n",
    "print(\"-- Serial Model --\")\n",
    "print(serial,\"\\n\")\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", serial.name)\n",
    "print(\"sublayers :\", serial.sublayers)\n",
    "print(\"expected inputs :\", serial.n_in)\n",
    "print(\"promised outputs :\", serial.n_out)\n",
    "print(\"weights & biases:\", serial.weights, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = serial(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good old numpy :  <class 'numpy.ndarray'> \n",
      "\n",
      "jax trax numpy :  <class 'jax.interpreters.xla._DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "# Numpy vs fastmath.numpy have different data types\n",
    "# Regular ol' numpy\n",
    "x_numpy = np.array([1, 2, 3])\n",
    "print(\"good old numpy : \", type(x_numpy), \"\\n\")\n",
    "\n",
    "# Fastmath and jax numpy\n",
    "x_jax = fastmath.numpy.array([1, 2, 3])\n",
    "print(\"jax trax numpy : \", type(x_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and subclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Parameters, methods and instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: #Definition of My_class\n",
    "    x = None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter x of instance_a: None\n",
      "Parameter x of instance_b: None\n"
     ]
    }
   ],
   "source": [
    "instance_a= My_Class() #To create an instance from class \"My_Class\" you have to call \"My_Class\"\n",
    "instance_b= My_Class()\n",
    "print('Parameter x of instance_a: ' + str(instance_a.x)) #To get a parameter 'x' from an instance 'a', write 'a.x'\n",
    "print('Parameter x of instance_b: ' + str(instance_b.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter x of instance_a: 5\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE (1 line) ### \n",
    "instance_a.x = 5\n",
    "### END CODE HERE ###\n",
    "print('Parameter x of instance_a: ' + str(instance_a.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 The __init__ method\n",
    "\n",
    "When you want to assign values to the parameters of your class when an instance is created, it is necessary to define a special method: __init__. The __init__ method is called when you create an instance of a class. It can have multiple arguments to initialize the paramenters of your instance. In the next cell I will define My_Class with an __init__ method that takes the instance (self) and an argument y as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n",
    "        self.x = y         # Sets parameter x to be equal to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter x of instance_c: 10\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE (1 line) ### \n",
    "instance_c = My_Class(10)\n",
    "### END CODE HERE ###\n",
    "print('Parameter x of instance_c: ' + str(instance_c.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 The __call__ method\n",
    "\n",
    "Another important method is the __call__ method. It is performed whenever you call an initialized instance of a class. It can have multiple arguments and you can define it to do whatever you want like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y): # The __init__ method takes as input the instance to be initialized and a variable y\n",
    "        self.x = y         # Sets parameter x to be equal to y\n",
    "    def __call__(self, z): # __call__ method with self and z as arguments\n",
    "        self.x += z        # Adds z to parameter x when called \n",
    "        print(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_d = My_Class(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "instance_d(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n",
    "        ### START CODE HERE (2 lines) ### \n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "        ### END CODE HERE ###\n",
    "    def __call__(self):       #When called, adds the values of parameters x_1 and x_2, prints and returns the result \n",
    "        ### START CODE HERE (1 line) ### \n",
    "        result = self.x_1 + self.x_2 \n",
    "        ### END CODE HERE ### \n",
    "        print(\"Addition of {} and {} is {}\".format(self.x_1,self.x_2,result))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition of 10 and 15 is 25\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "instance_e = My_Class(10,15)\n",
    "def test_class_definition():\n",
    "    \n",
    "    assert instance_e.x_1 == 10, \"Check the value assigned to x_1\"\n",
    "    assert instance_e.x_2 == 15, \"Check the value assigned to x_2\"\n",
    "    assert instance_e() == 25, \"Check the __call__ method\"\n",
    "    \n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "    \n",
    "test_class_definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Custom methods\n",
    "In addition to the __init__ and __call__ methods, your classes can have custom-built methods to do whatever you want when called. To define a custom method, you have to indicate its input arguments, the instructions that you want it to perform and the values to return (if any). In the next cell, My_Class is defined with my_method that multiplies the values of x_1 and x_2, sums that product with an input w, and returns the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Class: \n",
    "    def __init__(self, y, z): #Initialization of x_1 and x_2 with arguments y and z\n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "    def __call__(self):       #Performs an operation with x_1 and x_2, and returns the result\n",
    "        a = self.x_1 - 2*self.x_2 \n",
    "        return a\n",
    "    def my_method(self, w):   #Multiplies x_1 and x_2, adds argument w and returns the result\n",
    "        result = self.x_1*self.x_2 + w\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of my_method: 26\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE (1 line) ### \n",
    "instance_f = My_Class(1,10)\n",
    "### END CODE HERE ### \n",
    "print(\"Output of my_method:\",instance_f.my_method(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden-cell\n",
    "class My_Class: \n",
    "    def __init__(self, y, z):      #Initialization of x_1 and x_2 with arguments y and z\n",
    "        self.x_1 = y\n",
    "        self.x_2 = z\n",
    "    def __call__(self):            #Performs an operation with x_1 and x_2, and returns the result\n",
    "        a = self.x_1 - 2*self.x_2 \n",
    "        return a\n",
    "    def my_method(self, w):        #Multiplies x_1 and x_2, adds argument w and returns the result\n",
    "        b = self.x_1*self.x_2 + w\n",
    "        return b\n",
    "    def new_method(self, v):       #Calls My_method with argument v\n",
    "        result = self.my_method(v)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of my_method: 26\n",
      "Output of new_method: 26\n"
     ]
    }
   ],
   "source": [
    "instance_g = My_Class(1,10)\n",
    "print(\"Output of my_method:\",instance_g.my_method(16))\n",
    "print(\"Output of new_method:\",instance_g.new_method(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Subclasses and Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sub_c(My_Class):           #Subclass sub_c from My_class\n",
    "    def additional_method(self): #Prints the value of parameter x_1\n",
    "        print(self.x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter x_1 of instance_sub_a: 1\n",
      "Parameter x_2 of instance_sub_a: 10\n",
      "Output of my_method of instance_sub_a: 26\n"
     ]
    }
   ],
   "source": [
    "instance_sub_a = sub_c(1,10)\n",
    "print('Parameter x_1 of instance_sub_a: ' + str(instance_sub_a.x_1))\n",
    "print('Parameter x_2 of instance_sub_a: ' + str(instance_sub_a.x_2))\n",
    "print(\"Output of my_method of instance_sub_a:\",instance_sub_a.my_method(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sub_c(My_Class):           #Subclass sub_c from My_class\n",
    "    def my_method(self):         #Multiplies x_1 and x_2 and returns the result\n",
    "        ### START CODE HERE (1 line) ###\n",
    "        b = self.x_1*self.x_2 \n",
    "        ### END CODE HERE ###\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of overridden my_method of test: 30\n"
     ]
    }
   ],
   "source": [
    "test = sub_c(3,10)\n",
    "assert test.my_method() == 30, \"The method my_method should return the product between x_1 and x_2\"\n",
    "\n",
    "print(\"Output of overridden my_method of test:\",test.my_method()) #notice we didn't pass any parameter to call my_method\n",
    "#print(\"Output of overridden my_method of test:\",test.my_method(16)) #try to see what happens if you call it with 1 argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My_method for an instance of sub_c returns: 10\n",
      "My_method for an instance of My_Class returns: 20\n"
     ]
    }
   ],
   "source": [
    "y,z= 1,10\n",
    "instance_sub_a = sub_c(y,z)\n",
    "instance_a = My_Class(y,z)\n",
    "print('My_method for an instance of sub_c returns: ' + str(instance_sub_a.my_method()))\n",
    "print('My_method for an instance of My_Class returns: ' + str(instance_a.my_method(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 1, 2, 3, 4, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "\n",
    "# Example of traversing a list of indexes to create a circular list\n",
    "a = [1, 2, 3, 4]\n",
    "b = [0] * 10\n",
    "\n",
    "a_size = len(a)\n",
    "b_size = len(b)\n",
    "lines_index = [*range(a_size)] # is equivalent to [i for i in range(0,a_size)], the difference being the advantage of using * to pass values of range iterator to list directly\n",
    "index = 0                      # similar to index in data_generator below\n",
    "for i in range(b_size):        # `b` is longer than `a` forcing a wrap\n",
    "    # We wrap by resetting index to 0 so the sequences circle back at the end to point to the first index\n",
    "    if index >= a_size:\n",
    "        index = 0\n",
    "    \n",
    "    b[i] = a[lines_index[index]]     #  `indexes_list[index]` point to a index of a. Store the result in b\n",
    "    index += 1\n",
    "    \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original order of index: [0, 1, 2, 3]\n",
      "Shuffled order of index: [0, 1, 2, 3]\n",
      "New value order for first batch: [1, 2, 3, 4]\n",
      "\n",
      "Shuffled Indexes for Batch No.2 :[2, 0, 1, 3]\n",
      "Values for Batch No.2 :[3, 1, 2, 4]\n",
      "\n",
      "Shuffled Indexes for Batch No.3 :[2, 0, 1, 3]\n",
      "Values for Batch No.3 :[3, 1, 2, 4]\n",
      "\n",
      "Final value of b: [1, 2, 3, 4, 3, 1, 2, 4, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "# Example of traversing a list of indexes to create a circular list\n",
    "a = [1, 2, 3, 4]\n",
    "b = []\n",
    "\n",
    "a_size = len(a)\n",
    "b_size = 10\n",
    "lines_index = [*range(a_size)]\n",
    "print(\"Original order of index:\",lines_index)\n",
    "\n",
    "# if we shuffle the index_list we can change the order of our circular list\n",
    "# without modifying the order or our original data\n",
    "random.shuffle(lines_index) # Shuffle the order\n",
    "print(\"Shuffled order of index:\",lines_index)\n",
    "\n",
    "print(\"New value order for first batch:\",[a[index] for index in lines_index])\n",
    "batch_counter = 1\n",
    "index = 0                # similar to index in data_generator below\n",
    "for i in range(b_size):  # `b` is longer than `a` forcing a wrap\n",
    "    # We wrap by resetting index to 0\n",
    "    if index >= a_size:\n",
    "        index = 0\n",
    "        batch_counter += 1\n",
    "        random.shuffle(lines_index) # Re-shuffle the order\n",
    "        print(\"\\nShuffled Indexes for Batch No.{} :{}\".format(batch_counter,lines_index))\n",
    "        print(\"Values for Batch No.{} :{}\".format(batch_counter,[a[index] for index in lines_index]))\n",
    "    \n",
    "    b.append(a[lines_index[index]])     #  `indexes_list[index]` point to a index of a. Store the result in b\n",
    "    index += 1\n",
    "print()    \n",
    "print(\"Final value of b:\",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, data_x, data_y, shuffle=True):\n",
    "\n",
    "    data_lng = len(data_x) # len(data_x) must be equal to len(data_y)\n",
    "    index_list = [*range(data_lng)] # Create a list with the ordered indexes of sample data\n",
    "    \n",
    "    # If shuffle is set to true, we traverse the list in a random way\n",
    "    if shuffle:\n",
    "        rnd.shuffle(index_list) # Inplace shuffle of the list\n",
    "    \n",
    "    index = 0 # Start with the first element\n",
    "    while True:\n",
    "        X = [0] * batch_size # We can create a list with batch_size elements. \n",
    "        Y = [0] * batch_size # We can create a list with batch_size elements. \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            # Wrap the index each time that we reach the end of the list\n",
    "            if index >= data_lng:\n",
    "                index = 0\n",
    "                # Shuffle the index_list if shuffle is true\n",
    "                if shuffle:\n",
    "                    rnd.shuffle(index_list) # re-shuffle the order\n",
    "            \n",
    "            X[i] = data_x[index_list[index]] \n",
    "            Y[i] = data_y[index_list[index]] \n",
    "            \n",
    "            index += 1\n",
    "        \n",
    "        yield((X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_data_generator():\n",
    "    x = [1, 2, 3, 4]\n",
    "    y = [xi ** 2 for xi in x]\n",
    "    \n",
    "    generator = data_generator(3, x, y, shuffle=False)\n",
    "\n",
    "    assert np.allclose(next(generator), ([1, 2, 3], [1, 4, 9])),  \"First batch does not match\"\n",
    "    assert np.allclose(next(generator), ([4, 1, 2], [16, 1, 4])), \"Second batch does not match\"\n",
    "    assert np.allclose(next(generator), ([3, 4, 1], [9, 16, 1])), \"Third batch does not match\"\n",
    "    assert np.allclose(next(generator), ([2, 3, 4], [4, 9, 16])), \"Fourth batch does not match\"\n",
    "\n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "\n",
    "test_data_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with DeepNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, twitter_samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "# Stop words are messy and not that compelling; \n",
    "# \"very\" and \"not\" are considered stop words, but they are obviously expressing sentiment\n",
    "\n",
    "# The porter stemmer lemmatizes \"was\" to \"wa\".  Seriously???\n",
    "\n",
    "# I'm not sure we want to get into stop words\n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "# Also have my doubts about stemming...\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \n",
    "    '''\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    ### START CODE HERE ###\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "            word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "    ### END CODE HERE ###\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's not reuse variables\n",
    "#all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "#all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "def load_tweets():\n",
    "    all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    all_negative_tweets = twitter_samples.strings('negative_tweets.json')  \n",
    "    return all_positive_tweets, all_negative_tweets\n",
    "    \n",
    "# Layers have weights and a foward function.\n",
    "# They create weights when layer.initialize is called and use them.\n",
    "# remove this or make it optional \n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base class for layers.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "  \n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        pass\n",
    "\n",
    "    def init(self, input_signature, random_key):\n",
    "        self.init_weights_and_state(input_signature, random_key)\n",
    "        return self.weights\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random as rnd\n",
    "\n",
    "# import relevant libraries\n",
    "import trax\n",
    "\n",
    "# set random seeds to make this notebook easier to replicate\n",
    "trax.supervised.trainer_lib.init_random_number_generators(31)\n",
    "\n",
    "# import trax.fastmath.numpy\n",
    "import trax.fastmath.numpy as np\n",
    "\n",
    "# import trax.layers\n",
    "from trax import layers as tl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array using trax.fastmath.numpy\n",
    "a = np.array(5.0)\n",
    "\n",
    "# View the returned array\n",
    "display(a)\n",
    "\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will use the trax.fastmath.numpy array\n",
    "def f(x):\n",
    "    \n",
    "    # f = x^2\n",
    "    return (x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "print(f\"f(a) for a={a} is {f(a)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly use trax.fastmath.grad to calculate the gradient (derivative) of the function\n",
    "grad_f = trax.fastmath.grad(fun=f)  # df / dx - Gradient of function f(x) with respect to x\n",
    "\n",
    "# View the type of the retuned object (it's a function)\n",
    "type(grad_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the newly created function and pass in a value for x (the DeviceArray stored in 'a')\n",
    "grad_calculation = grad_f(a)\n",
    "\n",
    "# View the result of calling the grad_f function\n",
    "display(grad_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT EDIT THIS CELL\n",
    "\n",
    "# Import functions from the utils.py file\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load positive and negative tweets\n",
    "all_positive_tweets, all_negative_tweets = load_tweets()\n",
    "\n",
    "# View the total number of positive and negative tweets.\n",
    "print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
    "print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
    "\n",
    "# Split positive set into validation and training\n",
    "val_pos   = all_positive_tweets[4000:] # generating validation set for positive tweets\n",
    "train_pos  = all_positive_tweets[:4000]# generating training set for positive tweets\n",
    "\n",
    "# Split negative set into validation and training\n",
    "val_neg   = all_negative_tweets[4000:] # generating validation set for negative tweets\n",
    "train_neg  = all_negative_tweets[:4000] # generating training set for nagative tweets\n",
    "\n",
    "# Combine training data into one set\n",
    "train_x = train_pos + train_neg \n",
    "\n",
    "# Combine validation data into one set\n",
    "val_x  = val_pos + val_neg\n",
    "\n",
    "# Set the labels for the training set (1 for positive, 0 for negative)\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "\n",
    "# Set the labels for the validation set (1 for positive, 0 for negative)\n",
    "val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "print(f\"length of train_x {len(train_x)}\")\n",
    "print(f\"length of val_x {len(val_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a function that processes the tweets\n",
    "# from utils import process_tweet\n",
    "\n",
    "# Try out function that processes tweets\n",
    "print(\"original tweet at training position 0\")\n",
    "print(train_pos[0])\n",
    "\n",
    "print(\"Tweet at training position 0 after processing:\")\n",
    "process_tweet(train_pos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "# Unit Test Note - There is no test set here only train/val\n",
    "\n",
    "# Include special tokens \n",
    "# started with pad, end of line and unk tokens\n",
    "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "# Note that we build vocab using training data\n",
    "for tweet in train_x: \n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    for word in processed_tweet:\n",
    "        if word not in Vocab: \n",
    "            Vocab[word] = len(Vocab)\n",
    "    \n",
    "print(\"Total words in vocab are\",len(Vocab))\n",
    "display(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Converting a tweet to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: tweet_to_tensor\n",
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet - A string containing a tweet\n",
    "        vocab_dict - The words dictionary\n",
    "        unk_token - The special string for unknown tokens\n",
    "        verbose - Print info durign runtime\n",
    "    Output:\n",
    "        tensor_l - A python list with\n",
    "        \n",
    "    '''  \n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # Process the tweet into a list of words\n",
    "    # where only important words are kept (stop words removed)\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"List of words from the processed tweet:\")\n",
    "        print(word_l)\n",
    "        \n",
    "    # Initialize the list that will contain the unique integer IDs of each word\n",
    "    tensor_l = []\n",
    "    \n",
    "    # Get the unique integer ID of the __UNK__ token\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
    "        \n",
    "    # for each word in the list:\n",
    "    for word in word_l:\n",
    "        \n",
    "        # Get the unique integer ID.\n",
    "        # If the word doesn't exist in the vocab dictionary,\n",
    "        # use the unique ID for __UNK__ instead.\n",
    "        word_ID = vocab_dict[word] if word in vocab_dict else unk_ID\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "        # Append the unique integer ID to the tensor list.\n",
    "        tensor_l.append(word_ID) \n",
    "    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual tweet is\\n\", val_pos[0])\n",
    "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test tweet_to_tensor\n",
    "\n",
    "def test_tweet_to_tensor():\n",
    "    test_cases = [\n",
    "        \n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\": [val_pos[1], Vocab],\n",
    "            \"expected\":[444, 2, 304, 567, 56, 9],\n",
    "            \"error\":\"The function gives bad output for val_pos[1]. Test failed\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"datatype_check\",\n",
    "            \"input\":[val_pos[1], Vocab],\n",
    "            \"expected\":type([]),\n",
    "            \"error\":\"Datatype mismatch. Need only list not np.array\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"without_unk_check\",\n",
    "            \"input\":[val_pos[1], Vocab],\n",
    "            \"expected\":6,\n",
    "            \"error\":\"Unk word check not done- Please check if you included mapping for unknown word\"\n",
    "        }\n",
    "    ]\n",
    "    count = 0\n",
    "    for test_case in test_cases:\n",
    "        \n",
    "        try:\n",
    "            if test_case['name'] == \"simple_test_check\":\n",
    "                assert test_case[\"expected\"] == tweet_to_tensor(*test_case['input'])\n",
    "                count += 1\n",
    "            if test_case['name'] == \"datatype_check\":\n",
    "                assert isinstance(tweet_to_tensor(*test_case['input']), test_case[\"expected\"])\n",
    "                count += 1\n",
    "            if test_case['name'] == \"without_unk_check\":\n",
    "                assert None not in tweet_to_tensor(*test_case['input'])\n",
    "                count += 1\n",
    "                \n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print(test_case['error'])\n",
    "    if count == 3:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(count,\" Tests passed out of 3\")\n",
    "test_tweet_to_tensor() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Creating a batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED: Data generator\n",
    "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "    '''\n",
    "    Input: \n",
    "        data_pos - Set of posstive examples\n",
    "        data_neg - Set of negative examples\n",
    "        batch_size - number of samples per batch. Must be even\n",
    "        loop - True or False\n",
    "        vocab_dict - The words dictionary\n",
    "        shuffle - Shuffle the data order\n",
    "    Yield:\n",
    "        inputs - Subset of positive and negative examples\n",
    "        targets - The corresponding labels for the subset\n",
    "        example_weights - An array specifying the importance of each example\n",
    "        \n",
    "    '''     \n",
    "### START GIVEN CODE ###\n",
    "    # make sure the batch size is an even number\n",
    "    # to allow an equal number of positive and negative samples\n",
    "    assert batch_size % 2 == 0\n",
    "    \n",
    "    # Number of positive examples in each batch is half of the batch size\n",
    "    # same with number of negative examples in each batch\n",
    "    n_to_take = batch_size // 2\n",
    "    \n",
    "    # Use pos_index to walk through the data_pos array\n",
    "    # same with neg_index and data_neg\n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    \n",
    "    len_data_pos = len(data_pos)\n",
    "    len_data_neg = len(data_neg)\n",
    "    \n",
    "    # Get and array with the data indexes\n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "    # shuffle lines if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "    stop = False\n",
    "    \n",
    "    # Loop indefinitely\n",
    "    while not stop:  \n",
    "        \n",
    "        # create a batch with positive and negative examples\n",
    "        batch = []\n",
    "        \n",
    "        # First part: Pack n_to_take positive examples\n",
    "        \n",
    "        # Start from pos_index and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "                    \n",
    "            # If the positive index goes past the positive dataset lenght,\n",
    "            if pos_index >= len_data_pos: \n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                pos_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the positive sample\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "                    \n",
    "            # get the tweet as pos_index\n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment pos_index by one\n",
    "            pos_index = pos_index + 1\n",
    "\n",
    "### END GIVEN CODE ###\n",
    "            \n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "        # Second part: Pack n_to_take negative examples\n",
    "    \n",
    "        # Using the same batch list, start from neg_index and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "            \n",
    "            # If the negative index goes past the negative dataset length,\n",
    "            if neg_index >= len_data_neg:\n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                    \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                neg_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the negative sample\n",
    "                    rnd.shuffle(neg_index_lines)\n",
    "            # get the tweet as neg_index\n",
    "            tweet = data_neg[neg_index_lines[neg_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment neg_index by one\n",
    "            neg_index = neg_index + 1\n",
    "\n",
    "### END CODE HERE ###        \n",
    "\n",
    "### START GIVEN CODE ###\n",
    "        if stop:\n",
    "            break;\n",
    "\n",
    "        # Update the start index for positive data \n",
    "        # so that it's n_to_take positions after the current pos_index\n",
    "        pos_index += n_to_take\n",
    "        \n",
    "        # Update the start index for negative data \n",
    "        # so that it's n_to_take positions after the current neg_index\n",
    "        neg_index += n_to_take\n",
    "        \n",
    "        # Get the max tweet length (the length of the longest tweet) \n",
    "        # (you will pad all shorter tweets to have this length)\n",
    "        max_len = max([len(t) for t in batch]) \n",
    "        \n",
    "        \n",
    "        # Initialize the input_l, which will \n",
    "        # store the padded versions of the tensors\n",
    "        tensor_pad_l = []\n",
    "        # Pad shorter tweets with zeros\n",
    "        for tensor in batch:\n",
    "### END GIVEN CODE ###\n",
    "\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
    "            n_pad = max_len - len(tensor)\n",
    "            \n",
    "            # Generate a list of zeros, with length n_pad\n",
    "            pad_l = [0]*n_pad\n",
    "            \n",
    "            # concatenate the tensor and the list of padded zeros\n",
    "            tensor_pad = tensor + pad_l\n",
    "            \n",
    "            # append the padded tensor to the list of padded tensors\n",
    "            tensor_pad_l.append(tensor_pad)\n",
    "\n",
    "        # convert the list of padded tensors to a numpy array\n",
    "        # and store this as the model inputs\n",
    "        inputs = np.array(tensor_pad_l)\n",
    "  \n",
    "        # Generate the list of targets for the positive examples (a list of ones)\n",
    "        # The length is the number of positive examples in the batch\n",
    "        target_pos = [1]*n_to_take\n",
    "        \n",
    "        # Generate the list of targets for the negative examples (a list of ones)\n",
    "        # The length is the number of negative examples in the batch\n",
    "        target_neg = [0]*n_to_take\n",
    "        \n",
    "        # Concatenate the positve and negative targets\n",
    "        target_l = target_pos + target_neg\n",
    "        \n",
    "        # Convert the target list into a numpy array\n",
    "        targets = np.array(target_l)\n",
    "\n",
    "        # Example weights: Treat all examples equally importantly.\n",
    "        example_weights = np.ones_like(targets)\n",
    "        \n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "### GIVEN CODE ###\n",
    "        # note we use yield and not return\n",
    "        yield inputs, targets, example_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random number generator for the shuffle procedure\n",
    "rnd.seed(30) \n",
    "\n",
    "# Create the training data generator\n",
    "def train_generator(batch_size, shuffle = False):\n",
    "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def val_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def test_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n",
    "\n",
    "# Get a batch from the train_generator and inspect.\n",
    "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
    "\n",
    "# this will print a list of 4 tensors padded with zeros\n",
    "print(f'Inputs: {inputs}')\n",
    "print(f'Targets: {targets}')\n",
    "print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the train_generator\n",
    "\n",
    "# Create a data generator for training data,\n",
    "# which produces batches of size 4 (for tensors and their respective targets)\n",
    "tmp_data_gen = train_generator(batch_size = 4)\n",
    "\n",
    "# Call the data generator to get one batch and its targets\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
    "\n",
    "print(f\"The inputs shape is {tmp_inputs.shape}\")\n",
    "print(f\"The targets shape is {tmp_targets.shape}\")\n",
    "print(f\"The example weights shape is {tmp_example_weights.shape}\")\n",
    "\n",
    "for i,t in enumerate(tmp_inputs):\n",
    "    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Defining classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 ReLU class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: Relu\n",
    "class Relu(Layer):\n",
    "    \"\"\"Relu activation function implementation\"\"\"\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Input: \n",
    "            - x (a numpy array): the input\n",
    "        Output:\n",
    "            - activation (numpy array): all positive or 0 version of x\n",
    "        '''\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        \n",
    "        activation = np.maximum(x,0)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your relu function\n",
    "x = np.array([[-2.0, -1.0, 0.0], [0.0, 1.0, 2.0]], dtype=float)\n",
    "relu_layer = Relu()\n",
    "print(\"Test data is:\")\n",
    "print(x)\n",
    "print(\"Output of Relu is:\")\n",
    "print(relu_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Dense class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the fastmath module within trax\n",
    "from trax import fastmath\n",
    "\n",
    "# use the numpy module from trax\n",
    "np = fastmath.numpy\n",
    "\n",
    "# use the fastmath.random module from trax\n",
    "random = fastmath.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how the fastmath.trax.random.normal function works\n",
    "tmp_key = random.get_prng(seed=1)\n",
    "print(\"The random seed generated by random.get_prng\")\n",
    "display(tmp_key)\n",
    "\n",
    "print(\"choose a matrix with 2 rows and 3 columns\")\n",
    "tmp_shape=(2,3)\n",
    "display(tmp_shape)\n",
    "\n",
    "# Generate a weight matrix\n",
    "# Note that you'll get an error if you try to set dtype to tf.float32, where tf is tensorflow\n",
    "# Just avoid setting the dtype and allow it to use the default data type\n",
    "tmp_weight = trax.fastmath.random.normal(key=tmp_key, shape=tmp_shape)\n",
    "\n",
    "print(\"Weight matrix generated with a normal distribution with mean 0 and stdev of 1\")\n",
    "display(tmp_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: Dense\n",
    "\n",
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    A dense (fully-connected) layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # __init__ is implemented for you\n",
    "    def __init__(self, n_units, init_stdev=0.1):\n",
    "        \n",
    "        # Set the number of units in this layer\n",
    "        self._n_units = n_units\n",
    "        self._init_stdev = init_stdev\n",
    "\n",
    "    # Please implement 'forward()'\n",
    "    def forward(self, x):\n",
    "\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "        # Matrix multiply x and the weight matrix\n",
    "        dense = np.dot(x, self.weights) \n",
    "        \n",
    "### END CODE HERE ###\n",
    "        return dense\n",
    "\n",
    "    # init_weights\n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        \n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        # The input_signature has a .shape attribute that gives the shape as a tuple\n",
    "        input_shape = input_signature.shape\n",
    "\n",
    "        # Generate the weight matrix from a normal distribution, \n",
    "        # and standard deviation of 'stdev'        \n",
    "        w = self._init_stdev * random.normal(key = random_key, shape = (input_shape[-1], self._n_units))\n",
    "        \n",
    "### END CODE HERE ###     \n",
    "        self.weights = w\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing your Dense layer \n",
    "dense_layer = Dense(n_units=10)  #sets  number of units in dense layer\n",
    "random_key = random.get_prng(seed=0)  # sets random seed\n",
    "z = np.array([[2.0, 7.0, 25.0]]) # input array \n",
    "\n",
    "dense_layer.init(z, random_key)\n",
    "print(\"Weights are\\n \",dense_layer.weights) #Returns randomly generated weights\n",
    "print(\"Foward function output is \", dense_layer(z)) # Returns multiplied values of units and weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View documentation on tl.Dense\n",
    "help(tl.Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View documentation on tl.Serial\n",
    "help(tl.Serial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View documentation for tl.Embedding\n",
    "help(tl.Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_embed = tl.Embedding(vocab_size=3, d_feature=2)\n",
    "display(tmp_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the documentation for tl.mean\n",
    "help(tl.Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretend the embedding matrix uses \n",
    "# 2 elements for embedding the meaning of a word\n",
    "# and has a vocabulary size of 3\n",
    "# So it has shape (2,3)\n",
    "tmp_embed = np.array([[1,2,3,],\n",
    "                    [4,5,6]\n",
    "                   ])\n",
    "\n",
    "# take the mean along axis 0\n",
    "print(\"The mean along axis 0 creates a vector whose length equals the vocabulary size\")\n",
    "display(np.mean(tmp_embed,axis=0))\n",
    "\n",
    "print(\"The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding\")\n",
    "display(np.mean(tmp_embed,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tl.LogSoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: classifier\n",
    "def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\n",
    "        \n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # create embedding layer\n",
    "    embed_layer = tl.Embedding(\n",
    "        vocab_size=vocab_size, # Size of the vocabulary\n",
    "        d_feature=embedding_dim)  # Embedding dimension\n",
    "    \n",
    "    # Create a mean layer, to create an \"average\" word embedding\n",
    "    mean_layer = tl.Mean(axis=1)\n",
    "    \n",
    "    # Create a dense layer, one unit for each output\n",
    "    dense_output_layer = tl.Dense(n_units = output_dim)\n",
    "\n",
    "    \n",
    "    # Create the log softmax layer (no parameters needed)\n",
    "    log_softmax_layer = tl.LogSoftmax()\n",
    "    \n",
    "    # Use tl.Serial to combine all layers\n",
    "    # and create the classifier\n",
    "    # of type trax.layers.combinators.Serial\n",
    "    model = tl.Serial(\n",
    "      embed_layer,  # embedding layer\n",
    "      mean_layer, # mean layer\n",
    "      dense_output_layer, # dense output layer \n",
    "      log_softmax_layer # log softmax layer\n",
    "    )\n",
    "### END CODE HERE ###     \n",
    "    \n",
    "    # return the model of type\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tmp_model))\n",
    "display(tmp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View documentation for trax.supervised.training.TrainTask\n",
    "help(trax.supervised.training.TrainTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View documentation for trax.supervised.training.EvalTask\n",
    "help(trax.supervised.training.EvalTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View documentation for trax.supervised.training.Loop\n",
    "help(trax.supervised.training.Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View optimizers that you could choose from\n",
    "help(trax.optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "batch_size = 16\n",
    "rnd.seed(271)\n",
    "\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n",
    "    loss_layer=tl.CrossEntropyLoss(),\n",
    "    optimizer=trax.optimizers.Adam(0.01),\n",
    "    n_steps_per_checkpoint=10,\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    ")\n",
    "\n",
    "model = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '~/model/'\n",
    "output_dir_expand = os.path.expanduser(output_dir)\n",
    "print(output_dir_expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: train_model\n",
    "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
    "    '''\n",
    "    Input: \n",
    "        classifier - the model you are building\n",
    "        train_task - Training task\n",
    "        eval_task - Evaluation task\n",
    "        n_steps - the evaluation steps\n",
    "        output_dir - folder to save your files\n",
    "    Output:\n",
    "        trainer -  trax trainer\n",
    "    '''\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    training_loop = training.Loop(\n",
    "                                classifier,  # The learning model\n",
    "                                train_task,  # The training task\n",
    "                                eval_task = eval_task, # The evaluation task\n",
    "                                output_dir = output_dir) # The output directory\n",
    "\n",
    "    training_loop.run(n_steps = n_steps)\n",
    "### END CODE HERE ###\n",
    "\n",
    "    # Return the training_loop, since it has the model.\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop = train_model(model, train_task, eval_task, 100, output_dir_expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Practice Making a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a generator object\n",
    "tmp_train_generator = train_generator(16)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_train_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "print(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \n",
    "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
    "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
    "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
    "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
    "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
    "print()\n",
    "print(\"View the prediction array\")\n",
    "tmp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn probabilites into category predictions\n",
    "tmp_is_positive = tmp_pred[:,1] > tmp_pred[:,0]\n",
    "for i, p in enumerate(tmp_is_positive):\n",
    "    print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the array of booleans\n",
    "print(\"Array of booleans\")\n",
    "display(tmp_is_positive)\n",
    "\n",
    "# convert boolean to type int32\n",
    "# True is converted to 1\n",
    "# False is converted to 0\n",
    "tmp_is_positive_int = tmp_is_positive.astype(np.int32)\n",
    "\n",
    "\n",
    "# View the array of integers\n",
    "print(\"Array of integers\")\n",
    "display(tmp_is_positive_int)\n",
    "\n",
    "# convert boolean to type float32\n",
    "tmp_is_positive_float = tmp_is_positive.astype(np.float32)\n",
    "\n",
    "# View the array of floats\n",
    "print(\"Array of floats\")\n",
    "display(tmp_is_positive_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"True == 1: {True == 1}\")\n",
    "print(f\"True == 2: {True == 2}\")\n",
    "print(f\"False == 0: {False == 0}\")\n",
    "print(f\"False == 2: {False == 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: compute_accuracy\n",
    "def compute_accuracy(preds, y, y_weights):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        preds: a tensor of shape (dim_batch, output_dim) \n",
    "        y: a tensor of shape (dim_batch, output_dim) with the true labels\n",
    "        y_weights: a n.ndarray with the a weight for each example\n",
    "    Output: \n",
    "        accuracy: a float between 0-1 \n",
    "        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n",
    "        sum_weights (np.float32): Sum of the weights\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # Create an array of booleans, \n",
    "    # True if the probability of positive sentiment is greater than\n",
    "    # the probability of negative sentiment\n",
    "    # else False\n",
    "    is_pos =  preds[:, 1] > preds[:, 0] \n",
    "\n",
    "    # convert the array of booleans into an array of np.int32\n",
    "    is_pos_int = is_pos.astype(np.int32)\n",
    "    \n",
    "    # compare the array of predictions (as int32) with the target (labels) of type int32\n",
    "    correct = is_pos_int == y \n",
    "\n",
    "    # Count the sum of the weights.\n",
    "    sum_weights = np.sum(y_weights)\n",
    "    \n",
    "    # convert the array of correct predictions (boolean) into an arrayof np.float32\n",
    "    correct_float = correct.astype(np.float32)\n",
    "    \n",
    "    # Multiply each prediction with its corresponding weight.\n",
    "    weighted_correct_float = correct_float * y_weights\n",
    "\n",
    "    # Sum up the weighted correct predictions (of type np.float32), to go in the\n",
    "    # denominator.\n",
    "    weighted_num_correct = np.sum(weighted_correct_float)\n",
    " \n",
    "    # Divide the number of weighted correct predictions by the sum of the\n",
    "    # weights.\n",
    "    accuracy = weighted_num_correct / sum_weights\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return accuracy, weighted_num_correct, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your function\n",
    "tmp_val_generator = val_generator(64)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_val_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "\n",
    "tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n",
    "\n",
    "print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n",
    "print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Testing your model on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: test_model\n",
    "def test_model(generator, model):\n",
    "    '''\n",
    "    Input: \n",
    "        generator: an iterator instance that provides batches of inputs and targets\n",
    "        model: a model instance \n",
    "    Output: \n",
    "        accuracy: float corresponding to the accuracy\n",
    "    '''\n",
    "    \n",
    "    accuracy = 0.\n",
    "    total_num_correct = 0\n",
    "    total_num_pred = 0\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    for batch in generator: \n",
    "        \n",
    "        # Retrieve the inputs from the batch\n",
    "        inputs = batch[0]\n",
    "        \n",
    "        # Retrieve the targets (actual labels) from the batch\n",
    "        targets = batch[1]\n",
    "        \n",
    "        # Retrieve the example weight.\n",
    "        example_weight = batch[2]\n",
    "\n",
    "        # Make predictions using the inputs\n",
    "        pred = model(inputs)\n",
    "        \n",
    "        # Calculate accuracy for the batch by comparing its predictions and targets\n",
    "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred, targets, example_weight) \n",
    "        \n",
    "        # Update the total number of correct predictions\n",
    "        # by adding the number of correct predictions from this batch\n",
    "        total_num_correct += batch_num_correct\n",
    "        \n",
    "        # Update the total number of predictions \n",
    "        # by adding the number of predictions made for the batch\n",
    "        total_num_pred += batch_num_pred\n",
    "\n",
    "    # Calculate accuracy over all examples\n",
    "    accuracy = total_num_correct / total_num_pred\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "# testing the accuracy of your model: this takes around 20 seconds\n",
    "model = training_loop.eval_model\n",
    "accuracy = test_model(test_generator(16), model)\n",
    "\n",
    "print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Testing with your own input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used to predict on your own sentnece\n",
    "def predict(sentence):\n",
    "    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))\n",
    "    \n",
    "    # Batch size 1, add dimension for batch, to work with the model\n",
    "    inputs = inputs[None, :]  \n",
    "    \n",
    "    # predict with the model\n",
    "    preds_probs = model(inputs)\n",
    "    \n",
    "    # Turn probabilities into categories\n",
    "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
    "    \n",
    "    sentiment = \"negative\"\n",
    "    if preds == 1:\n",
    "        sentiment = 'positive'\n",
    "\n",
    "    return preds, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a positive sentence\n",
    "sentence = \"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
    "\n",
    "print()\n",
    "# try a negative sentence\n",
    "sentence = \"I hated my day, it was the worst, I'm so sad.\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
