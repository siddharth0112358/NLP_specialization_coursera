{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformer Efficient Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trax Efficient Attention classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import trax\n",
    "from trax import layers as tl  # core building block\n",
    "import jax\n",
    "from trax import fastmath  # uses jax, offers numpy on steroids\n",
    "\n",
    "# fastmath.use_backend('tensorflow-numpy')\n",
    "import functools\n",
    "from trax.fastmath import numpy as np  # note, using fastmath subset of numpy!\n",
    "from trax.layers import (\n",
    "    tie_in,\n",
    "    length_normalized,\n",
    "    apply_broadcasted_dropout,\n",
    "    look_adjacent,\n",
    "    permute_via_gather,\n",
    "    permute_via_sort,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_self_attention(\n",
    "    dots, q_info, kv_info, causal=True, exclude_self=True, masked=False\n",
    "):\n",
    "    \"\"\"Performs masking for self-attention.\"\"\"\n",
    "    if causal:\n",
    "        mask = fastmath.lt(q_info, kv_info).astype(np.float32)\n",
    "        dots = dots - 1e9 * mask\n",
    "    if exclude_self:\n",
    "        mask = np.equal(q_info, kv_info).astype(np.float32)\n",
    "        dots = dots - 1e5 * mask\n",
    "    if masked:\n",
    "        zeros_like_kv_info = tie_in(kv_info, np.zeros_like(kv_info))\n",
    "        mask = fastmath.lt(kv_info, zeros_like_kv_info).astype(np.float32)\n",
    "        dots = dots - 1e9 * mask\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_softmax(x, passthrough=False):\n",
    "    \"\"\" softmax with passthrough\"\"\"\n",
    "    logsumexp = fastmath.logsumexp(x, axis=-1, keepdims=True)\n",
    "    o = np.exp(x - logsumexp)\n",
    "    if passthrough:\n",
    "        return (x, np.zeros_like(logsumexp))\n",
    "    else:\n",
    "        return (o, logsumexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0320586  0.08714432 0.2368828  0.6439142 ]\n",
      "[0.0320586  0.08714431 0.23688279 0.64391416]\n",
      "[4.44019]\n"
     ]
    }
   ],
   "source": [
    "## compare softmax(a) using both methods\n",
    "a = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "sma = np.exp(a) / sum(np.exp(a))\n",
    "print(sma)\n",
    "sma2, a_logsumexp = our_softmax(a)\n",
    "print(sma2)\n",
    "print(a_logsumexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_simple_attend(\n",
    "    q, k=None, v=None,\n",
    "    mask_fn=None, q_info=None, kv_info=None,\n",
    "    dropout=0.0, rng=None, verbose=False, passthrough=False\n",
    "    ):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_simple_attend(\n",
    "    q,\n",
    "    k=None,\n",
    "    v=None,\n",
    "    mask_fn=None,\n",
    "    q_info=None,\n",
    "    kv_info=None,\n",
    "    dropout=0.0,\n",
    "    rng=None,\n",
    "    verbose=False,\n",
    "    passthrough=False,\n",
    "):\n",
    "    \"\"\"Dot-product attention,  with masking, without optional chunking and/or.\n",
    "\n",
    "  Args:\n",
    "    q: Query vectors, shape [q_len, d_qk]\n",
    "    k: Key vectors, shape [kv_len, d_qk]; or None\n",
    "    v: Value vectors, shape [kv_len, d_v]\n",
    "    mask_fn: a function reference that implements masking (e.g. mask_self_attention)\n",
    "    q_info: Query-associated metadata for masking\n",
    "    kv_info: Key-associated metadata for masking\n",
    "    dropout: Dropout rate\n",
    "    rng: RNG for dropout\n",
    "\n",
    "  Returns:\n",
    "    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and\n",
    "    dots_logsumexp has shape [q_len]. The logsumexp of the attention\n",
    "    probabilities is useful for combining multiple rounds of attention (as in\n",
    "    LSH attention).\n",
    "  \"\"\"\n",
    "    assert v is not None\n",
    "    share_qk = k is None\n",
    "    if share_qk:\n",
    "        k = q\n",
    "        if kv_info is None:\n",
    "            kv_info = q_info\n",
    "\n",
    "    if share_qk:\n",
    "        k = length_normalized(k)\n",
    "    k = k / np.sqrt(k.shape[-1])\n",
    "\n",
    "    # Dot-product attention.\n",
    "    kr = np.swapaxes(k, -1, -2)  # note the fancy transpose for later..\n",
    "\n",
    "    ## Step 1  ##\n",
    "    dots = np.matmul(q, kr)\n",
    "    if verbose: print(\"Our attend dots\", dots.shape)\n",
    "\n",
    "    # Masking\n",
    "    if mask_fn is not None:\n",
    "        dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])\n",
    "\n",
    "    # Softmax.\n",
    "    #dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)  #original\n",
    "    #dots = np.exp(dots - dots_logsumexp)  #original\n",
    "    ## Step 2  ##\n",
    "    # replace with our_softmax()\n",
    "    dots, dots_logsumexp = our_softmax(dots, passthrough=passthrough)\n",
    "    if verbose: \n",
    "        print(\"Our attend dots post softmax\", dots.shape, dots_logsumexp.shape)\n",
    "\n",
    "\n",
    "    if dropout > 0.0:\n",
    "        assert rng is not None\n",
    "        # Dropout is broadcast across the bin dimension\n",
    "        dropout_shape = (dots.shape[-2], dots.shape[-1])\n",
    "        keep_prob = tie_in(dots, 1.0 - dropout)\n",
    "        keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)\n",
    "        multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)\n",
    "        dots = dots * multiplier\n",
    "\n",
    "    ## Step 3  ##\n",
    "    # The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.\n",
    "    out = np.matmul(dots, v)\n",
    "    if verbose:\n",
    "        print(\"Our attend out1\", out.shape)\n",
    "    out = np.reshape(out, (-1, out.shape[-1]))\n",
    "    if verbose:\n",
    "        print(\"Our attend out2\", out.shape)\n",
    "    dots_logsumexp = np.reshape(dots_logsumexp, (-1,))\n",
    "    return out, dots_logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our attend dots (8, 8)\n",
      "Our attend dots post softmax (8, 8) (8, 1)\n",
      "Our attend out1 (8, 4)\n",
      "Our attend out2 (8, 4)\n",
      "[[0.56063247 0.72906053 0.5251243  0.47101077]\n",
      " [0.5713517  0.7199196  0.5033343  0.46975708]\n",
      " [0.56228864 0.72884583 0.5217213  0.46318394]\n",
      " [0.5568317  0.72234154 0.542236   0.46997213]\n",
      " [0.565045   0.72274375 0.5204978  0.4723133 ]\n",
      " [0.5617596  0.72167814 0.5329314  0.48003793]\n",
      " [0.56754    0.72232544 0.5141734  0.46625745]\n",
      " [0.5710045  0.707855   0.53253627 0.45907974]] \n",
      " [2.6512175 2.1914332 2.6630518 2.7792363 2.4583826 2.5421977 2.4145055\n",
      " 2.5111294]\n"
     ]
    }
   ],
   "source": [
    "seq_len = 8\n",
    "emb_len = 5\n",
    "d_qk = 3\n",
    "d_v = 4\n",
    "with fastmath.use_backend(\"jax\"):  # specify the backend for consistency\n",
    "    rng_attend = fastmath.random.get_prng(1)\n",
    "    q = k = jax.random.uniform(rng_attend, (seq_len, d_qk), dtype=np.float32)\n",
    "    v = jax.random.uniform(rng_attend, (seq_len, d_v), dtype=np.float32)\n",
    "    o, logits = our_simple_attend(\n",
    "        q,\n",
    "        k,\n",
    "        v,\n",
    "        mask_fn=None,\n",
    "        q_info=None,\n",
    "        kv_info=None,\n",
    "        dropout=0.0,\n",
    "        rng=rng_attend,\n",
    "        verbose=True,\n",
    "    )\n",
    "print(o, \"\\n\", logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class OurSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurSelfAttention(tl.SelfAttention):\n",
    "    \"\"\"Our self-attention. Just the Forward Function.\"\"\"\n",
    "\n",
    "    def forward_unbatched(\n",
    "        self, x, mask=None, *, weights, state, rng, update_state, verbose=False\n",
    "    ):\n",
    "        print(\"ourSelfAttention:forward_unbatched\")\n",
    "        del update_state\n",
    "        attend_rng, output_rng = fastmath.random.split(rng)\n",
    "        if self.bias:\n",
    "            if self.share_qk:\n",
    "                w_q, w_v, w_o, b_q, b_v = weights\n",
    "            else:\n",
    "                w_q, w_k, w_v, w_o, b_q, b_k, b_v = weights\n",
    "        else:\n",
    "            if self.share_qk:\n",
    "                w_q, w_v, w_o = weights\n",
    "            else:\n",
    "                w_q, w_k, w_v, w_o = weights\n",
    "\n",
    "        print(\"x.shape,w_q.shape\", x.shape, w_q.shape)\n",
    "        q = np.matmul(x, w_q)\n",
    "        k = None\n",
    "        if not self.share_qk:\n",
    "            k = np.matmul(x, w_k)\n",
    "        v = np.matmul(x, w_v)\n",
    "\n",
    "        if self.bias:\n",
    "            q = q + b_q\n",
    "            if not self.share_qk:\n",
    "                k = k + b_k\n",
    "            v = v + b_v\n",
    "\n",
    "        mask_fn = functools.partial(\n",
    "            mask_self_attention,\n",
    "            causal=self.causal,\n",
    "            exclude_self=self.share_qk,\n",
    "            masked=self.masked,\n",
    "        )\n",
    "        q_info = kv_info = tie_in(x, np.arange(q.shape[-2], dtype=np.int32))\n",
    "\n",
    "        assert (mask is not None) == self.masked\n",
    "        if self.masked:\n",
    "            # mask is a boolean array (True means \"is valid token\")\n",
    "            ones_like_mask = tie_in(x, np.ones_like(mask, dtype=np.int32))\n",
    "            kv_info = kv_info * np.where(mask, ones_like_mask, -ones_like_mask)\n",
    "\n",
    "        # Notice, we are callout our vesion of attend\n",
    "        o, _ = our_simple_attend(\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            mask_fn=mask_fn,\n",
    "            q_info=q_info,\n",
    "            kv_info=kv_info,\n",
    "            dropout=self.attention_dropout,\n",
    "            rng=attend_rng,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        # Notice, wo weight matrix applied to output of attend in forward_unbatched\n",
    "        out = np.matmul(o, w_o)\n",
    "        out = apply_broadcasted_dropout(out, self.output_dropout, output_rng)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal = False\n",
    "masked = False\n",
    "mask = None\n",
    "attention_dropout = 0.0\n",
    "n_heads = 3\n",
    "d_qk = 3\n",
    "d_v = 4\n",
    "seq_len = 8\n",
    "emb_len = 5\n",
    "batch_size = 1\n",
    "\n",
    "osa = OurSelfAttention(\n",
    "    n_heads=n_heads,\n",
    "    d_qk=d_qk,\n",
    "    d_v=d_v,\n",
    "    causal=causal,\n",
    "    use_reference_code=True,\n",
    "    attention_dropout=attention_dropout,\n",
    "    mode=\"train\",\n",
    ")\n",
    "\n",
    "rng_osa = fastmath.random.get_prng(1)\n",
    "x = jax.random.uniform(\n",
    "    jax.random.PRNGKey(0), (batch_size, seq_len, emb_len), dtype=np.float32\n",
    ")\n",
    "_, _ = osa.init(tl.shapes.signature(x), rng=rng_osa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ourSelfAttention:forward_unbatched\n",
      "x.shape,w_q.shape (8, 5) (5, 3)\n",
      "Our attend dots (8, 8)\n",
      "Our attend dots post softmax (8, 8) (8, 1)\n",
      "Our attend out1 (8, 4)\n",
      "Our attend out2 (8, 4)\n",
      "ourSelfAttention:forward_unbatched\n",
      "x.shape,w_q.shape (8, 5) (5, 3)\n",
      "Our attend dots (8, 8)\n",
      "Our attend dots post softmax (8, 8) (8, 1)\n",
      "Our attend out1 (8, 4)\n",
      "Our attend out2 (8, 4)\n",
      "ourSelfAttention:forward_unbatched\n",
      "x.shape,w_q.shape (8, 5) (5, 3)\n",
      "Our attend dots (8, 8)\n",
      "Our attend dots post softmax (8, 8) (8, 1)\n",
      "Our attend out1 (8, 4)\n",
      "Our attend out2 (8, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[ 6.70414150e-01, -1.04319841e-01, -5.33822298e-01,\n",
       "                1.92711830e-01, -4.52995300e-05],\n",
       "              [ 6.64090276e-01, -1.01875424e-01, -5.35732985e-01,\n",
       "                1.88311636e-01, -6.30623102e-03],\n",
       "              [ 6.73379958e-01, -1.06952399e-01, -5.31989932e-01,\n",
       "                1.90056771e-01,  1.30265951e-03],\n",
       "              [ 6.84564888e-01, -1.13240302e-01, -5.50182462e-01,\n",
       "                1.95673436e-01,  5.47635555e-03],\n",
       "              [ 6.81435883e-01, -1.11069024e-01, -5.32343268e-01,\n",
       "                1.91912323e-01,  5.69400191e-03],\n",
       "              [ 6.80724978e-01, -1.08496964e-01, -5.34994245e-01,\n",
       "                1.96332291e-01,  5.89764118e-03],\n",
       "              [ 6.80933297e-01, -1.14087194e-01, -5.18660128e-01,\n",
       "                1.90674156e-01,  1.14096403e-02],\n",
       "              [ 6.80265129e-01, -1.09031767e-01, -5.38248658e-01,\n",
       "                1.94203168e-01,  4.23955917e-03]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osa(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trax LSHSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_hash_vectors(vecs, rng, n_buckets, n_hashes, mask=None, verbose=False):\n",
    "    \"\"\" \n",
    "  Args:\n",
    "    vecs: tensor of at least 2 dimension, \n",
    "    rng: random number generator\n",
    "    n_buckets: number of buckets in each hash table\n",
    "    n_hashes: the number of hash tables\n",
    "    mask: None indicating no mask or a 1D boolean array of length vecs.shape[0], containing the location of padding value\n",
    "    verbose: controls prints for debug\n",
    "  Returns:\n",
    "    A vector of size n_hashes * vecs.shape[0] containing the buckets associated with each input vector per hash table.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # check for even, integer bucket sizes\n",
    "    assert isinstance(n_buckets, int) and n_buckets % 2 == 0\n",
    "\n",
    "    rng = fastmath.stop_gradient(tie_in(vecs, rng))\n",
    "    rot_size = n_buckets\n",
    "    ### Start Code Here\n",
    "\n",
    "    ### Step 1 ###\n",
    "    rotations_shape = (vecs.shape[-1], n_hashes, rot_size // 2)\n",
    "    random_rotations = fastmath.random.normal(rng, rotations_shape).astype(np.float32)\n",
    "    if verbose:\n",
    "        print(\"random.rotations.shape\", random_rotations.shape)\n",
    "\n",
    "    ### Step 2 ###\n",
    "    if fastmath.backend_name() == \"jax\":\n",
    "        rotated_vecs = np.einsum(\"tf,fhb->htb\", vecs, random_rotations)\n",
    "        print(\"using jax\")\n",
    "    else:\n",
    "        # Step 2a\n",
    "        random_rotations = np.reshape(random_rotations,\n",
    "                                    [-1, n_hashes * (rot_size // 2)])\n",
    "        if verbose:\n",
    "            print(\"random_rotations reshaped\", random_rotations.shape)\n",
    "        # Step 2b\n",
    "        rotated_vecs = np.dot(vecs, random_rotations)\n",
    "        if verbose:\n",
    "            print(\"rotated_vecs1\", rotated_vecs.shape)\n",
    "        # Step 2c\n",
    "        rotated_vecs = np.reshape(rotated_vecs, [-1, n_hashes, rot_size//2])\n",
    "        if verbose:\n",
    "            print(\"rotated_vecs2\", rotated_vecs.shape)\n",
    "        # Step 2d\n",
    "        rotated_vecs = np.transpose(rotated_vecs, (1, 0, 2))\n",
    "        if verbose:\n",
    "            print(\"rotated_vecs3\", rotated_vecs.shape)\n",
    "\n",
    "    ### Step 3 ###\n",
    "    rotated_vecs = np.concatenate([rotated_vecs, -rotated_vecs], axis=-1)\n",
    "    if verbose:\n",
    "        print(\"rotated_vecs.shape\", rotated_vecs.shape)\n",
    "    ### Step 4 ###\n",
    "    buckets = np.argmax(rotated_vecs, axis=-1).astype(np.int32)\n",
    "    if verbose:\n",
    "        print(\"buckets.shape\", buckets.shape)\n",
    "    if verbose:\n",
    "        print(\"buckets\", buckets)\n",
    "\n",
    "    if mask is not None:\n",
    "        n_buckets += 1  # Create an extra bucket for padding tokens only\n",
    "        buckets = np.where(mask[None, :], buckets, n_buckets - 1)\n",
    "\n",
    "    # buckets is now (n_hashes, seqlen). Next we add offsets so that\n",
    "    # bucket numbers from different hashing rounds don't overlap.\n",
    "    offsets = tie_in(buckets, np.arange(n_hashes, dtype=np.int32))\n",
    "    offsets = np.reshape(offsets * n_buckets, (-1, 1))\n",
    "    ### Step 5 ###\n",
    "    buckets = np.reshape(buckets + offsets, (-1,))\n",
    "    if verbose:\n",
    "        print(\"buckets with offsets\", buckets.shape, \"\\n\", buckets)\n",
    "    ### End Code Here\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random.rotations.shape (5, 3, 2)\n",
      "random_rotations reshaped (5, 6)\n",
      "rotated_vecs1 (8, 6)\n",
      "rotated_vecs2 (8, 3, 2)\n",
      "rotated_vecs3 (3, 8, 2)\n",
      "rotated_vecs.shape (3, 8, 4)\n",
      "buckets.shape (3, 8)\n",
      "buckets ndarray<tf.Tensor(\n",
      "[[3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3]], shape=(3, 8), dtype=int32)>\n",
      "buckets with offsets (24,) \n",
      " ndarray<tf.Tensor([ 3  3  3  3  3  3  3  3  7  7  7  7  7  7  7  7 11 11 11 11 11 11 11 11], shape=(24,), dtype=int32)>\n",
      "ohv shape (24,) \n",
      "ohv ndarray<tf.Tensor([ 3  3  3  3  3  3  3  3  7  7  7  7  7  7  7  7 11 11 11 11 11 11 11 11], shape=(24,), dtype=int32)>\n",
      "using jax\n",
      "ohv shape (24,) \n",
      "ohv [ 3  3  3  3  3  3  3  3  5  5  5  5  5  5  5  5 11 11 11 11 11 11 11 11]\n"
     ]
    }
   ],
   "source": [
    "# example code. Note for reference, the sizes in this example match the values in the diagram above.\n",
    "ohv_q = np.ones((8, 5))  # (seq_len=8, n_q=5)\n",
    "ohv_n_buckets = 4  # even number\n",
    "ohv_n_hashes = 3\n",
    "with fastmath.use_backend(\"tf\"):\n",
    "    ohv_rng = fastmath.random.get_prng(1)\n",
    "    ohv = our_hash_vectors(\n",
    "        ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask=None, verbose=True\n",
    "    )\n",
    "    print(\"ohv shape\", ohv.shape, \"\\nohv\", ohv)  # (ohv_n_hashes * ohv_n_buckets)\n",
    "# note the random number generators do not produce the same results with different backends\n",
    "with fastmath.use_backend(\"jax\"):\n",
    "    ohv_rng = fastmath.random.get_prng(1)\n",
    "    ohv = our_hash_vectors(ohv_q, ohv_rng, ohv_n_buckets, ohv_n_hashes, mask=None)\n",
    "    print(\"ohv shape\", ohv.shape, \"\\nohv\", ohv)  # (ohv_n_hashes * ohv_n_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_buckets(buckets, q, v, n_buckets, n_hashes, seqlen, verbose=True):\n",
    "    \"\"\" \n",
    "  Args:\n",
    "    buckets: tensor of at least 2 dimension, \n",
    "    n_buckets: number of buckets in each hash table\n",
    "    n_hashes: the number of hash tables    \n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"---sort_buckets--\")\n",
    "    ## Step 1\n",
    "    ticker = np.arange(n_hashes * seqlen)\n",
    "    if verbose:\n",
    "        print(\"ticker\", ticker.shape, ticker)\n",
    "    ## Step 2\n",
    "    buckets_and_t = seqlen * buckets + (ticker % seqlen)  # provided\n",
    "    if verbose:\n",
    "        print(\"buckets_and_t\", buckets_and_t.shape, buckets_and_t)\n",
    "\n",
    "    # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
    "    # Step 3\n",
    "    sbuckets_and_t, sticker = fastmath.sort_key_val(buckets_and_t, ticker, dimension=-1)\n",
    "    if verbose:\n",
    "        print(\"sbuckets_and_t\", sbuckets_and_t.shape, sbuckets_and_t)\n",
    "    if verbose:\n",
    "        print(\"sticker\", sticker.shape, sticker)\n",
    "    # Step 4\n",
    "    _, undo_sort = fastmath.sort_key_val(sticker, ticker, dimension=-1)\n",
    "    if verbose:\n",
    "        print(\"undo_sort\", undo_sort.shape, undo_sort)\n",
    "\n",
    "    # Step 5\n",
    "    st = (sticker % seqlen)\n",
    "    sq = np.take(q, st, axis=0)\n",
    "    sv = np.take(v, st, axis=0)\n",
    "    return sq, sv, sticker, undo_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q\n",
      " [[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]\n",
      " [3. 3. 3.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]\n",
      " [3. 3. 3.]]\n",
      "t_buckets:  [0 1 2 3 0 1 2 3 4 5 6 7 4 5 6 7]\n",
      "---sort_buckets--\n",
      "ticker (16,) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "buckets_and_t (16,) [ 0  9 18 27  4 13 22 31 32 41 50 59 36 45 54 63]\n",
      "sbuckets_and_t (16,) [ 0  4  9 13 18 22 27 31 32 36 41 45 50 54 59 63]\n",
      "sticker (16,) [ 0  4  1  5  2  6  3  7  8 12  9 13 10 14 11 15]\n",
      "undo_sort (16,) [ 0  2  4  6  1  3  5  7  8 10 12 14  9 11 13 15]\n",
      "sq.shape (16, 3) sv.shape (16, 5)\n",
      "sq\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "t_n_hashes = 2\n",
    "t_n_buckets = 4\n",
    "t_n_seq = t_seqlen = 8\n",
    "t_n_q = 3\n",
    "n_v = 5\n",
    "\n",
    "t_q = (np.array([(j % t_n_buckets) for j in range(t_n_seq)]) * np.ones((t_n_q, 1))).T\n",
    "t_v = np.ones((t_n_seq, n_v))\n",
    "t_buckets = np.array(\n",
    "    [\n",
    "        (j % t_n_buckets) + t_n_buckets * i\n",
    "        for i in range(t_n_hashes)\n",
    "        for j in range(t_n_seq)\n",
    "    ]\n",
    ")\n",
    "print(\"q\\n\", t_q)\n",
    "print(\"t_buckets: \", t_buckets)\n",
    "\n",
    "t_sq, t_sv, t_sticker, t_undo_sort = sort_buckets(\n",
    "    t_buckets, t_q, t_v, t_n_buckets, t_n_hashes, t_seqlen, verbose=True\n",
    ")\n",
    "\n",
    "print(\"sq.shape\", t_sq.shape, \"sv.shape\", t_sv.shape)\n",
    "print(\"sq\\n\", t_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunked dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(16 * 3).reshape((16, 3))\n",
    "chunksize = 2\n",
    "ar = np.reshape(\n",
    "    a, (-1, chunksize, a.shape[-1])\n",
    ")  # the -1 usage is very handy, see numpy reshape\n",
    "print(ar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotandv(\n",
    "    sq, sv, undo_sort, kv_chunk_len, n_hashes, seqlen, passthrough, verbose=False\n",
    "):\n",
    "    # Step 1\n",
    "    rsq = np.reshape(sq,(-1, kv_chunk_len, sq.shape[-1]))\n",
    "    rsqt =  np.swapaxes(rsq, -1, -2)\n",
    "    if verbose:\n",
    "        print(\"rsq.shape,rsqt.shape: \", rsq.shape, rsqt.shape)\n",
    "    dotlike = np.matmul(rsq, rsqt)\n",
    "    if verbose:\n",
    "        print(\"dotlike\\n\", dotlike)\n",
    "\n",
    "    # Step 2\n",
    "    dotlike, slogits = our_softmax(dotlike, passthrough)\n",
    "    if verbose:\n",
    "        print(\"dotlike post softmax\\n\", dotlike)\n",
    "\n",
    "    # Step 3\n",
    "    vr = np.reshape(sv, (-1, kv_chunk_len, sv.shape[-1]))\n",
    "    if verbose:\n",
    "        print(\"dotlike.shape, vr.shape:\", dotlike.shape, vr.shape)\n",
    "    so = np.matmul(dotlike, vr)\n",
    "    if verbose:\n",
    "        print(\"so.shape:\", so.shape)\n",
    "    so = np.reshape(so, (-1, so.shape[-1]))\n",
    "    slogits = np.reshape(slogits, (-1,))  # provided\n",
    "    if verbose:\n",
    "        print(\"so.shape,slogits.shape\", so.shape, slogits.shape)\n",
    "\n",
    "    # Step 4\n",
    "    o = np.take(so, undo_sort, axis=0)\n",
    "    logits = np.take(slogits, undo_sort, axis=0)\n",
    "    if verbose:\n",
    "        print(\"o.shape,o\", o.shape, o)\n",
    "    if verbose:\n",
    "        print(\"logits.shape, logits\", logits.shape, logits)\n",
    "\n",
    "    # Step 5 (Provided)\n",
    "    if n_hashes > 1:\n",
    "        o = np.reshape(o, (n_hashes, seqlen, o.shape[-1]))\n",
    "        logits = np.reshape(logits, (n_hashes, seqlen, 1))\n",
    "        probs = np.exp(logits - fastmath.logsumexp(logits, axis=0, keepdims=True))\n",
    "        o = np.sum(o * probs, axis=0)\n",
    "\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsq.shape,rsqt.shape:  (8, 2, 3) (8, 3, 2)\n",
      "dotlike\n",
      " [[[ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 3.  3.]\n",
      "  [ 3.  3.]]\n",
      "\n",
      " [[12. 12.]\n",
      "  [12. 12.]]\n",
      "\n",
      " [[27. 27.]\n",
      "  [27. 27.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 3.  3.]\n",
      "  [ 3.  3.]]\n",
      "\n",
      " [[12. 12.]\n",
      "  [12. 12.]]\n",
      "\n",
      " [[27. 27.]\n",
      "  [27. 27.]]]\n",
      "dotlike post softmax\n",
      " [[[ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 3.  3.]\n",
      "  [ 3.  3.]]\n",
      "\n",
      " [[12. 12.]\n",
      "  [12. 12.]]\n",
      "\n",
      " [[27. 27.]\n",
      "  [27. 27.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 3.  3.]\n",
      "  [ 3.  3.]]\n",
      "\n",
      " [[12. 12.]\n",
      "  [12. 12.]]\n",
      "\n",
      " [[27. 27.]\n",
      "  [27. 27.]]]\n",
      "dotlike.shape, vr.shape: (8, 2, 2) (8, 2, 5)\n",
      "so.shape: (8, 2, 5)\n",
      "so.shape,slogits.shape (16, 5) (16,)\n",
      "o.shape,o (16, 5) [[ 0.  0.  0.  0.  0.]\n",
      " [ 6.  6.  6.  6.  6.]\n",
      " [24. 24. 24. 24. 24.]\n",
      " [54. 54. 54. 54. 54.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 6.  6.  6.  6.  6.]\n",
      " [24. 24. 24. 24. 24.]\n",
      " [54. 54. 54. 54. 54.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 6.  6.  6.  6.  6.]\n",
      " [24. 24. 24. 24. 24.]\n",
      " [54. 54. 54. 54. 54.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 6.  6.  6.  6.  6.]\n",
      " [24. 24. 24. 24. 24.]\n",
      " [54. 54. 54. 54. 54.]]\n",
      "logits.shape, logits (16,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "out\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      " [ 6.  6.  6.  6.  6.]\n",
      " [24. 24. 24. 24. 24.]\n",
      " [54. 54. 54. 54. 54.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 6.  6.  6.  6.  6.]\n",
      " [24. 24. 24. 24. 24.]\n",
      " [54. 54. 54. 54. 54.]]\n",
      "\n",
      "-----With softmax enabled----\n",
      "\n",
      "rsq.shape,rsqt.shape:  (8, 2, 3) (8, 3, 2)\n",
      "dotlike\n",
      " [[[ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 3.  3.]\n",
      "  [ 3.  3.]]\n",
      "\n",
      " [[12. 12.]\n",
      "  [12. 12.]]\n",
      "\n",
      " [[27. 27.]\n",
      "  [27. 27.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 3.  3.]\n",
      "  [ 3.  3.]]\n",
      "\n",
      " [[12. 12.]\n",
      "  [12. 12.]]\n",
      "\n",
      " [[27. 27.]\n",
      "  [27. 27.]]]\n",
      "dotlike post softmax\n",
      " [[[0.5        0.5       ]\n",
      "  [0.5        0.5       ]]\n",
      "\n",
      " [[0.5        0.5       ]\n",
      "  [0.5        0.5       ]]\n",
      "\n",
      " [[0.49999976 0.49999976]\n",
      "  [0.49999976 0.49999976]]\n",
      "\n",
      " [[0.49999976 0.49999976]\n",
      "  [0.49999976 0.49999976]]\n",
      "\n",
      " [[0.5        0.5       ]\n",
      "  [0.5        0.5       ]]\n",
      "\n",
      " [[0.5        0.5       ]\n",
      "  [0.5        0.5       ]]\n",
      "\n",
      " [[0.49999976 0.49999976]\n",
      "  [0.49999976 0.49999976]]\n",
      "\n",
      " [[0.49999976 0.49999976]\n",
      "  [0.49999976 0.49999976]]]\n",
      "dotlike.shape, vr.shape: (8, 2, 2) (8, 2, 5)\n",
      "so.shape: (8, 2, 5)\n",
      "so.shape,slogits.shape (16, 5) (16,)\n",
      "o.shape,o (16, 5) [[1.        1.        1.        1.        1.       ]\n",
      " [1.        1.        1.        1.        1.       ]\n",
      " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
      " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
      " [1.        1.        1.        1.        1.       ]\n",
      " [1.        1.        1.        1.        1.       ]\n",
      " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
      " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
      " [1.        1.        1.        1.        1.       ]\n",
      " [1.        1.        1.        1.        1.       ]\n",
      " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
      " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
      " [1.        1.        1.        1.        1.       ]\n",
      " [1.        1.        1.        1.        1.       ]\n",
      " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]\n",
      " [0.9999995 0.9999995 0.9999995 0.9999995 0.9999995]]\n",
      "logits.shape, logits (16,) [ 0.6931472  3.6931472 12.693148  27.693148   0.6931472  3.6931472\n",
      " 12.693148  27.693148   0.6931472  3.6931472 12.693148  27.693148\n",
      "  0.6931472  3.6931472 12.693148  27.693148 ]\n",
      "out\n",
      " [[1.         1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.        ]\n",
      " [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]\n",
      " [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]\n",
      " [1.         1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.        ]\n",
      " [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]\n",
      " [0.99999905 0.99999905 0.99999905 0.99999905 0.99999905]]\n"
     ]
    }
   ],
   "source": [
    "t_kv_chunk_len = 2\n",
    "out = dotandv(\n",
    "    t_sq,\n",
    "    t_sv,\n",
    "    t_undo_sort,\n",
    "    t_kv_chunk_len,\n",
    "    t_n_hashes,\n",
    "    t_seqlen,\n",
    "    passthrough=True,\n",
    "    verbose=True,\n",
    ")\n",
    "print(\"out\\n\", out)\n",
    "print(\"\\n-----With softmax enabled----\\n\")\n",
    "out = dotandv(\n",
    "    t_sq,\n",
    "    t_sv,\n",
    "    t_undo_sort,\n",
    "    t_kv_chunk_len,\n",
    "    t_n_hashes,\n",
    "    t_seqlen,\n",
    "    passthrough=False,\n",
    "    verbose=True,\n",
    ")\n",
    "print(\"out\\n\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OurLSHSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original version from trax 1.3.4\n",
    "def attend(\n",
    "    q,\n",
    "    k=None,\n",
    "    v=None,\n",
    "    q_chunk_len=None,\n",
    "    kv_chunk_len=None,\n",
    "    n_chunks_before=0,\n",
    "    n_chunks_after=0,\n",
    "    mask_fn=None,\n",
    "    q_info=None,\n",
    "    kv_info=None,\n",
    "    dropout=0.0,\n",
    "    rng=None,\n",
    "):\n",
    "    \"\"\"Dot-product attention, with optional chunking and/or masking.\n",
    "\n",
    "  Args:\n",
    "    q: Query vectors, shape [q_len, d_qk]\n",
    "    k: Key vectors, shape [kv_len, d_qk]; or None\n",
    "    v: Value vectors, shape [kv_len, d_v]\n",
    "    q_chunk_len: Set to non-zero to enable chunking for query vectors\n",
    "    kv_chunk_len: Set to non-zero to enable chunking for key/value vectors\n",
    "    n_chunks_before: Number of adjacent previous chunks to attend to\n",
    "    n_chunks_after: Number of adjacent subsequent chunks to attend to\n",
    "    mask_fn: TODO(kitaev) doc\n",
    "    q_info: Query-associated metadata for masking\n",
    "    kv_info: Key-associated metadata for masking\n",
    "    dropout: Dropout rate\n",
    "    rng: RNG for dropout\n",
    "\n",
    "  Returns:\n",
    "    A tuple (output, dots_logsumexp). The output has shape [q_len, d_v], and\n",
    "    dots_logsumexp has shape [q_len]. The logsumexp of the attention\n",
    "    probabilities is useful for combining multiple rounds of attention (as in\n",
    "    LSH attention).\n",
    "  \"\"\"\n",
    "    assert v is not None\n",
    "    share_qk = k is None\n",
    "\n",
    "    if q_info is None:\n",
    "        q_info = np.arange(q.shape[-2], dtype=np.int32)\n",
    "\n",
    "    if kv_info is None and not share_qk:\n",
    "        kv_info = np.arange(v.shape[-2], dtype=np.int32)\n",
    "\n",
    "    # Split q/k/v into chunks along the time axis, if desired.\n",
    "    if q_chunk_len is not None:\n",
    "        q = np.reshape(q, (-1, q_chunk_len, q.shape[-1]))\n",
    "        q_info = np.reshape(q_info, (-1, q_chunk_len))\n",
    "\n",
    "    if share_qk:\n",
    "        assert kv_chunk_len is None or kv_chunk_len == q_chunk_len\n",
    "        k = q\n",
    "        kv_chunk_len = q_chunk_len\n",
    "        if kv_info is None:\n",
    "            kv_info = q_info\n",
    "        elif kv_chunk_len is not None:\n",
    "            # kv_info is not None, but reshape as required.\n",
    "            kv_info = np.reshape(kv_info, (-1, kv_chunk_len))\n",
    "    elif kv_chunk_len is not None:\n",
    "        k = np.reshape(k, (-1, kv_chunk_len, k.shape[-1]))\n",
    "        kv_info = np.reshape(kv_info, (-1, kv_chunk_len))\n",
    "\n",
    "    if kv_chunk_len is not None:\n",
    "        v = np.reshape(v, (-1, kv_chunk_len, v.shape[-1]))\n",
    "\n",
    "    if share_qk:\n",
    "        k = length_normalized(k)\n",
    "    k = k / np.sqrt(k.shape[-1])\n",
    "\n",
    "    # Optionally include adjacent chunks.\n",
    "    if q_chunk_len is not None or kv_chunk_len is not None:\n",
    "        assert q_chunk_len is not None and kv_chunk_len is not None\n",
    "    else:\n",
    "        assert n_chunks_before == 0 and n_chunks_after == 0\n",
    "\n",
    "    k = look_adjacent(k, n_chunks_before, n_chunks_after)\n",
    "    v = look_adjacent(v, n_chunks_before, n_chunks_after)\n",
    "    kv_info = look_adjacent(kv_info, n_chunks_before, n_chunks_after)\n",
    "\n",
    "    # Dot-product attention.\n",
    "    dots = np.matmul(q, np.swapaxes(k, -1, -2))\n",
    "\n",
    "    # Masking\n",
    "    if mask_fn is not None:\n",
    "        dots = mask_fn(dots, q_info[..., :, None], kv_info[..., None, :])\n",
    "\n",
    "    # Softmax.\n",
    "    dots_logsumexp = fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
    "    dots = np.exp(dots - dots_logsumexp)\n",
    "\n",
    "    if dropout > 0.0:\n",
    "        assert rng is not None\n",
    "        # Dropout is broadcast across the bin dimension\n",
    "        dropout_shape = (dots.shape[-2], dots.shape[-1])\n",
    "        #\n",
    "        keep_prob = tie_in(dots, 1.0 - dropout)\n",
    "        keep = fastmath.random.bernoulli(rng, keep_prob, dropout_shape)\n",
    "        multiplier = keep.astype(dots.dtype) / tie_in(keep, keep_prob)\n",
    "        dots = dots * multiplier\n",
    "\n",
    "    # The softmax normalizer (dots_logsumexp) is used by multi-round LSH attn.\n",
    "    out = np.matmul(dots, v)\n",
    "    out = np.reshape(out, (-1, out.shape[-1]))\n",
    "    dots_logsumexp = np.reshape(dots_logsumexp, (-1,))\n",
    "    return out, dots_logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurLSHSelfAttention(tl.LSHSelfAttention):\n",
    "    \"\"\"Our simplified LSH self-attention \"\"\"\n",
    "\n",
    "    def forward_unbatched(self, x, mask=None, *, weights, state, rng, update_state):\n",
    "        attend_rng, output_rng = fastmath.random.split(rng)\n",
    "        w_q, w_v, w_o = weights\n",
    "\n",
    "        q = np.matmul(x, w_q)\n",
    "        v = np.matmul(x, w_v)\n",
    "\n",
    "        if update_state:\n",
    "            _, old_hash_rng = state\n",
    "            hash_rng, hash_subrng = fastmath.random.split(old_hash_rng)\n",
    "            #      buckets = self.hash_vectors(q, hash_subrng, mask)  #  original\n",
    "            ## use our version of hash\n",
    "            buckets = our_hash_vectors(\n",
    "                q, hash_subrng, self.n_buckets, self.n_hashes, mask=mask\n",
    "            )\n",
    "            s_buckets = buckets\n",
    "            if self._max_length_for_buckets:\n",
    "                length = self.n_hashes * self._max_length_for_buckets\n",
    "                if buckets.shape[0] < length:\n",
    "                    s_buckets = np.concatenate(\n",
    "                        [buckets, np.zeros(length - buckets.shape[0], dtype=np.int32)],\n",
    "                        axis=0,\n",
    "                    )\n",
    "            state = (s_buckets, hash_rng)\n",
    "        else:\n",
    "            buckets, _ = state\n",
    "            if self._max_length_for_buckets:\n",
    "                buckets = buckets[: self.n_hashes * x.shape[0]]\n",
    "\n",
    "        seqlen = x.shape[0]\n",
    "        assert int(buckets.shape[0]) == self.n_hashes * seqlen\n",
    "\n",
    "        ticker = tie_in(x, np.arange(self.n_hashes * seqlen, dtype=np.int32))\n",
    "        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n",
    "        buckets_and_t = fastmath.stop_gradient(buckets_and_t)\n",
    "\n",
    "        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
    "        sbuckets_and_t, sticker = fastmath.sort_key_val(\n",
    "            buckets_and_t, ticker, dimension=-1\n",
    "        )\n",
    "        _, undo_sort = fastmath.sort_key_val(sticker, ticker, dimension=-1)\n",
    "        sbuckets_and_t = fastmath.stop_gradient(sbuckets_and_t)\n",
    "        sticker = fastmath.stop_gradient(sticker)\n",
    "        undo_sort = fastmath.stop_gradient(undo_sort)\n",
    "\n",
    "        st = sticker % seqlen\n",
    "        sq = np.take(q, st, axis=0)\n",
    "        sv = np.take(v, st, axis=0)\n",
    "\n",
    "        mask_fn = functools.partial(\n",
    "            mask_self_attention,\n",
    "            causal=self.causal,\n",
    "            exclude_self=True,\n",
    "            masked=self.masked,\n",
    "        )\n",
    "        q_info = st\n",
    "\n",
    "        assert (mask is not None) == self.masked\n",
    "        kv_info = None\n",
    "        if self.masked:\n",
    "            # mask is a boolean array (True means \"is valid token\")\n",
    "            smask = np.take(mask, st, axis=0)\n",
    "            ones_like_mask = tie_in(x, np.ones_like(smask, dtype=np.int32))\n",
    "            kv_info = q_info * np.where(smask, ones_like_mask, -ones_like_mask)\n",
    "\n",
    "        ## use original version of attend (could use ours but lacks masks and masking)\n",
    "        so, slogits = attend(\n",
    "            sq,\n",
    "            k=None,\n",
    "            v=sv,\n",
    "            q_chunk_len=self.chunk_len,\n",
    "            n_chunks_before=self.n_chunks_before,\n",
    "            n_chunks_after=self.n_chunks_after,\n",
    "            mask_fn=mask_fn,\n",
    "            q_info=q_info,\n",
    "            kv_info=kv_info,\n",
    "            dropout=self.attention_dropout,\n",
    "            rng=attend_rng,\n",
    "        )\n",
    "\n",
    "        # np.take(so, undo_sort, axis=0); np.take(slogits, undo_sort, axis=0) would\n",
    "        # also work, but these helpers include performance optimizations for TPU.\n",
    "        o = permute_via_gather(so, undo_sort, sticker, axis=0)\n",
    "        logits = permute_via_sort(slogits, sticker, buckets_and_t, axis=-1)\n",
    "\n",
    "        if self.n_hashes > 1:\n",
    "            o = np.reshape(o, (self.n_hashes, seqlen, o.shape[-1]))\n",
    "            logits = np.reshape(logits, (self.n_hashes, seqlen, 1))\n",
    "            probs = np.exp(logits - fastmath.logsumexp(logits, axis=0, keepdims=True))\n",
    "            o = np.sum(o * probs, axis=0)\n",
    "\n",
    "        assert o.shape == (seqlen, w_v.shape[-1])\n",
    "        out = np.matmul(o, w_o)\n",
    "        out = apply_broadcasted_dropout(out, self.output_dropout, output_rng)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're going to try out our LSHSelfAttention\n",
    "n_heads = 3\n",
    "causal = False\n",
    "masked = False\n",
    "mask = None\n",
    "chunk_len = 8\n",
    "n_chunks_before = 0\n",
    "n_chunks_after = 0\n",
    "attention_dropout = 0.0\n",
    "n_hashes = 5\n",
    "n_buckets = 4\n",
    "seq_len = 8\n",
    "emb_len = 5\n",
    "al = OurLSHSelfAttention(\n",
    "    n_heads=n_heads,\n",
    "    d_qk=3,\n",
    "    d_v=4,\n",
    "    causal=causal,\n",
    "    chunk_len=8,\n",
    "    n_chunks_before=n_chunks_before,\n",
    "    n_chunks_after=n_chunks_after,\n",
    "    n_hashes=n_hashes,\n",
    "    n_buckets=n_buckets,\n",
    "    use_reference_code=True,\n",
    "    attention_dropout=attention_dropout,\n",
    "    mode=\"train\",\n",
    ")\n",
    "\n",
    "x = jax.random.uniform(jax.random.PRNGKey(0), (1, seq_len, emb_len), dtype=np.float32)\n",
    "al_osa = fastmath.random.get_prng(1)\n",
    "_, _ = al.init(tl.shapes.signature(x), rng=al_osa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using jax\n",
      "using jax\n",
      "using jax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[ 6.6842866e-01, -1.1364317e-01, -5.4430598e-01,\n",
       "                2.1126229e-01, -1.0988474e-02],\n",
       "              [ 7.0949805e-01, -1.5455173e-01, -5.9923279e-01,\n",
       "                2.2719435e-01,  1.3833910e-02],\n",
       "              [ 7.1442688e-01, -1.2046629e-01, -5.3956538e-01,\n",
       "                1.7320301e-01, -1.6552359e-02],\n",
       "              [ 6.7178923e-01, -7.6611072e-02, -5.9399861e-01,\n",
       "                2.1236289e-01,  7.9482794e-04],\n",
       "              [ 7.1518469e-01, -1.1359149e-01, -5.7821870e-01,\n",
       "                2.1304403e-01,  3.0598417e-02],\n",
       "              [ 6.8235356e-01, -9.3979865e-02, -5.5341852e-01,\n",
       "                2.1608178e-01, -6.6673756e-04],\n",
       "              [ 6.1286646e-01, -8.1026971e-02, -4.8148811e-01,\n",
       "                1.9373316e-01,  3.1555206e-02],\n",
       "              [ 7.2203499e-01, -1.0199657e-01, -5.5215180e-01,\n",
       "                1.7872262e-01, -2.2289127e-02]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the \"Re\" in Reformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trax\n",
    "from trax import layers as tl               # core building block\n",
    "import numpy as np                          # regular ol' numpy\n",
    "from trax.models.reformer.reformer import (\n",
    "    ReversibleHalfResidualV2 as ReversibleHalfResidual,\n",
    ")                                           # unique spot\n",
    "from trax import fastmath                   # uses jax, offers numpy on steroids\n",
    "from trax import shapes                     # data signatures: dimensionality and type\n",
    "from trax.fastmath import numpy as jnp      # For use in defining new layer types.\n",
    "from trax.shapes import ShapeDtype\n",
    "from trax.shapes import signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] [3] [4]\n",
      "name: add1 number of inputs: 1 number of outputs: 1\n"
     ]
    }
   ],
   "source": [
    "# simple function taking one input and one output\n",
    "bl_add1 = tl.Fn(\"add1\", lambda x0: (x0 + 1), n_out=1)\n",
    "bl_add2 = tl.Fn(\"add2\", lambda x0: (x0 + 2), n_out=1)\n",
    "bl_add3 = tl.Fn(\"add3\", lambda x0: (x0 + 3), n_out=1)\n",
    "# try them out\n",
    "x = np.array([1])\n",
    "print(bl_add1(x), bl_add2(x), bl_add3(x))\n",
    "# some information about our new layers\n",
    "print(\n",
    "    \"name:\",\n",
    "    bl_add1.name,\n",
    "    \"number of inputs:\",\n",
    "    bl_add1.n_in,\n",
    "    \"number of outputs:\",\n",
    "    bl_add1.n_out,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Branch_out3[\n",
       "  add1\n",
       "  add2\n",
       "  add3\n",
       "]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_3add1s = tl.Branch(bl_add1, bl_add2, bl_add3)\n",
    "bl_3add1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), array([3]), array([4]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_in = 1, Each bl_addx pushes n_out = 1 elements onto the stack\n",
    "bl_3add1s(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), array([3]), array([4]), 'n', 'm')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n = np.array([10]); m = np.array([20])  # n, m will remain on the stack\n",
    "n = \"n\"\n",
    "m = \"m\"  # n, m will remain on the stack\n",
    "bl_3add1s([x, n, m]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_addab = tl.Fn(\n",
    "    \"addab\", lambda x0, x1: (x0 + x1), n_out=1\n",
    ")  # Trax figures out how many inputs there are\n",
    "bl_rep3x = tl.Fn(\n",
    "    \"add2x\", lambda x0: (x0, x0, x0), n_out=3\n",
    ")  # but you have to tell it how many outputs there are\n",
    "bl_3ops = tl.Branch(bl_add1, bl_addab, bl_rep3x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), array([4]), array([1]), array([1]), array([1]), 'n', 'm')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before Running this cell, what is the output you are expecting?\n",
    "y = np.array([3])\n",
    "bl_3ops([x, y, n, m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), array([1]), 'n', 'm')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_2ops = tl.Branch(bl_add1, None)\n",
    "bl_2ops([x, n, m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The Trax Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Lint as: python3\n",
    "\"\"\"Combinators for composing layers.\"\"\"\n",
    "\n",
    "from trax import fastmath\n",
    "from trax.fastmath import numpy as jnp\n",
    "from trax.layers import base\n",
    "from trax.layers.base import Fn\n",
    "from trax.shapes import ShapeDtype\n",
    "\n",
    "\n",
    "class Serial(base.Layer):\n",
    "  \"\"\"Combinator that applies layers serially (by function composition).\n",
    "  This combinator is commonly used to construct deep networks, e.g., like this::\n",
    "      mlp = tl.Serial(\n",
    "        tl.Dense(128),\n",
    "        tl.Relu(),\n",
    "        tl.Dense(10),\n",
    "        tl.LogSoftmax()\n",
    "      )\n",
    "  A Serial combinator uses stack semantics to manage data for its sublayers.\n",
    "  Each sublayer sees only the inputs it needs and returns only the outputs it\n",
    "  has generated. The sublayers interact via the data stack. For instance, a\n",
    "  sublayer k, following sublayer j, gets called with the data stack in the\n",
    "  state left after layer j has applied. The Serial combinator then:\n",
    "    - takes n_in items off the top of the stack (n_in = k.n_in) and calls\n",
    "      layer k, passing those items as arguments; and\n",
    "    - takes layer k's n_out return values (n_out = k.n_out) and pushes\n",
    "      them onto the data stack.\n",
    "  A Serial instance with no sublayers acts as a special-case (but useful)\n",
    "  1-input 1-output no-op.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, *sublayers, name=None, sublayers_to_print=None):\n",
    "    super().__init__(\n",
    "        name=name, sublayers_to_print=sublayers_to_print)\n",
    "\n",
    "    sublayers = _ensure_flat(sublayers)\n",
    "    self._sublayers = sublayers\n",
    "    self._n_layers = len(sublayers)\n",
    "\n",
    "    if sublayers:\n",
    "      self._n_in, self._n_out = self._n_inputs_n_outputs(sublayers)\n",
    "      self._weights = tuple(None for l in sublayers)\n",
    "      self._state = tuple(None for l in sublayers)\n",
    "\n",
    "  def forward(self, xs):\n",
    "    self._validate_forward_inputs(xs)\n",
    "    state, weights = self.state, self.weights\n",
    "    rngs = _split_rngs(self.rng, self._n_layers)\n",
    "    if not self.sublayers:  # No-op: leave args unchanged.\n",
    "      return xs\n",
    "\n",
    "    stack = xs\n",
    "    new_state = []\n",
    "    n_layers = self._n_layers\n",
    "    if len(weights) != n_layers:\n",
    "      raise ValueError(\n",
    "          f'Number of weight elements ({len(weights)}) does not equal '\n",
    "          f'number of sublayers ({n_layers}).')\n",
    "    if len(state) != n_layers:\n",
    "      raise ValueError(\n",
    "          f'Number of state elements ({len(state)}) does not equal '\n",
    "          f'number of sublayers ({n_layers}).')\n",
    "\n",
    "    for layer, w, s, rng in zip(self.sublayers, weights, state, rngs):\n",
    "      inputs = _inputs_from_stack(layer, stack)\n",
    "      outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\n",
    "      stack = _outputs_onto_stack(layer, outputs, stack)\n",
    "      new_state.append(s)\n",
    "    self.state = new_state\n",
    "    return stack\n",
    "\n",
    "  # pylint: disable=protected-access\n",
    "  def init_weights_and_state(self, input_signature):\n",
    "    weights = []\n",
    "    states = []\n",
    "    # In the code below, stack, inputs, and outputs are abstract (shapes and\n",
    "    # dtypes), but weights and states are non-abstract actual values.\n",
    "    stack = input_signature\n",
    "    for sublayer in self.sublayers:\n",
    "      inputs = _inputs_from_stack(sublayer, stack)\n",
    "      weights_or_cache_marker, state_or_cache_marker = (\n",
    "          sublayer.init(inputs, use_cache=True))\n",
    "      outputs, _ = sublayer._forward_abstract(inputs)\n",
    "      stack = _outputs_onto_stack(sublayer, outputs, stack)\n",
    "\n",
    "      weights.append(weights_or_cache_marker)\n",
    "      states.append(state_or_cache_marker)\n",
    "    self.state = states\n",
    "    self.weights = weights\n",
    "  # pylint: enable=protected-access\n",
    "\n",
    "  def _n_inputs_n_outputs(self, layers):\n",
    "    del self\n",
    "    running_max = 0\n",
    "    running_total = 0\n",
    "    for layer in layers:\n",
    "      running_total += layer.n_in\n",
    "      running_max = max(running_max, running_total)\n",
    "      running_total -= layer.n_out\n",
    "    return running_max, (running_max - running_total)\n",
    "\n",
    "  def _validate_forward_inputs(self, xs):\n",
    "    if not isinstance(xs, (tuple, list)) and self._n_in != 1:\n",
    "      raise TypeError(f'Serial.forward input must be a tuple or list; '\n",
    "                      f'instead got {type(xs)}.')\n",
    "      # TODO(jonni): Include full xs (or shape) in error message?\n",
    "    len_xs = 1 if isinstance(xs, jnp.ndarray) else len(xs)\n",
    "    if len_xs < self.n_in:\n",
    "      raise ValueError(\n",
    "          f'Number of inputs ({len(xs)}) to Serial.forward less than n_in '\n",
    "          f'({self.n_in}).')\n",
    "\n",
    "\n",
    "class Parallel(base.Layer):\n",
    "  \"\"\"Combinator that applies a list of layers in parallel to its inputs.\n",
    "  Layers in the list apply to successive spans of inputs, where the spans are\n",
    "  determined how many inputs each layer takes. The resulting output is the\n",
    "  (flattened) concatenation of the respective layer outputs.\n",
    "  For example, suppose one has three layers:\n",
    "    - F: 1 input, 1 output\n",
    "    - G: 3 inputs, 1 output\n",
    "    - H: 2 inputs, 2 outputs (h1, h2)\n",
    "  Then Parallel(F, G, H) will take 6 inputs and give 4 outputs:\n",
    "    - inputs: a, b, c, d, e, f\n",
    "    - outputs: F(a), G(b, c, d), h1, h2\n",
    "  As an important special case, a None argument to Parallel acts as if it takes\n",
    "  one argument, which it leaves unchanged. (It acts as a one-arg no-op.) For\n",
    "  example:\n",
    "    Parallel(None, F)\n",
    "  creates a layer that passes its first input unchanged and applies F to the\n",
    "  following input(s).\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, *sublayers, name=None):\n",
    "    \"\"\"The constructor.\n",
    "    Args:\n",
    "      *sublayers: A list of sublayers.\n",
    "      name: Descriptive name for this layer.\n",
    "    Returns:\n",
    "      A new layer in which each of the given sublayers applies to its\n",
    "      corresponding span of elements in the dataflow stack.\n",
    "    \"\"\"\n",
    "    super().__init__(name=name)\n",
    "    sublayers = self._validate(sublayers)\n",
    "    self._n_layers = len(sublayers)\n",
    "    self._sublayers = sublayers\n",
    "    self._n_in = sum(l.n_in for l in sublayers)\n",
    "    self._n_out = sum(l.n_out for l in sublayers)\n",
    "    self._weights = tuple(None for l in sublayers)\n",
    "    self._state = tuple(None for l in sublayers)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    n_layers, layers = self._n_layers, self.sublayers\n",
    "    sublayer_inputs = self._allot_to_sublayers(inputs)\n",
    "    state, weights = self.state, self.weights\n",
    "    rngs = _split_rngs(self.rng, n_layers)\n",
    "    if len(sublayer_inputs) != n_layers:\n",
    "      raise ValueError(\n",
    "          f'Number of inputs for sublayers ({len(sublayer_inputs)}) does not equal '\n",
    "          f'number of sublayers ({n_layers}).')\n",
    "    if len(weights) != n_layers:\n",
    "      raise ValueError(\n",
    "          f'Number of weight elements ({len(weights)}) does not equal '\n",
    "          f'number of sublayers ({n_layers}).')\n",
    "    if len(state) != n_layers:\n",
    "      raise ValueError(\n",
    "          f'Number of state elements ({len(state)}) does not equal '\n",
    "          f'number of sublayers ({n_layers}).')\n",
    "    if len(rngs) != n_layers:\n",
    "      raise ValueError(\n",
    "          f'Number of rngs ({len(rngs)}) does not equal '\n",
    "          f'number of sublayers ({n_layers}).')\n",
    "    outputs = []\n",
    "    new_state = []\n",
    "    for layer, x, w, s, r in zip(layers, sublayer_inputs, weights, state, rngs):\n",
    "      # Note that zip silently truncates its result if lengths don't match.\n",
    "      sub_outputs, sub_state = layer.pure_fn(x, w, s, r, use_cache=True)\n",
    "      if layer.n_out == 1:\n",
    "        outputs.append(sub_outputs)\n",
    "      else:\n",
    "        outputs.extend(sub_outputs)\n",
    "      new_state.append(sub_state)\n",
    "    output = outputs[0] if self.n_out == 1 else tuple(outputs)\n",
    "    self.state = tuple(new_state)\n",
    "    return output\n",
    "\n",
    "  def init_weights_and_state(self, input_signature):\n",
    "    sublayer_signatures = self._allot_to_sublayers(input_signature)\n",
    "    inits = [layer.init(signature, use_cache=True)\n",
    "             for layer, signature\n",
    "             in zip(self.sublayers, sublayer_signatures)]\n",
    "    if inits:\n",
    "      weights, state = tuple(zip(*inits))\n",
    "      self.state = state\n",
    "      self.weights = weights\n",
    "\n",
    "  def _validate(self, layers):\n",
    "    if not layers or len(layers) < 2:\n",
    "      raise ValueError(\n",
    "          f'layers ({layers}) must be a list with at least two elements')\n",
    "    layers = list(layers)  # Ensure we can modify layers.\n",
    "    for i, obj in enumerate(layers):\n",
    "      if obj is None or obj == []:  # pylint: disable=g-explicit-bool-comparison\n",
    "        layers[i] = Serial(None)\n",
    "      elif isinstance(obj, (list, tuple)):\n",
    "        layers[i] = Serial(obj)\n",
    "      else:\n",
    "        if not isinstance(obj, base.Layer):\n",
    "          raise ValueError(\n",
    "              f'Found nonlayer object ({obj}) in layers list: [{layers}]')\n",
    "      if layers[i].n_in == 0:\n",
    "        raise ValueError(\n",
    "            f'Sublayer with n_in = 0 not allowed in Parallel: {layers[i]}')\n",
    "    return layers\n",
    "\n",
    "  def _allot_to_sublayers(self, inputs):\n",
    "    \"\"\"Divides Parallel's inputs for use by the sublayers.\n",
    "    Args:\n",
    "      inputs: Tuple of ndarrays or ShapeDtype instances.\n",
    "    Returns:\n",
    "      A tuple that partitions this layer's inputs among its sublayers.\n",
    "      Sublayers that take one argument get that argument directly. All other\n",
    "      sublayers get a tuple of items.\n",
    "    \"\"\"\n",
    "    start, end = 0, 0\n",
    "    sub_inputs = []\n",
    "    for layer in self.sublayers:\n",
    "      n_in = layer.n_in\n",
    "      end = start + n_in\n",
    "      if n_in == 1:\n",
    "        sub_inputs.append(inputs[start])\n",
    "      else:\n",
    "        sub_inputs.append(inputs[start:end])\n",
    "      start = end\n",
    "    return tuple(sub_inputs)\n",
    "\n",
    "\n",
    "class Concatenate(base.Layer):\n",
    "  \"\"\"Concatenates n tensors into a single tensor.\"\"\"\n",
    "\n",
    "  def __init__(self, n_items=2, axis=-1):\n",
    "    name = 'Concatenate' if axis == -1 else f'Concatenate_axis{axis}'\n",
    "    super().__init__(n_in=n_items, name=name)\n",
    "    self._n_items = n_items\n",
    "    self._axis = axis\n",
    "\n",
    "  def forward(self, xs):\n",
    "    return jnp.concatenate(xs, self._axis)\n",
    "\n",
    "\n",
    "class Split(base.Layer):\n",
    "  \"\"\"Splits the input into n items along an axis.\"\"\"\n",
    "\n",
    "  def __init__(self, n_items=2, axis=-1):\n",
    "    super().__init__(n_out=n_items)\n",
    "    self._n_items = n_items\n",
    "    self._axis = axis\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    return tuple(jnp.split(inputs, self._n_items, self._axis))\n",
    "\n",
    "\n",
    "def _scan(f, xs, init_value, axis=0, remat=False):\n",
    "  \"\"\"Scans the f over the given axis of xs.\n",
    "  In pseudo-python, the scan function would look as follows:\n",
    "  def scan(f, xs, init_value, axis):\n",
    "    xs  = [xs[..., i, ...] for i in range(xs.shape[axis])]\n",
    "    cur_value = init_value\n",
    "    ys = []\n",
    "    for x in xs:\n",
    "      y, cur_value = f(x, cur_value)\n",
    "      ys.append(y)\n",
    "    return np.stack(ys, axis), cur_value\n",
    "  Args:\n",
    "    f: function (x, carry) -> (y, new_carry)\n",
    "    xs: tensor, x will be xs slices on axis\n",
    "    init_value: tensor, initial value of the carry-over\n",
    "    axis: int, the axis on which to slice xs\n",
    "    remat: whether to re-materialize f\n",
    "  Returns:\n",
    "    A pair (ys, last_value) as described above.\n",
    "  \"\"\"\n",
    "  def swapaxes(x):\n",
    "    transposed_axes = list(range(len(x.shape)))\n",
    "    transposed_axes[axis] = 0\n",
    "    transposed_axes[0] = axis\n",
    "    return jnp.transpose(x, axes=transposed_axes)\n",
    "  if axis != 0:\n",
    "    xs = fastmath.nested_map(swapaxes, xs)\n",
    "  def transposed_f(c, x):\n",
    "    y, d = f(x, c)\n",
    "    return d, y\n",
    "  if remat:\n",
    "    transposed_f = fastmath.remat(transposed_f)\n",
    "  last_value, ys = fastmath.scan(transposed_f, init_value, xs)\n",
    "  if axis != 0:\n",
    "    ys = fastmath.nested_map(swapaxes, ys)\n",
    "  return ys, last_value\n",
    "\n",
    "\n",
    "class Scan(base.Layer):\n",
    "  \"\"\"Applies a layer progressively/cumulatively to an axis-derived sequence.\n",
    "  Conceptually, this is a function from a list to a same-length list of partial\n",
    "  (cumulative) results. For instance, a list of values (`[1, 2, 3, 4, 5]`) can\n",
    "  transform to a list of cumulative sums (`[1, 3, 6, 10, 15]`). Functions for\n",
    "  the same concept are called `scan` in Scala, `scanl` in Haskell, and\n",
    "  `accumulate*` in Factor.\n",
    "  In more detail, we assume the layer takes a tuple of inputs of the following\n",
    "  form:\n",
    "    (input1, ..., inputN, carry1, ..., carryM)\n",
    "  and returns:\n",
    "    (output1, ..., outputK, new_carry1, ..., new_carryM)\n",
    "  The scanned version applies the layer iteratively to a tensor treating values\n",
    "  at the given axis as if they were a list. For example, to calculate all\n",
    "  sums of prefixes of a tensor, we can do this::\n",
    "    def add(x, carry):\n",
    "      def f(input, carry):\n",
    "        res = input + carry\n",
    "        return res, res  # output and carry are the same\n",
    "      return tl.Fn('add', f, n_out=2)\n",
    "    Scan(add)([1, 2, 3], 0) = [1, 3, 6], 6\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, layer, axis=0, n_carry=1, remat=False):\n",
    "    super().__init__(n_in=layer.n_in, n_out=layer.n_out)\n",
    "    self._sublayers = [layer]\n",
    "    self._n_carry = n_carry\n",
    "    self._axis = axis\n",
    "    self._remat = remat\n",
    "    self._weights = (None,)\n",
    "    self._state = (None,)\n",
    "\n",
    "  @property\n",
    "  def sublayer(self):\n",
    "    \"\"\"Returns the unique sublayer managed by this layer.\"\"\"\n",
    "    return self._sublayers[0]\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    weights = self.weights[0]\n",
    "    if isinstance(inputs, list):\n",
    "      inputs = tuple(inputs)  # so that inputs structure matches outputs\n",
    "    n_carry = self._n_carry\n",
    "    def scannable_fn(x, carry_and_state):  # pylint: disable=invalid-name\n",
    "      carry, state = carry_and_state\n",
    "      x_and_carry = x + carry if n_carry > 0 else x\n",
    "      res, new_state = self.sublayer.pure_fn(\n",
    "          x_and_carry, weights, state, self.rng, use_cache=True)\n",
    "      if n_carry > 0:\n",
    "        return (res[:-n_carry], (res[-n_carry:], new_state))\n",
    "      else:\n",
    "        return (res, ([], new_state))\n",
    "\n",
    "    if n_carry > 0:\n",
    "      xs = inputs[:-n_carry]  # Split input stack into inputs and carry.\n",
    "      init = (inputs[-n_carry:], self.state[0])\n",
    "    else:\n",
    "      xs, init = inputs, ([], self.state[0])\n",
    "    ys, (carry, new_state) = _scan(scannable_fn, xs, init,\n",
    "                                   axis=self._axis, remat=self._remat)\n",
    "    res = ys + carry if n_carry > 0 else ys\n",
    "    self.state = (new_state,)\n",
    "    return res  # Put outputs and carry back on stack.\n",
    "\n",
    "  def init_weights_and_state(self, input_signature):\n",
    "    n_carry = self._n_carry\n",
    "    if n_carry == 0:\n",
    "      if isinstance(input_signature, (list, tuple)):\n",
    "        layer_sig = [ShapeDtype(_shape_without_axis(x, self._axis), x.dtype)\n",
    "                     for x in input_signature]\n",
    "        layer_sig = tuple(layer_sig)\n",
    "      else:\n",
    "        layer_sig = ShapeDtype(_shape_without_axis(input_signature, self._axis),\n",
    "                               input_signature.dtype)\n",
    "      weights, state = self.sublayer.init(layer_sig)\n",
    "      self.state = (state,)\n",
    "      self.weights = (weights,)\n",
    "    else:\n",
    "      xs = input_signature[:-n_carry]\n",
    "      init = input_signature[-n_carry:]\n",
    "      xs_slices = [ShapeDtype(_shape_without_axis(x, self._axis), x.dtype)\n",
    "                   for x in xs]\n",
    "      layer_signature = tuple(xs_slices + list(init))\n",
    "      weights, state = self.sublayer.init(layer_signature, use_cache=True)\n",
    "      self.state = (state,)\n",
    "      self.weights = (weights,)\n",
    "\n",
    "\n",
    "class Cond(base.Layer):\n",
    "  \"\"\"Applies layers conditionally.\n",
    "  For parameters `cond`, `true`, and `false` runs the equivalent of `true(y)\n",
    "  if cond(x) else false(y)`, where `x` is `cond.n_in` elements from front of the\n",
    "  stack and `y` is the rest of the stack.\n",
    "  Exactly one of `true` and `false` functions is executed, so it can be used to\n",
    "  conditionally run long computations. The state of non-executed function is not\n",
    "  updated. Note that different branches may be executed on different devices\n",
    "  if `cond` returns different values on them.\n",
    "  `cond` must return exactly one element: a Boolean value.\n",
    "  `true` and `false` must have the same n_in, and the same n_out.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, cond, true, false, name=None):\n",
    "    super(Cond, self).__init__(name=name)\n",
    "\n",
    "    sublayers = [cond, true, false]\n",
    "    self._sublayers = sublayers\n",
    "    self._n_layers = len(sublayers)\n",
    "    self._cond = cond\n",
    "    self._true = true\n",
    "    self._false = false\n",
    "\n",
    "    if cond.n_out != 1:\n",
    "      raise ValueError(\n",
    "          'cond.n_out must be 1: cond:{}->{}'.format(cond.n_in, cond.n_out))\n",
    "    if true.n_in != false.n_in:\n",
    "      raise ValueError(\n",
    "          'true.n_in and false.n_in must be equal: true:{}->{} ; false:{}->{}'\n",
    "          .format(true.n_in, true.n_out, false.n_in, false.n_out))\n",
    "    if true.n_out != false.n_out:\n",
    "      raise ValueError(\n",
    "          'true.n_out and false.n_out must be equal: true:{}->{} ; false:{}->{}'\n",
    "          .format(true.n_in, true.n_out, false.n_in, false.n_out))\n",
    "\n",
    "    self._n_in = cond.n_in + true.n_in\n",
    "    self._n_out = true.n_out\n",
    "    self._weights = tuple(None for l in sublayers)\n",
    "    self._state = tuple(None for l in sublayers)\n",
    "\n",
    "  # pylint: disable=protected-access\n",
    "  def init_weights_and_state(self, input_signature):\n",
    "    weights = []\n",
    "    states = []\n",
    "    # In the code below, stack, inputs, and outputs are abstract (shapes and\n",
    "    # dtypes), but weights and states are non-abstract actual values.\n",
    "    stack = _make_tuple(input_signature)\n",
    "\n",
    "    # Inputs/outputs of `cond`.\n",
    "    inputs = _inputs_from_stack(self._cond, stack)\n",
    "    weights_or_cache_marker, state_or_cache_marker = (\n",
    "        self._cond.init(inputs, use_cache=True))\n",
    "    weights.append(weights_or_cache_marker)\n",
    "    states.append(state_or_cache_marker)\n",
    "    self._cond._forward_abstract(inputs)\n",
    "    stack = _make_tuple(_outputs_onto_stack(self._cond, [], stack))\n",
    "\n",
    "    # Inputs/outputs of `true` and `false`.\n",
    "    for sublayer in [self._true, self._false]:\n",
    "      inputs = _inputs_from_stack(sublayer, stack)\n",
    "      weights_or_cache_marker, state_or_cache_marker = (\n",
    "          sublayer.init(inputs, use_cache=True))\n",
    "      weights.append(weights_or_cache_marker)\n",
    "      states.append(state_or_cache_marker)\n",
    "\n",
    "    self.state = states\n",
    "    self.weights = weights\n",
    "    # pylint: enable=protected-access\n",
    "\n",
    "  def _validate_forward_inputs(self, xs):\n",
    "    xs = _make_tuple(xs)\n",
    "    if len(xs) < self.n_in:\n",
    "      raise ValueError(\n",
    "          f'Number of inputs ({len(xs)}) to Cond.forward less than n_in '\n",
    "          f'({self.n_in}).')\n",
    "\n",
    "  def forward(self, xs):\n",
    "    # TODO(jaszczur): modify; it's a copy from SkippingSerial\n",
    "    self._validate_forward_inputs(xs)\n",
    "    layers_state = self.state\n",
    "    # Get 3 rngs, one for each layer.\n",
    "    rngs = _split_rngs(self.rng, 3)\n",
    "\n",
    "    # Prepare the stack and do some safety checks as in the parent class.\n",
    "    stack = _make_tuple(xs)\n",
    "    weights = self.weights\n",
    "    if len(weights) != 3:\n",
    "      raise ValueError('number of weights ({}) not equal to 3'\n",
    "                       .format(len(weights)))\n",
    "    if len(layers_state) != 3:\n",
    "      raise ValueError('length of state ({}) not equal to 3'\n",
    "                       .format(len(layers_state)))\n",
    "\n",
    "    def true_func(t):\n",
    "      outputs, new_true_state = self._true.pure_fn(\n",
    "          t[0][0], t[1][0], t[2][0], t[3][0])\n",
    "      # t[2][1] is old_false_state which is not changing if true is executed.\n",
    "      return outputs, (new_true_state, t[2][1])\n",
    "    def false_func(t):\n",
    "      outputs, new_false_state = self._false.pure_fn(\n",
    "          t[0][1], t[1][1], t[2][1], t[3][1])\n",
    "      # t[2][1] is old_true_state, which is not changing if false is executed.\n",
    "      return outputs, (t[2][0], new_false_state)\n",
    "\n",
    "    cond_inputs = _inputs_from_stack(self._cond, xs)\n",
    "    cond_output, s = self._cond.pure_fn(cond_inputs, self.weights[0],\n",
    "                                        self.state[0], rngs[0], use_cache=True)\n",
    "    stack = _outputs_onto_stack(self._cond, [], stack)\n",
    "    self._cond.state = s\n",
    "\n",
    "    outputs, both_states = fastmath.cond(\n",
    "        cond_output,\n",
    "        true_func,\n",
    "        false_func,\n",
    "        [(stack, stack),\n",
    "         (self.weights[1], self.weights[2]),\n",
    "         (self.state[1], self.state[2]),\n",
    "         (rngs[1], rngs[2])]\n",
    "    )\n",
    "    stack = _outputs_onto_stack(self._cond, [], stack)\n",
    "\n",
    "    # We don't know which (`true` or `false`) branch was run, but both of them\n",
    "    # are adding (n_out) and removing (n_in) the same number of elements of the\n",
    "    # stack (this was checked in __init__). _outputs_onto_stack just uses the\n",
    "    # layer's n_in and n_out, so we can pass either `true` or `false` to it.\n",
    "    # Note that `outputs` is the actual output of `true` or `false` branch,\n",
    "    # whichever was run, and we add it to the stack in any case.\n",
    "    stack = _outputs_onto_stack(self._true, outputs, stack)\n",
    "    self._true.state = both_states[0]\n",
    "    self._false.state = both_states[1]\n",
    "    return _make_singleitem_or_original(stack)\n",
    "\n",
    "\n",
    "# pylint: disable=invalid-name\n",
    "def Branch(*layers, name='Branch'):\n",
    "  \"\"\"Combinator that applies a list of layers in parallel to copies of inputs.\n",
    "  Each layer in the input list is applied to as many inputs from the stack\n",
    "  as it needs, and their outputs are successively combined on stack.\n",
    "  For example, suppose one has three layers:\n",
    "    - F: 1 input, 1 output\n",
    "    - G: 3 inputs, 1 output\n",
    "    - H: 2 inputs, 2 outputs (h1, h2)\n",
    "  Then Branch(F, G, H) will take 3 inputs and give 4 outputs:\n",
    "    - inputs: a, b, c\n",
    "    - outputs: F(a), G(a, b, c), h1, h2    where h1, h2 = H(a, b)\n",
    "  As an important special case, a None argument to Branch acts as if it takes\n",
    "  one argument, which it leaves unchanged. (It acts as a one-arg no-op.)\n",
    "  Args:\n",
    "    *layers: List of layers.\n",
    "    name: Descriptive name for this layer.\n",
    "  Returns:\n",
    "    A branch layer built from the given sublayers.\n",
    "  \"\"\"\n",
    "  if len(layers) == 1:\n",
    "    return layers[0]\n",
    "  parallel_layer = Parallel(*layers)\n",
    "  indices = [list(range(layer.n_in)) for layer in parallel_layer.sublayers]\n",
    "  return Serial(Select(_deep_flatten(indices)), parallel_layer,\n",
    "                name=name, sublayers_to_print=layers)\n",
    "\n",
    "\n",
    "def Residual(*layers, shortcut=None):\n",
    "  \"\"\"Wraps a series of layers with a residual connection.\n",
    "  Args:\n",
    "    *layers: One or more layers, to be applied in series.\n",
    "    shortcut: If None (the usual case), the Residual layer computes the\n",
    "        element-wise sum of the stack-top input with the output of the layer\n",
    "        series. If specified, the `shortcut` layer applies to a copy of the\n",
    "        inputs and (elementwise) adds its output to the output from the main\n",
    "        layer series.\n",
    "  Returns:\n",
    "      A layer representing a residual connection paired with a layer series.\n",
    "  \"\"\"\n",
    "  layers = _ensure_flat(layers)\n",
    "  layer = layers[0] if len(layers) == 1 else Serial(layers)\n",
    "  # TODO(jonni): Should we require layer.n_out = 1 and shortcut.n_out = 1?\n",
    "  return Serial(\n",
    "      Branch(shortcut, layer),\n",
    "      Add(),  # pylint: disable=no-value-for-parameter\n",
    "  )\n",
    "\n",
    "\n",
    "def Select(indices, n_in=None, name=None):\n",
    "  \"\"\"Copies, reorders, or deletes stack elements according to `indices`.\n",
    "  Args:\n",
    "    indices: A list or tuple of 0-based indices to select elements relative to\n",
    "        the top of the stack.\n",
    "    n_in: Number of input elements to pop from the stack, and replace with\n",
    "        those specified by `indices`. If not specified, its value will be\n",
    "        calculated as `max(indices) + 1`.\n",
    "    name: Descriptive name for this layer.\n",
    "  Returns:\n",
    "    Tensors, matching the number selected (`n_out = len(indices)`).\n",
    "    Specifically:\n",
    "      - n_out = 0: an empty tuple\n",
    "      - n_out = 1: one tensor (NOT wrapped in a tuple)\n",
    "      - n_out > 1: a tuple of tensors, with n_out items\n",
    "  \"\"\"\n",
    "  if n_in is None:\n",
    "    n_in = max(indices) + 1\n",
    "  if name is None:\n",
    "    name = f'Select{indices}'.replace(' ', '')\n",
    "\n",
    "  def select(xs):  # pylint: disable=invalid-name\n",
    "    if not isinstance(xs, (tuple, list)):\n",
    "      xs = (xs,)\n",
    "    selected = tuple(xs[i] for i in indices)\n",
    "    return selected[0] if len(selected) == 1 else selected\n",
    "\n",
    "  return base.PureLayer(select, n_in=n_in, n_out=len(indices), name=name)\n",
    "\n",
    "\n",
    "def Drop():\n",
    "  \"\"\"Drops the top stack element.\"\"\"\n",
    "  return Fn('Drop', lambda x: (), n_out=0)\n",
    "\n",
    "\n",
    "def Dup():\n",
    "  \"\"\"Duplicates (copies) the top element on the data stack.\"\"\"\n",
    "  return Fn('Dup', lambda x: (x, x), n_out=2)\n",
    "\n",
    "\n",
    "def Swap():\n",
    "  \"\"\"Swaps the top two stack elements.\"\"\"\n",
    "  return Fn('Swap', lambda x0, x1: (x1, x0), n_out=2)\n",
    "\n",
    "\n",
    "def SerialWithSideOutputs(layers, n_side_outputs=1):\n",
    "  \"\"\"Serial layer with side outputs.\n",
    "  This layer makes it easier to manage the stack when layers have side outputs.\n",
    "  In the simplest case of layers with n_in=1, n_out=2 and with\n",
    "  n_side_outputs=1, this layer runs the following computation on x::\n",
    "    side_outputs = []\n",
    "    for i in range(len(layers)):\n",
    "      x, side_output = layers[i](x)\n",
    "      side_outputs.append(side_output)\n",
    "    return [x] + side_outputs\n",
    "  In the general case of layers with variable n_in and n_out and\n",
    "  n_side_outputs being a list of N integers, it does the following::\n",
    "    side_outputs = []\n",
    "    for i in range(N):\n",
    "      res = layer[i](cur_stack)  # remove n_in from stack\n",
    "      cur_stack.append(res[:n_side_outputs[i]])  # put back some on stack\n",
    "      side_outputs.extend(res[n_side_outputs:])\n",
    "    return cur_stack + side_outputs\n",
    "  Args:\n",
    "    layers: a list of layers to execute\n",
    "    n_side_outputs: an int or a list of ints, how many outputs of each layer\n",
    "        to put aside\n",
    "  Returns:\n",
    "    A layer that performs the above computation.\n",
    "  \"\"\"\n",
    "  if isinstance(n_side_outputs, int):\n",
    "    n_side_outputs = [n_side_outputs] * len(layers)\n",
    "\n",
    "  # Calculate the n_in for this layer.\n",
    "  running_max = 0\n",
    "  running_total = 0\n",
    "  for layer, n_side_output in zip(layers, n_side_outputs):\n",
    "    running_total += layer.n_in\n",
    "    running_max = max(running_max, running_total)\n",
    "    running_total -= layer.n_out - n_side_output\n",
    "  n_in = running_max\n",
    "\n",
    "  # Create the list of layers to run serially.\n",
    "  cur_stack_size = n_in\n",
    "  serial_layers = []\n",
    "  for layer, n_side_output in zip(layers, n_side_outputs):\n",
    "    serial_layers.append(layer)\n",
    "    cur_stack_size += layer.n_out - layer.n_in\n",
    "    # Indices to move n_side_outputs to the back of the stack.\n",
    "    # Don't touch first n_out - n_side_outputs.\n",
    "    move_back_indices = list(range(layer.n_out - n_side_output))\n",
    "    # Then comes the rest of the stack that we're not moving.\n",
    "    move_back_indices += [i + layer.n_out\n",
    "                          for i in range(cur_stack_size - layer.n_out)]\n",
    "    # Finally the indices we move.\n",
    "    move_back_indices += [i + layer.n_out - n_side_output\n",
    "                          for i in range(n_side_output)]\n",
    "    # Swap them on stack.\n",
    "    serial_layers.append(Select(move_back_indices))\n",
    "\n",
    "  return Serial(serial_layers)\n",
    "\n",
    "\n",
    "def FlattenList():\n",
    "  \"\"\"Flatten lists.\"\"\"\n",
    "  # TODO(jonni): Consider renaming layer to DeepFlatten.\n",
    "  return Fn('FlattenList', lambda x: tuple(_deep_flatten(x)))\n",
    "\n",
    "\n",
    "def Add():\n",
    "  \"\"\"Adds two tensors.\"\"\"\n",
    "  return Fn('Add', lambda x0, x1: x0 + x1)\n",
    "\n",
    "\n",
    "def SubtractTop():\n",
    "  \"\"\"Subtracts the first tensor from the second.\"\"\"\n",
    "  return Fn('SubtractTop', lambda x0, x1: x1 - x0)\n",
    "\n",
    "\n",
    "def Multiply():\n",
    "  \"\"\"Multiplies two tensors.\"\"\"\n",
    "  return Fn('Multiply', lambda x0, x1: x0 * x1)\n",
    "\n",
    "\n",
    "def Gate():\n",
    "  \"\"\"Returns a gating layer on a (memory, gate, candidate) tuple.\n",
    "  Final update is memory * gate + (1 - gate) * candidate\n",
    "  This gating equation may also be referred to as Highway Network.\n",
    "  Highway Networks: https://arxiv.org/abs/1505.00387\n",
    "  \"\"\"\n",
    "  return Fn('Gate', lambda m, g, c: g * m + (1.0 - g) * c)\n",
    "\n",
    "\n",
    "class Cache(base.Layer):\n",
    "  \"\"\"Applies a layer on the first run and returns the outputs on next calls.\"\"\"\n",
    "\n",
    "  def __init__(self, layer):\n",
    "    super().__init__(n_in=layer.n_in, n_out=layer.n_out)\n",
    "    self._sublayers = [layer]\n",
    "\n",
    "  @property\n",
    "  def sublayer(self):\n",
    "    \"\"\"Returns the unique sublayer managed by this layer.\"\"\"\n",
    "    return self._sublayers[0]\n",
    "\n",
    "  @property\n",
    "  def state(self):\n",
    "    \"\"\"Returns a tuple containing this layer's state; may be empty.\"\"\"\n",
    "    return self._state\n",
    "\n",
    "  @state.setter\n",
    "  def state(self, state):\n",
    "    \"\"\"Recursively sets state on this layer and all sublayers.\"\"\"\n",
    "    if isinstance(state, dict) and state == base.GET_STATE_FROM_CACHE:\n",
    "      return\n",
    "    self._state = state\n",
    "    self.sublayer.state = state[1]\n",
    "\n",
    "  def init_weights_and_state(self, input_signature):\n",
    "    weights, layer_state = self.sublayer.init(input_signature, use_cache=True)\n",
    "    self.state = ((), layer_state)\n",
    "    self._weights = (weights,)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    state, weights = self.state, self.weights[0]\n",
    "    if state[0] is ():  # pylint: disable=literal-comparison\n",
    "      res, layer_state = self.sublayer.pure_fn(\n",
    "          inputs, weights, state[1], self.rng)\n",
    "      self.state = (res, layer_state)\n",
    "      return res\n",
    "    else:\n",
    "      return state[0]\n",
    "\n",
    "\n",
    "class BatchLeadingAxes(base.Layer):\n",
    "  \"\"\"Applies a layer after flattening all but n_last_axes_to_keep to batch.\n",
    "  This can be used to make layers accept an arbitrary number of leading\n",
    "  axes (dimensions) as batch. For example, a Convolution layer may normally\n",
    "  only operate on tensors of shape [B, W, H, C]. In this case, the layer\n",
    "      BatchLeadingAxes(Convolution(), n_last_axes_to_keep=3)\n",
    "  will operate on any tensor [..., W, H, C] and treat the leading axes as batch.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, layer, n_last_axes_to_keep=1):\n",
    "    super().__init__(n_in=layer.n_in, n_out=layer.n_out)\n",
    "    self._sublayers = [layer]\n",
    "    self._n_last_axes_to_keep = n_last_axes_to_keep\n",
    "    self._weights = (None,)\n",
    "    self._state = (None,)\n",
    "\n",
    "  @property\n",
    "  def sublayer(self):\n",
    "    \"\"\"Returns the unique sublayer managed by this layer.\"\"\"\n",
    "    return self._sublayers[0]\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    batched_axes_shape = list(inputs.shape[:-self._n_last_axes_to_keep])\n",
    "    batched_shape = [-1] + list(inputs.shape[-self._n_last_axes_to_keep:])\n",
    "    inputs = jnp.reshape(inputs, batched_shape)\n",
    "    res, layer_state = self.sublayer.pure_fn(\n",
    "        inputs, self.weights[0], self.state[0], self.rng)\n",
    "    self.state = (layer_state,)\n",
    "    return jnp.reshape(res, batched_axes_shape + list(res.shape[1:]))\n",
    "\n",
    "  def init_weights_and_state(self, input_signature):\n",
    "    weights, layer_state = self.sublayer.init(input_signature, use_cache=True)\n",
    "    self.state = (layer_state,)\n",
    "    self.weights = (weights,)\n",
    "\n",
    "\n",
    "# All module-private helper functions are below.\n",
    "# pylint: disable=invalid-name\n",
    "\n",
    "\n",
    "def _deep_flatten(items):\n",
    "  \"\"\"Returns a list of objects, flattening sublists/subtuples along the way.\n",
    "  Example: _deep_flatten([1, (2, 3, (4, 5), [6, 7]), [[[8]]]]) would return\n",
    "  the list [1, 2, 3, 4, 5, 6, 7, 8].\n",
    "  Args:\n",
    "    items: An iterable. If elements of this iterable are lists or tuples, they\n",
    "        will be (recursively) flattened until non-list non-tuple objects are\n",
    "        reached.\n",
    "  Returns:\n",
    "    A list of non-list, non-tuple objects.\n",
    "  \"\"\"\n",
    "  def _flat_gen(xs):\n",
    "    for x in xs:\n",
    "      if isinstance(x, (list, tuple)):\n",
    "        for y in _flat_gen(x):\n",
    "          yield y\n",
    "      else:\n",
    "        yield x\n",
    "  return list(_flat_gen(items))\n",
    "\n",
    "\n",
    "def _ensure_sublayers(layers):\n",
    "  \"\"\"Ensures that elements in a layer list are layers.\n",
    "  Args:\n",
    "    layers: A tuple or list whose elements can each be a layer, tuple, or list,\n",
    "        and so on recursively.\n",
    "  Returns:\n",
    "    An analogous collection of layers in which embedded layer lists are\n",
    "    wrapped in Serial layer instances.\n",
    "  \"\"\"\n",
    "  if not layers:  # None or an empty list can signal a no-op.\n",
    "    return Serial(None)  # no-op, but still handles shapes and initialization\n",
    "  elif isinstance(layers, (list, tuple)):\n",
    "    sublayers_not_lists = []\n",
    "    for layer in layers:\n",
    "      sublayers_not_lists.append(\n",
    "          Serial(layer) if isinstance(layer, (list, tuple)) else layer)\n",
    "    return sublayers_not_lists\n",
    "  else:\n",
    "    raise TypeError(type(layers))\n",
    "\n",
    "\n",
    "def _split_rngs(rng, n_copies):\n",
    "  if rng is None:\n",
    "    return (None,) * n_copies\n",
    "  return fastmath.random.split(rng, n_copies)\n",
    "\n",
    "\n",
    "def _inputs_from_stack(layer, stack, n_in=None):\n",
    "  \"\"\"Returns the correct number/format of inputs for the given layer.\"\"\"\n",
    "  if n_in is None:\n",
    "    n_in = layer.n_in\n",
    "  stack = _make_tuple(stack)\n",
    "  return _make_singleitem_or_original(stack[:n_in])\n",
    "\n",
    "\n",
    "def _outputs_onto_stack(layer, outputs, stack, n_in=None, n_out=None):\n",
    "  \"\"\"\"Returns the new stack after outputs have been pushed onto it.\"\"\"\n",
    "  if n_in is None:\n",
    "    n_in = layer.n_in\n",
    "  if n_out is None:\n",
    "    n_out = layer.n_out\n",
    "  outputs = _make_tuple(outputs)\n",
    "  stack = _make_tuple(stack)\n",
    "  return _make_singleitem_or_original(outputs + stack[n_in:])\n",
    "\n",
    "\n",
    "def _make_tuple(xs):\n",
    "  \"\"\"Returns a tuple from a list, a tuple, or a single element.\"\"\"\n",
    "  if isinstance(xs, (list, tuple)):\n",
    "    return tuple(xs)\n",
    "  else:\n",
    "    return (xs,)\n",
    "\n",
    "\n",
    "def _make_singleitem_or_original(xs):\n",
    "  \"\"\"Returns a single element if possible, or the original list/tuple if not.\"\"\"\n",
    "  if isinstance(xs, (list, tuple)) and len(xs) == 1:\n",
    "    return xs[0]\n",
    "  else:\n",
    "    return xs\n",
    "\n",
    "\n",
    "def _shape_without_axis(x, axis):\n",
    "  return x.shape[:axis] + x.shape[axis + 1:]\n",
    "\n",
    "\n",
    "def _ensure_flat(layers):\n",
    "  \"\"\"Ensures that layers is a single flat list of Layer instances.\"\"\"\n",
    "  if len(layers) == 1 and layers[0] is None:\n",
    "    layers = ()\n",
    "  else:\n",
    "    layers = _deep_flatten(layers)\n",
    "  for obj in layers:\n",
    "    if not isinstance(obj, base.Layer):\n",
    "      raise ValueError(\n",
    "          f'Found nonlayer object ({obj}) in layers: {layers}')\n",
    "  return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyResidual(layer):\n",
    "    return tl.Serial(\n",
    "        ### START CODE HERE ###\n",
    "        # tl.----,\n",
    "        # tl.----,\n",
    "        ### END CODE HERE ###\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1]), 'n', 'm']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets Try it\n",
    "mr = MyResidual(bl_add1)\n",
    "x = np.array([1])\n",
    "mr([x, n, m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fl = tl.Fn(\"F\", lambda x0: (2 * x0), n_out=1)\n",
    "Gl = tl.Fn(\"G\", lambda x0: (10 * x0), n_out=1)\n",
    "x1 = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "resfg = tl.Serial(\n",
    "    ### START CODE HERE ###\n",
    "    # None,  #Fl    # x + F(x)\n",
    "    # None,  #Gl    # x + F(x) + G(x + F(x)) etc\n",
    "    # None,  #Fl\n",
    "    # None,  #Gl\n",
    "    ### END CODE HERE ###\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1]), 'n', 'm']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try it\n",
    "resfg([x1, n, m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reversible Residual Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refm = trax.models.reformer.ReformerLM(\n",
    "    vocab_size=33000, n_layers=2, mode=\"train\"  # Add more options.\n",
    ")\n",
    "refm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([1])\n",
    "x2 = np.array([5])\n",
    "# Dup() duplicates the Top of Stack and returns the stack\n",
    "dl = tl.Dup()\n",
    "dl(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2020 The Trax Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Lint as: python3\n",
    "\"\"\"Implementations of reversible layers.\"\"\"\n",
    "\n",
    "from trax import fastmath\n",
    "from trax.layers import base\n",
    "from trax.layers import combinators as cb\n",
    "\n",
    "# pylint: disable=protected-access\n",
    "_inputs_from_stack = cb._inputs_from_stack\n",
    "_outputs_onto_stack = cb._outputs_onto_stack\n",
    "_split_rngs = cb._split_rngs\n",
    "# pylint: enable=protected-access\n",
    "\n",
    "\n",
    "class ReversibleLayer(base.Layer):\n",
    "  \"\"\"Reversible Layer.\"\"\"\n",
    "\n",
    "  def reverse(self, output, weights=(), state=(), new_state=(), rng=None):\n",
    "    \"\"\"Reverse this layer: compute input given output.\"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def reverse_and_grad(self, output, grad, weights=(), state=(), new_state=(),\n",
    "                       rng=None):\n",
    "    \"\"\"Backward pass: computes the inverse of a layer and propagates gradients.\n",
    "    While you may choose to only implement reverse, some layers implement this\n",
    "    function directly as computation may be shared between reversing and\n",
    "    computing gradients.\n",
    "    Args:\n",
    "      output: Output activations; can be a (possibly nested) tuple.\n",
    "      grad: gradient signal (cotangent) computed based on subsequent layers.\n",
    "        The structure and shape must match the output.\n",
    "      weights: layer weights\n",
    "      state: start state\n",
    "      new_state: updated state computed by the forward pass\n",
    "      rng: Single-use random number generator (JAX PRNG key).\n",
    "    Returns:\n",
    "      A tuple (x, (x_grad, weights_grad)), where x is the reconstructed input,\n",
    "      x_grad is the gradient signal for the input, and weights_grad is the\n",
    "      gradient signal for the weights.\n",
    "    \"\"\"\n",
    "    def _do_forward(x, weights):\n",
    "      old_weights, old_state, old_rng = self.weights, self.state, self._rng\n",
    "      self.state, self._rng = state, rng\n",
    "      self.weights = weights\n",
    "      res = self.forward(x)\n",
    "      self.weights, self.state, self._rng = old_weights, old_state, old_rng\n",
    "      return res\n",
    "\n",
    "    reconstructed_x = self.reverse(output, weights, state, new_state, rng)\n",
    "    _, vjpfun = fastmath.vjp(_do_forward, reconstructed_x, weights)\n",
    "    x_weights_grad = vjpfun(grad)\n",
    "    return reconstructed_x, x_weights_grad\n",
    "\n",
    "  @property\n",
    "  def has_backward(self):\n",
    "    return True\n",
    "\n",
    "  def backward(self, inputs, output, grad, weights, state, new_state, rng):\n",
    "    del inputs\n",
    "    _, inputs_weights_grad = (\n",
    "        self.reverse_and_grad(output, grad, weights, state, new_state, rng))\n",
    "    return inputs_weights_grad\n",
    "\n",
    "\n",
    "class ReversibleSwap(ReversibleLayer):\n",
    "  \"\"\"Swap the first two element on the stack.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__(n_in=2, n_out=2)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    x0, x1 = inputs\n",
    "    return x1, x0\n",
    "\n",
    "  def reverse(self, output, weights=(), state=(), new_state=(), rng=None):\n",
    "    del state, new_state, rng, weights\n",
    "    # Swap is its own inverse, except that reverse doesn't return the state.\n",
    "    return self.forward(output)\n",
    "\n",
    "\n",
    "class ReversibleSerial(ReversibleLayer, cb.Serial):\n",
    "  \"\"\"A reversible version of tl.Serial (requires reversible sub-layers).\"\"\"\n",
    "\n",
    "  def __init__(self, *layers):\n",
    "    super().__init__(*layers)\n",
    "  # def __init__(self, *layers):  # pylint: disable=super-init-not-called\n",
    "  #   cb.Serial.__init__(self, layers)\n",
    "\n",
    "    # Note that sublayers has already been flattened to remove nested lists.\n",
    "    for i, layer in enumerate(self.sublayers):\n",
    "      if not isinstance(layer, ReversibleLayer):\n",
    "        raise ValueError(\n",
    "            'Sub-layer {} of ReversibleSerial is not reversible: {}'.format(\n",
    "                i, layer))\n",
    "\n",
    "  def reverse(self, output, weights=(), state=(), new_state=(), rng=None):\n",
    "    rngs = (None,) * self._n_layers\n",
    "    if rng is not None:\n",
    "      rngs = fastmath.random.split(rng, self._n_layers)\n",
    "\n",
    "    stack = output\n",
    "    for layer, p, s, ns, rng in reversed(list(zip(\n",
    "        self.sublayers, weights, state, new_state, rngs))):\n",
    "      layer_val = _inputs_from_stack(layer, stack, layer.n_out)\n",
    "      layer_val = layer.reverse(layer_val, p, s, ns, rng=rng)\n",
    "      stack = _outputs_onto_stack(\n",
    "          layer, layer_val, stack, layer.n_out, layer.n_in)\n",
    "\n",
    "    return stack\n",
    "\n",
    "  def reverse_and_grad(self, output, grad, weights=(), state=(), new_state=(),\n",
    "                       rng=None):\n",
    "    rngs = (None,) * self._n_layers\n",
    "    if rng is not None:\n",
    "      rngs = fastmath.random.split(rng, self._n_layers)\n",
    "\n",
    "    stack = output\n",
    "    stack_grad = grad\n",
    "    weights_grad = []\n",
    "    for layer, p, s, ns, rng in reversed(list(zip(\n",
    "        self.sublayers, weights, state, new_state, rngs))):\n",
    "      layer_val = _inputs_from_stack(layer, stack, layer.n_out)\n",
    "      layer_ct = _inputs_from_stack(layer, stack_grad, layer.n_out)\n",
    "      layer_val, layer_ct = layer.reverse_and_grad(\n",
    "          layer_val, layer_ct, p, s, ns, rng=rng)\n",
    "      layer_ct, p_ct = layer_ct\n",
    "      weights_grad.insert(0, p_ct)\n",
    "      stack = _outputs_onto_stack(\n",
    "          layer, layer_val, stack, layer.n_out, layer.n_in)\n",
    "      stack_grad = _outputs_onto_stack(\n",
    "          layer, layer_ct, stack_grad, layer.n_out, layer.n_in)\n",
    "\n",
    "    return stack, (stack_grad, tuple(weights_grad))\n",
    "\n",
    "\n",
    "class ReversibleHalfResidual(ReversibleLayer):\n",
    "  \"\"\"Half of a RevNet-style residual that optionally performs attention.\n",
    "  When attention_layer is None, this layer has the signature ::\n",
    "      [accumulator, *context] -> [accumulator + f(context), *context]\n",
    "  The attention_layer must be an instance of EfficientAttentionBase or one of\n",
    "  its subclasses (see efficient_attention.py), or None.\n",
    "  Attention is special-cased for the following two reasons:\n",
    "  - LSH attention needs to save bucket assignments from the forward pass to the\n",
    "    backward pass, for training stability. This requires special-casing it.\n",
    "  - We can call attention_layer.forward_and_or_backward to compute its output\n",
    "    (needed for inverting a reversible residual layer) while simultaneously\n",
    "    performing the backward pass. Sharing computation between these two\n",
    "    operations improves training speed.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, *residual_layers, attention_layer=None):\n",
    "    super().__init__()\n",
    "\n",
    "    self.compute_residual = cb.Serial(*residual_layers)\n",
    "    self.attention_layer = attention_layer\n",
    "\n",
    "    if self.attention_layer is None:\n",
    "      self._sublayers = (self.compute_residual,)\n",
    "    else:\n",
    "      assert hasattr(attention_layer, 'forward_and_or_backward')\n",
    "      self._sublayers = (self.compute_residual, self.attention_layer)\n",
    "\n",
    "    running_max = 0\n",
    "    running_total = 0\n",
    "    for layer in self._sublayers:\n",
    "      running_total += layer.n_in\n",
    "      running_max = max(running_max, running_total)\n",
    "      running_total -= layer.n_out\n",
    "    self._n_in = self._n_out = running_max + 1\n",
    "\n",
    "  def forward(self, xs):\n",
    "    rngs = _split_rngs(self.rng, len(self.sublayers))\n",
    "    accumulator, *context = xs\n",
    "    stack = context = tuple(context)\n",
    "    new_state = []\n",
    "    for layer, w, s, rng in zip(self.sublayers, self.weights, self.state, rngs):\n",
    "      inputs = _inputs_from_stack(layer, stack)\n",
    "      outputs, s = layer.pure_fn(inputs, w, s, rng)\n",
    "      stack = _outputs_onto_stack(layer, outputs, stack)\n",
    "      new_state.append(s)\n",
    "    residual = stack[0] if isinstance(stack, (tuple, list)) else stack\n",
    "\n",
    "    output = accumulator + residual\n",
    "    stack = (output,) + context\n",
    "    self.state = tuple(new_state)\n",
    "    return stack\n",
    "\n",
    "  def reverse(self, output, weights=(), state=(), new_state=(), rng=None):\n",
    "    raise NotImplementedError('Only reverse_and_grad is actually used.')\n",
    "\n",
    "  def reverse_and_grad(self, output, ct, weights=(), state=(), new_state=(),\n",
    "                       rng=None):\n",
    "    rngs = _split_rngs(rng, len(self.sublayers))\n",
    "\n",
    "    accumulator_output, *context = output\n",
    "    context = tuple(context)\n",
    "    accumulator_output_ct, *context_ct = ct\n",
    "    context_ct = tuple(context_ct)\n",
    "\n",
    "    # Forward pass through self.compute_residual. Outputs that will not receive\n",
    "    # a gradient signal from subsequent layers are moved to aux.\n",
    "    def call_compute_residual(x, weights):\n",
    "      res, _ = self.compute_residual.pure_fn(\n",
    "          x, weights=weights, state=state[0], rng=rngs[0])\n",
    "      if not isinstance(res, (tuple, list)):\n",
    "        return res, None\n",
    "      else:\n",
    "        n_differentiable = 1\n",
    "        if self.attention_layer is not None:\n",
    "          n_differentiable = min(len(res), self.attention_layer.n_in)\n",
    "        return res[:n_differentiable], res[n_differentiable:]\n",
    "\n",
    "    stack = context\n",
    "    inputs = _inputs_from_stack(self.compute_residual, stack)\n",
    "    outputs, compute_residual_vjpfun, outputs_aux = fastmath.vjp(\n",
    "        call_compute_residual, inputs, weights[0], has_aux=True)\n",
    "    if outputs_aux is not None:\n",
    "      n_differentiable_outputs = len(outputs)\n",
    "      outputs = outputs + outputs_aux\n",
    "    stack = _outputs_onto_stack(self.compute_residual, outputs, stack)\n",
    "\n",
    "    stack_ct = accumulator_output_ct\n",
    "    if self.attention_layer is None:\n",
    "      residual = stack[0] if isinstance(stack, (tuple, list)) else stack\n",
    "    else:\n",
    "      inputs = _inputs_from_stack(self.attention_layer, stack)\n",
    "      (residual, _, attn_inputs_ct, attn_weights_ct\n",
    "      ) = self.attention_layer.forward_and_or_backward(\n",
    "          inputs, weights[1], new_state[1], rngs[1],\n",
    "          output_grad=accumulator_output_ct,\n",
    "          compute_output=True, update_state=False)\n",
    "      stack_ct = _outputs_onto_stack(\n",
    "          self.attention_layer, attn_inputs_ct, stack_ct,\n",
    "          self.attention_layer.n_out, self.attention_layer.n_in)\n",
    "\n",
    "    compute_residual_ct = _inputs_from_stack(\n",
    "        self.compute_residual, stack_ct, self.compute_residual.n_out)\n",
    "    if outputs_aux is not None:\n",
    "      if not isinstance(compute_residual_ct, (tuple, list)):\n",
    "        compute_residual_ct = (compute_residual_ct,)\n",
    "      compute_residual_ct = compute_residual_ct[:n_differentiable_outputs]\n",
    "      assert len(compute_residual_ct) == n_differentiable_outputs\n",
    "    (compute_residual_inputs_ct, compute_residual_weights_ct\n",
    "    ) = compute_residual_vjpfun(compute_residual_ct)\n",
    "    stack_ct = _outputs_onto_stack(\n",
    "        self.compute_residual, compute_residual_inputs_ct, stack_ct,\n",
    "        self.compute_residual.n_out, self.compute_residual.n_in)\n",
    "    if not isinstance(stack_ct, (tuple, list)):\n",
    "      stack_ct = (stack_ct,)\n",
    "    stack_ct = (accumulator_output_ct,) + fastmath.nested_map_multiarg(\n",
    "        lambda x, y: x+y, context_ct[:len(stack_ct)], stack_ct\n",
    "        ) + context_ct[len(stack_ct):]\n",
    "\n",
    "    reconstructed_x = accumulator_output - residual\n",
    "    stack = (reconstructed_x,) + context\n",
    "    if self.attention_layer is None:\n",
    "      weights_ct = (compute_residual_weights_ct,)\n",
    "    else:\n",
    "      weights_ct = (compute_residual_weights_ct, attn_weights_ct)\n",
    "    return stack, (stack_ct, weights_ct)\n",
    "\n",
    "  # pylint: disable=protected-access\n",
    "  def init_weights_and_state(self, input_signature):\n",
    "    stack = input_signature[1:]\n",
    "    if len(stack) == 1:\n",
    "      stack = stack[0]\n",
    "\n",
    "    inputs = _inputs_from_stack(self.compute_residual, stack)\n",
    "    weights, state = self.compute_residual.init(inputs)\n",
    "    outputs, _ = self.compute_residual._forward_abstract(inputs)\n",
    "    stack = _outputs_onto_stack(self.compute_residual, outputs, stack)\n",
    "\n",
    "    if self.attention_layer is None:\n",
    "      self.state = (state,)\n",
    "      self.weights = (weights,)\n",
    "    else:\n",
    "      inputs = _inputs_from_stack(self.attention_layer, stack)\n",
    "      attn_weights, attn_state = self.attention_layer.init(inputs)\n",
    "      self.state = (state, attn_state)\n",
    "      self.weights = (weights, attn_weights)\n",
    "  # pylint: enable=protected-access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReversibleSwap() duplicates the Top of Stack and returns the stack\n",
    "sl = tl.ReversibleSwap()\n",
    "sl([x1, x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate reverse swap\n",
    "print(x1, x2, sl.reverse([x1, x2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fl = tl.Fn(\"F\", lambda x0: (2 * x0), n_out=1)\n",
    "Gl = tl.Fn(\"G\", lambda x0: (10 * x0), n_out=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_res_F = ReversibleHalfResidual(Fl)\n",
    "print(type(half_res_F), \"\\n\", half_res_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_res_F([x1, x1])  # this is going to produce an error - why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to initialize the ReversibleHalfResidual layer to let it know what the input is going to look like\n",
    "half_res_F.init(shapes.signature([x1, x1]))\n",
    "half_res_F([x1, x1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a reversible model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import trax\n",
    "#from trax import layers as tl\n",
    "#from trax.fastmath import numpy as fastnp\n",
    "#from trax.supervised import training\n",
    "\n",
    "# UNIT TEST for UNQ_C1\n",
    "def test_get_conversation(target):\n",
    "\n",
    "    data = {'file1.json': {'log':[{'text': 'hi'},\n",
    "                                  {'text': 'hello'},\n",
    "                                  {'text': 'nice'}]},\n",
    "            'file2.json':{'log':[{'text': 'a b'}, \n",
    "                                 {'text': ''}, \n",
    "                                 {'text': 'good '}, \n",
    "                                 {'text': 'no?'}]}}\n",
    "    \n",
    "    res1 = target('file1.json', data)\n",
    "    res2 = target('file2.json', data)\n",
    "    \n",
    "    expected1 = ' Person 1: hi Person 2: hello Person 1: nice'\n",
    "    expected2 = ' Person 1: a b Person 2:  Person 1: good  Person 2: no?'\n",
    "\n",
    "    success = 0\n",
    "    fails = 0\n",
    "    \n",
    "    try:\n",
    "        assert res1 == expected1\n",
    "        success += 1\n",
    "    except ValueError:\n",
    "        print('Error in test 1 \\nResult  : ', res1, 'x \\nExpected: ', expected1)\n",
    "        fails += 1\n",
    "    try:\n",
    "        assert res2 == expected2\n",
    "        success += 1\n",
    "    except:\n",
    "        print('Error in test 2 \\nResult  : ', res2, ' \\nExpected: ', expected2)\n",
    "        fails += 1\n",
    "            \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "\n",
    "\n",
    "# UNIT TEST for UNQ_C2\n",
    "def test_reversible_layer_forward(target):\n",
    "    f1 = lambda x: x + 2\n",
    "    g1 = lambda x: x * 3\n",
    "    \n",
    "    f2 = lambda x: x + 1\n",
    "    g2 = lambda x: x * 2\n",
    "    \n",
    "    input_vector1 = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "    expected1 = np.array([8, 10, 12, 14, 29, 36, 43, 50])\n",
    "    \n",
    "    input_vector2 = np.array([1] * 128)\n",
    "    expected2 = np.array([3] * 64 + [7] * 64)\n",
    "    \n",
    "    success = 0\n",
    "    fails = 0\n",
    "    try:\n",
    "        res = target(input_vector1, f1, g1)\n",
    "        assert isinstance(res, np.ndarray)\n",
    "        success += 1\n",
    "    except:\n",
    "        print('Wrong type! Output is not of type np.ndarray')\n",
    "        fails += 1\n",
    "    try:\n",
    "        res = target(input_vector1, f1, g1)\n",
    "        assert np.allclose(res, expected1)\n",
    "        success += 1\n",
    "    except ValueError:\n",
    "        print('Error in test 1 \\nResult  : ', res, 'x \\nExpected: ', expected1)\n",
    "        fails += 1\n",
    "    try:\n",
    "        res = target(input_vector2, f2, g2)\n",
    "        assert np.allclose(res, expected2)\n",
    "        success += 1\n",
    "    except:\n",
    "        print('Error in test 2 \\nResult  : ', res, ' \\nExpected: ', expected2)\n",
    "        fails += 1\n",
    "            \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "\n",
    "\n",
    "# UNIT TEST for UNQ_C3\n",
    "def test_reversible_layer_reverse(target):\n",
    "    \n",
    "    f1 = lambda x: x + 2\n",
    "    g1 = lambda x: x * 3\n",
    "    \n",
    "    f2 = lambda x: x + 1\n",
    "    g2 = lambda x: x * 2\n",
    "    \n",
    "    input_vector1 = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "    expected1 = np.array([-3,  0,  3,  6,  2,  0, -2, -4])\n",
    "    \n",
    "    input_vector2 = np.array([1] * 128)\n",
    "    expected2 = np.array([1] * 64 + [-1] * 64)\n",
    "    \n",
    "    success = 0\n",
    "    fails = 0\n",
    "    try:\n",
    "        res = target(input_vector1, f1, g1)\n",
    "        assert isinstance(res, np.ndarray)\n",
    "        success += 1\n",
    "    except:\n",
    "        print('Wrong type! Output is not of type np.ndarray')\n",
    "        fails += 1\n",
    "    try:\n",
    "        res = target(input_vector1, f1, g1)\n",
    "        assert np.allclose(res, expected1)\n",
    "        success += 1\n",
    "    except ValueError:\n",
    "        print('Error in test 1 \\nResult  : ', res, 'x \\nExpected: ', expected1)\n",
    "        fails += 1\n",
    "    try:\n",
    "        res = target(input_vector2, f2, g2)\n",
    "        assert np.allclose(res, expected2)\n",
    "        success += 1\n",
    "    except:\n",
    "        print('Error in test 2 \\nResult  : ', res, ' \\nExpected: ', expected2)\n",
    "        fails += 1\n",
    "            \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "        \n",
    "\n",
    "# UNIT TEST for UNQ_C4\n",
    "def test_ReformerLM(target):\n",
    "    test_cases = [\n",
    "                {\n",
    "                    \"name\":\"layer_len_check\",\n",
    "                    \"expected\":11,\n",
    "                    \"error\":\"We found {} layers in your model. It should be 11.\\nCheck the LSTM stack before the dense layer\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\":\"simple_test_check\",\n",
    "      \"expected\":\"Serial[ShiftRight(1)Embedding_train_512DropoutPositionalEncodingDup_out2ReversibleSerial_in2_out2[ReversibleHalfResidualV2_in2_out2[Serial[LayerNorm]SelfAttention]ReversibleSwap_in2_out2ReversibleHalfResidualV2_in2_out2[Serial[LayerNormDense_2048DropoutFastGeluDense_512Dropout]]ReversibleSwap_in2_out2ReversibleHalfResidualV2_in2_out2[Serial[LayerNorm]SelfAttention]ReversibleSwap_in2_out2ReversibleHalfResidualV2_in2_out2[Serial[LayerNormDense_2048DropoutFastGeluDense_512Dropout]]ReversibleSwap_in2_out2]Concatenate_in2LayerNormDropoutDense_trainLogSoftmax]\",\n",
    "                    \"error\":\"The ReformerLM is not defined properly.\"\n",
    "                }\n",
    "            ]\n",
    "    temp_model = target('train')\n",
    "    \n",
    "    success = 0\n",
    "    fails = 0\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        try:\n",
    "            if test_case['name'] == \"simple_test_check\":\n",
    "                assert test_case[\"expected\"] == str(temp_model).replace(' ', '').replace('\\n','')\n",
    "                success += 1\n",
    "            if test_case['name'] == \"layer_len_check\":\n",
    "                if test_case[\"expected\"] == len(temp_model.sublayers):\n",
    "                    success += 1\n",
    "                else:\n",
    "                    print(test_case[\"error\"].format(len(temp_model.sublayers))) \n",
    "                    fails += 1\n",
    "        except:\n",
    "            print(test_case['error'])\n",
    "            fails += 1\n",
    "            \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "\n",
    "\n",
    "# UNIT TEST for UNQ_C5\n",
    "def test_tasks(train_task, eval_task):\n",
    "    target = train_task\n",
    "    success = 0\n",
    "    fails = 0\n",
    "     \n",
    "    # Test the labeled data parameter for train_task\n",
    "    try:\n",
    "        strlabel = str(target._labeled_data)\n",
    "        assert (\"generator\" in strlabel) and (\"add_loss_weights\" in  strlabel)\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong labeled data parameter in train_task\")\n",
    "    \n",
    "    # Test the cross entropy loss data parameter\n",
    "    try:\n",
    "        strlabel = str(target._loss_layer)\n",
    "        assert(strlabel == \"CrossEntropyLoss_in3\")\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong loss functions. CrossEntropyLoss_in3 was expected\")\n",
    "        \n",
    "     # Test the optimizer parameter\n",
    "    try:\n",
    "        assert(isinstance(target.optimizer, trax.optimizers.adam.Adam))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong optimizer\")\n",
    "        \n",
    "    # Test the schedule parameter\n",
    "    try:\n",
    "        assert(isinstance(target._lr_schedule,trax.supervised.lr_schedules._BodyAndTail))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong learning rate schedule type\")\n",
    "    \n",
    "    # Test the _n_steps_per_checkpoint parameter\n",
    "    try:\n",
    "        assert(target._n_steps_per_checkpoint==10)\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong checkpoint step frequency\")\n",
    "        \n",
    "    target = eval_task\n",
    "    # Test the labeled data parameter for eval_task\n",
    "    try:\n",
    "        strlabel = str(target._labeled_data)\n",
    "        assert (\"generator\" in strlabel) and (\"add_loss_weights\" in  strlabel)\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong labeled data parameter in eval_task\")\n",
    "    \n",
    "    # Test the metrics in eval_task \n",
    "    try:\n",
    "        strlabel = str(target._metrics).replace(' ', '')\n",
    "        assert(strlabel == \"[CrossEntropyLoss_in3,Accuracy_in3]\")\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(f\"Wrong metrics. found {strlabel} but expected [CrossEntropyLoss_in3,Accuracy_in3]\")\n",
    "        \n",
    "        \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the MultiWoz dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trax                               1.3.1\r\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "\n",
    "import trax   \n",
    "from trax import layers as tl\n",
    "from trax.supervised import training\n",
    "!pip list | grep trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename of the MultiWOZ dialogue dataset\n",
    "DATA_FILE = 'data.json'\n",
    "\n",
    "# data directory\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# dictionary where we will load the dialogue dataset\n",
    "DIALOGUE_DB = {}\n",
    "\n",
    "# vocabulary filename\n",
    "VOCAB_FILE = 'en_32k.subword'\n",
    "\n",
    "# vocabulary file directory\n",
    "VOCAB_DIR = 'data/vocabs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help function to load a JSON file\n",
    "def load_json(directory, file):\n",
    "    with open(f'{directory}/{file}') as file: \n",
    "        db = json.load(file)\n",
    "    return db\n",
    "\n",
    "# load the dialogue data set into our dictionary\n",
    "DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of dialogues is: {len(DIALOGUE_DB)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 7 keys from the dataset to see the filenames\n",
    "print(list(DIALOGUE_DB.keys())[0:7]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get keys of the fifth file in the list above\n",
    "print(DIALOGUE_DB['SNG0073.json'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIALOGUE_DB['SNG0073.json']['goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first element of the log list\n",
    "DIALOGUE_DB['SNG0073.json']['log'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' Person 1: ', DIALOGUE_DB['SNG0073.json']['log'][0]['text'])\n",
    "print(' Person 2: ',DIALOGUE_DB['SNG0073.json']['log'][1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: get_conversation\n",
    "def get_conversation(file, data_db):\n",
    "    '''\n",
    "    Args:\n",
    "        file (string): filename of the dialogue file saved as json\n",
    "        data_db (dict): dialogue database\n",
    "    \n",
    "    Returns:\n",
    "        string: A string containing the 'text' fields of  data[file]['log'][x]\n",
    "    '''\n",
    "    \n",
    "    # initialize empty string\n",
    "    result = ''\n",
    "    \n",
    "    # get length of file's log list\n",
    "    len_msg_log = len(data_db[file]['log'])\n",
    "    \n",
    "    # set the delimiter strings\n",
    "    delimiter_1 = ' Person 1: '\n",
    "    delimiter_2 = ' Person 2: '\n",
    "    \n",
    "    # loop over the file's log list\n",
    "    for i in range(len_msg_log):\n",
    "        \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "        # get i'th element of file log list\n",
    "        cur_log = data_db[file]['log'][i]\n",
    "        \n",
    "        # check if i is even\n",
    "        if i%2 == 0:                   \n",
    "            # append the 1st delimiter string\n",
    "            result += delimiter_1\n",
    "        else: \n",
    "            # append the 2nd delimiter string\n",
    "            result += delimiter_2\n",
    "        \n",
    "        # append the message text from the log\n",
    "        result += cur_log['text']\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "import w4_unittest\n",
    "w4_unittest.test_get_conversation(get_conversation)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'SNG01856.json'\n",
    "conversation = get_conversation(file, DIALOGUE_DB)\n",
    "\n",
    "# print raw output\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_conversation(conversation):\n",
    "    \n",
    "    delimiter_1 = 'Person 1: '\n",
    "    delimiter_2 = 'Person 2: '\n",
    "    \n",
    "    split_list_d1 = conversation.split(delimiter_1)\n",
    "    \n",
    "    for sublist in split_list_d1[1:]:\n",
    "        split_list_d2 = sublist.split(delimiter_2)\n",
    "        print(colored(f'Person 1: {split_list_d2[0]}', 'red'))\n",
    "        \n",
    "        if len(split_list_d2) > 1:\n",
    "            print(colored(f'Person 2: {split_list_d2[1]}', 'green'))\n",
    "\n",
    "            \n",
    "print_conversation(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIALOGUE_DB['SNG01856.json']['log'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example of the attractions file\n",
    "attraction_file = open('data/attraction_db.json')\n",
    "attractions = json.load(attraction_file)\n",
    "print(attractions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example of the hospital file\n",
    "hospital_file = open('data/hospital_db.json')\n",
    "hospitals = json.load(hospital_file)\n",
    "print(hospitals[0]) # feel free to index into other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example of the hotel file\n",
    "hotel_file = open('data/hotel_db.json')\n",
    "hotels = json.load(hotel_file)\n",
    "print(hotels[0]) # feel free to index into other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example of the police file\n",
    "police_file = open('data/police_db.json')\n",
    "police = json.load(police_file)\n",
    "print(police[0]) # feel free to index into other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example of a restuarant file\n",
    "restaurant_file = open('data/restaurant_db.json')\n",
    "restaurants = json.load(restaurant_file)\n",
    "print(restaurants[0]) # feel free to index into other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/README') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Processing the data for Reformer inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the keys are the file names\n",
    "all_files = DIALOGUE_DB.keys()\n",
    "\n",
    "# initialize empty list\n",
    "untokenized_data = []\n",
    "\n",
    "# loop over all files\n",
    "for file in all_files:\n",
    "    # this is the graded function you coded\n",
    "    # returns a string delimited by Person 1 and Person 2\n",
    "    result = get_conversation(file, DIALOGUE_DB)\n",
    "    \n",
    "    # append to the list\n",
    "    untokenized_data.append(result)\n",
    "\n",
    "# print the first element to check if it's the same as the one we got before\n",
    "print(untokenized_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the list we generated above\n",
    "random.shuffle(untokenized_data)\n",
    "\n",
    "# define a cutoff (5% of the total length for this assignment)\n",
    "# convert to int because we will use it as a list index\n",
    "cut_off = int(len(untokenized_data) * .05)\n",
    "\n",
    "# slice the list. the last elements after the cut_off value will be the eval set. the rest is for training. \n",
    "train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]\n",
    "\n",
    "print(f'number of conversations in the data set: {len(untokenized_data)}')\n",
    "print(f'number of conversations in train set: {len(train_data)}')\n",
    "print(f'number of conversations in eval set: {len(eval_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Tokenizing, batching with bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(data):\n",
    "    # loop over the entire data\n",
    "    while True:\n",
    "        # get a random element\n",
    "        d = random.choice(data)\n",
    "        \n",
    "        # yield a tuple pair of identical values \n",
    "        # (i.e. our inputs to the model will also be our targets during training)\n",
    "        yield (d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trax allows us to use combinators to generate our data pipeline\n",
    "data_pipeline = trax.data.Serial(\n",
    "    # randomize the stream\n",
    "    trax.data.Shuffle(),\n",
    "    \n",
    "    # tokenize the data\n",
    "    trax.data.Tokenize(vocab_dir=VOCAB_DIR,\n",
    "                       vocab_file=VOCAB_FILE),\n",
    "    \n",
    "    # filter too long sequences\n",
    "    trax.data.FilterByLength(2048),\n",
    "    \n",
    "    # bucket by length\n",
    "    trax.data.BucketByLength(boundaries=[128, 256,  512, 1024],\n",
    "                             batch_sizes=[16,    8,    4,   2, 1]),\n",
    "    \n",
    "    # add loss weights but do not add it to the padding tokens (i.e. 0)\n",
    "    trax.data.AddLossWeights(id_to_mask=0)\n",
    ")\n",
    "\n",
    "# apply the data pipeline to our train and eval sets\n",
    "train_stream = data_pipeline(stream(train_data))\n",
    "eval_stream = data_pipeline(stream(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stream generators will yield (input, target, weights). let's just grab the input for inspection\n",
    "inp, _, _ = next(train_stream)\n",
    "\n",
    "# print the shape. format is (batch size, token length)\n",
    "print(\"input shape: \", inp.shape)\n",
    "\n",
    "# detokenize the first element\n",
    "print(trax.data.detokenize(inp[0], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Reversible layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: reversible_layer_forward\n",
    "def reversible_layer_forward(x, f, g):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        x (np.array): an input vector or matrix\n",
    "        f (function): a function which operates on a vector/matrix\n",
    "        g (function): a function which operates on a vector/matrix\n",
    "    Returns: \n",
    "        y (np.array): an output vector or matrix whose form is determined by 'x', f and g\n",
    "    \"\"\"\n",
    "    # split the input vector into two (* along the last axis because it is the depth dimension)\n",
    "    x1, x2 = np.split(x, 2, axis=-1) \n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "    # get y1 using equation 3\n",
    "    y1 = x1 + f(x2)\n",
    "    \n",
    "    # get y2 using equation 4\n",
    "    y2 = x2 + g(y1)\n",
    "    \n",
    "    # concatenate y1 and y2 along the depth dimension. be sure output is of type np.ndarray\n",
    "    y = np.concatenate([y1, y2], axis=-1)\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w4_unittest.test_reversible_layer_forward(reversible_layer_forward)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: reversible_layer_reverse\n",
    "def reversible_layer_reverse(y, f, g):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        y (np.array): an input vector or matrix\n",
    "        f (function): a function which operates on a vector/matrix of the form of 'y'\n",
    "        g (function): a function which operates on a vector/matrix of the form of 'y'\n",
    "    Returns: \n",
    "        y (np.array): an output vector or matrix whose form is determined by 'y', f and g\n",
    "    \"\"\"\n",
    "    \n",
    "    # split the input vector into two (* along the last axis because it is the depth dimension)\n",
    "    y1, y2 = np.split(y, 2, axis=-1)\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "    # compute x2 using equation 5\n",
    "    x2 = y2 - g(y1)\n",
    "    \n",
    "    # compute x1 using equation 6\n",
    "    x1 = y1 - f(x2)\n",
    "    \n",
    "    # concatenate x1 and x2 along the depth dimension\n",
    "    x = np.concatenate([x1, x2], axis=-1) \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w4_unittest.test_reversible_layer_reverse(reversible_layer_reverse)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIT TEST COMMENT: assert at the end can be used in grading as well\n",
    "f = lambda x: x + 2\n",
    "g = lambda x: x * 3\n",
    "input_vector = np.random.uniform(size=(32,))\n",
    "\n",
    "output_vector = reversible_layer_forward(input_vector, f, g)\n",
    "reversed_vector = reversible_layer_reverse(output_vector, f, g)\n",
    "\n",
    "assert np.allclose(reversed_vector, input_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reversible layers and randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers like dropout have noise, so let's simulate it here:\n",
    "f = lambda x: x + np.random.uniform(size=x.shape)\n",
    "\n",
    "# See that the above doesn't work any more:\n",
    "output_vector = reversible_layer_forward(input_vector, f, g)\n",
    "reversed_vector = reversible_layer_reverse(output_vector, f, g)\n",
    "\n",
    "assert not np.allclose(reversed_vector, input_vector)  # Fails!!\n",
    "\n",
    "# It failed because the noise when reversing used a different random seed.\n",
    "\n",
    "random_seed = 27686\n",
    "rng = trax.fastmath.random.get_prng(random_seed)\n",
    "f = lambda x: x + trax.fastmath.random.uniform(key=rng, shape=x.shape)\n",
    "\n",
    "# See that it works now as the same rng is used on forward and reverse.\n",
    "output_vector = reversible_layer_forward(input_vector, f, g)\n",
    "reversed_vector = reversible_layer_reverse(output_vector, f, g)\n",
    "\n",
    "assert np.allclose(reversed_vector, input_vector,  atol=1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: ReformerLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION\n",
    "def ReformerLM(vocab_size=33000, n_layers=2, mode='train', attention_type=tl.SelfAttention):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        vocab_size (int): size of the vocabulary\n",
    "        n_layers (int): number of decoder layers\n",
    "        mode (string): setting of the model which can be 'train', 'eval', or 'predict' \n",
    "        attention_type(class): attention class to use \n",
    "    Returns: \n",
    "        model (ReformerLM): a reformer language model implemented in Trax\n",
    "    \"\"\"    \n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    # initialize an instance of Trax's ReformerLM class\n",
    "    model = trax.models.reformer.ReformerLM( \n",
    "        # set vocab size\n",
    "        vocab_size=vocab_size,\n",
    "        # set number of layers\n",
    "        n_layers=n_layers,\n",
    "        # set mode\n",
    "        mode=mode,\n",
    "        # set attention type\n",
    "        attention_type=attention_type\n",
    "    )\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the model\n",
    "temp_model = ReformerLM('train')\n",
    "print(str(temp_model))\n",
    "\n",
    "# free memory\n",
    "del temp_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w4_unittest.test_ReformerLM(ReformerLM)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED FUNCTION: train_model\n",
    "def training_loop(ReformerLM, train_gen, eval_gen, output_dir = \"./model/\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ReformerLM:  the Reformer language model you are building\n",
    "        train_gen (generator): train data generator.\n",
    "        eval_gen (generator): Validation generator. \n",
    "        output_dir (string): Path to save the model output. Defaults to './model/'.\n",
    "\n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # use the warmup_and_rsqrt_decay learning rate schedule\n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(\n",
    "        n_warmup_steps=1000, max_value=0.01)\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "    # define the train task\n",
    "    train_task = training.TrainTask(            \n",
    "        # labeled data\n",
    "        labeled_data=train_gen,\n",
    "        # loss layer\n",
    "        loss_layer=tl.CrossEntropyLoss(),\n",
    "        # optimizer\n",
    "        optimizer=trax.optimizers.Adam(0.01),\n",
    "        # lr_schedule\n",
    "        lr_schedule=lr_schedule,\n",
    "        # n_steps\n",
    "        n_steps_per_checkpoint=10\n",
    "    )\n",
    "\n",
    "    # define the eval task\n",
    "    eval_task = training.EvalTask(                      \n",
    "        # labeled data\n",
    "        labeled_data=eval_gen,\n",
    "        # metrics\n",
    "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    loop = training.Loop(ReformerLM(mode='train'),\n",
    "                         train_task,\n",
    "                         eval_tasks=[eval_task],\n",
    "                         output_dir=output_dir)\n",
    "    return loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIT TEST COMMENT: Use the train task and eval task for grading train_model\n",
    "test_loop = training_loop(ReformerLM, train_stream, eval_stream)\n",
    "train_task = test_loop._task\n",
    "eval_task = test_loop._eval_task\n",
    "\n",
    "print(train_task)\n",
    "print(eval_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w4_unittest.test_tasks(train_task, eval_task)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will now test your function\n",
    "!rm -f model/model.pkl.gz\n",
    "loop = training_loop(ReformerLM, train_stream, eval_stream)\n",
    "loop.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Decode from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention\n",
    "def attention(*args, **kwargs):\n",
    "    # number of input positions to remember in a cache when doing fast inference. \n",
    "    kwargs['predict_mem_len'] = 120\n",
    "    # number of input elements to drop once the fast inference input cache fills up.\n",
    "    kwargs['predict_drop_len'] = 120\n",
    "    # return the attention layer with the parameters defined above\n",
    "    return tl.SelfAttention(*args, **kwargs)\n",
    "\n",
    "# define the model using the ReformerLM function you implemented earlier.\n",
    "model = ReformerLM(\n",
    "    vocab_size=33000,\n",
    "    n_layers=6,\n",
    "    mode='predict',\n",
    "    attention_type=attention,\n",
    ")\n",
    "\n",
    "# define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.\n",
    "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize from file\n",
    "model.init_from_file('chatbot_model1.pkl.gz',\n",
    "                     weights_only=True, input_signature=shape11)\n",
    "\n",
    "# save the starting state\n",
    "STARTING_STATE = model.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, vocab_file, vocab_dir):\n",
    "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
    "\n",
    "def detokenize(tokens, vocab_file, vocab_dir):\n",
    "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6\n",
    "# GRADED FUNCTION\n",
    "def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ReformerLM:  the Reformer language model you just trained\n",
    "        start_sentence (string): starting sentence of the conversation\n",
    "        vocab_file (string): vocabulary filename\n",
    "        vocab_dir (string): directory of the vocabulary file\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "\n",
    "    Returns:\n",
    "        generator: yields the next symbol generated by the model\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "    # Create input tokens using the the tokenize function\n",
    "    input_tokens = tokenize(start_sentence, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
    "    \n",
    "    # Add batch dimension to array. Convert from (n,) to (x, n) where \n",
    "    # x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)\n",
    "    input_tokens_with_batch = np.array(input_tokens)[None, :]\n",
    "    \n",
    "    # call the autoregressive_sample_stream function from trax\n",
    "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
    "        # model\n",
    "        ReformerLM,\n",
    "        # inputs will be the tokens with batch dimension\n",
    "        inputs=input_tokens_with_batch,\n",
    "        # temperature\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return output_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "import pickle\n",
    "\n",
    "WEIGHTS_FROM_FILE = ()\n",
    "\n",
    "with open('weights', 'rb') as file:\n",
    "    WEIGHTS_FROM_FILE = pickle.load(file)\n",
    "\n",
    "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
    "\n",
    "def attention(*args, **kwargs):\n",
    "    kwargs['predict_mem_len'] = 120\n",
    "    kwargs['predict_drop_len'] = 120\n",
    "    return tl.SelfAttention(*args, **kwargs)\n",
    "\n",
    "test_model = ReformerLM(vocab_size=5, n_layers=1, mode='predict', attention_type=attention)\n",
    "\n",
    "test_output_gen = ReformerLM_output_gen(test_model, \"test\", vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=0)\n",
    "\n",
    "test_model.init_weights_and_state(shape11)\n",
    "\n",
    "test_model.weights = WEIGHTS_FROM_FILE\n",
    "\n",
    "output = []\n",
    "\n",
    "for i in range(6):\n",
    "    output.append(next(test_output_gen)[0])\n",
    "\n",
    "print(output)\n",
    "\n",
    "# free memory\n",
    "del test_model \n",
    "del WEIGHTS_FROM_FILE\n",
    "del test_output_gen\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
    "\n",
    "def attention(*args, **kwargs):\n",
    "    kwargs['predict_mem_len'] = 120  # max length for predictions\n",
    "    kwargs['predict_drop_len'] = 120  # never drop old stuff\n",
    "    return tl.SelfAttention(*args, **kwargs)\n",
    "\n",
    "model = ReformerLM(\n",
    "    vocab_size=33000,\n",
    "    n_layers=6,\n",
    "    mode='predict',\n",
    "    attention_type=attention,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_from_file('chatbot_model1.pkl.gz',\n",
    "                     weights_only=True, input_signature=shape11)\n",
    "\n",
    "STARTING_STATE = model.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ReformerLM:  the Reformer language model you just trained\n",
    "        model_state (np.array): initial state of the model before decoding\n",
    "        start_sentence (string): starting sentence of the conversation\n",
    "        vocab_file (string): vocabulary filename\n",
    "        vocab_dir (string): directory of the vocabulary file\n",
    "        max_len (int): maximum number of tokens to generate \n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "\n",
    "    Returns:\n",
    "        generator: yields the next symbol generated by the model\n",
    "    \"\"\"  \n",
    "    \n",
    "    # define the delimiters we used during training\n",
    "    delimiter_1 = 'Person 1: ' \n",
    "    delimiter_2 = 'Person 2: '\n",
    "    \n",
    "    # initialize detokenized output\n",
    "    sentence = ''\n",
    "    \n",
    "    # token counter\n",
    "    counter = 0\n",
    "    \n",
    "    # output tokens. we insert a ': ' for formatting\n",
    "    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
    "    \n",
    "    # reset the model state when starting a new dialogue\n",
    "    ReformerLM.state = model_state\n",
    "    \n",
    "    # calls the output generator implemented earlier\n",
    "    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
    "    \n",
    "    # print the starting sentence\n",
    "    print(start_sentence.split(delimiter_2)[0].strip())\n",
    "    \n",
    "    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\n",
    "    for o in output:\n",
    "        \n",
    "        result.append(o)\n",
    "        \n",
    "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
    "        \n",
    "        if sentence.endswith(delimiter_1):\n",
    "            sentence = sentence.split(delimiter_1)[0]\n",
    "            print(f'{delimiter_2}{sentence}')\n",
    "            sentence = ''\n",
    "            result.clear()\n",
    "        \n",
    "        elif sentence.endswith(delimiter_2):\n",
    "            sentence = sentence.split(delimiter_2)[0]\n",
    "            print(f'{delimiter_1}{sentence}')\n",
    "            sentence = ''\n",
    "            result.clear()\n",
    "\n",
    "        counter += 1\n",
    "        \n",
    "        if counter > max_len:\n",
    "            break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now feed in different starting sentences and see how the model generates the dialogue. You can even input your own starting sentence. Just remember to ask a question that covers the topics in the Multiwoz dataset so you can generate a meaningful conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
    "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '\n",
    "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = ' Person 1: Can you book a taxi? Person 2: '\n",
    "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
