{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentencePiece and Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentencePiece Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é = é : False\n"
     ]
    }
   ],
   "source": [
    "eaccent = '\\u00E9'\n",
    "e_accent = '\\u0065\\u0301'\n",
    "print(f'{eaccent} = {e_accent} : {eaccent == e_accent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é = é : True\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "norm_eaccent = normalize('NFKC', '\\u00E9')\n",
    "norm_e_accent = normalize('NFKC', '\\u0065\\u0301')\n",
    "print(f'{norm_eaccent} = {norm_e_accent} : {norm_eaccent == norm_e_accent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hex_encoding(s):\n",
    "    return ' '.join(hex(ord(c)) for c in s)\n",
    "\n",
    "def print_string_and_encoding(s):\n",
    "    print(f'{s} : {get_hex_encoding(s)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é : 0xe9\n",
      "é : 0x65 0x301\n",
      "é : 0xe9\n",
      "é : 0xe9\n"
     ]
    }
   ],
   "source": [
    "for s in [eaccent, e_accent, norm_eaccent, norm_e_accent]:\n",
    "    print_string_and_encoding(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Tokenization is hard.'\n",
    "s_ = s.replace(' ', '\\u2581')\n",
    "s_n = normalize('NFKC', 'Tokenization is hard.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e\n",
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n"
     ]
    }
   ],
   "source": [
    "print(get_hex_encoding(s))\n",
    "print(get_hex_encoding(s_))\n",
    "print(get_hex_encoding(s_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Tokenization is hard.'\n",
    "sn = normalize('NFKC', 'Tokenization is hard.')\n",
    "sn_ = s.replace(' ', '\\u2581')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x20 0x69 0x73 0x20 0x68 0x61 0x72 0x64 0x2e\n",
      "0x54 0x6f 0x6b 0x65 0x6e 0x69 0x7a 0x61 0x74 0x69 0x6f 0x6e 0x2581 0x69 0x73 0x2581 0x68 0x61 0x72 0x64 0x2e\n"
     ]
    }
   ],
   "source": [
    "print(get_hex_encoding(s))\n",
    "print(get_hex_encoding(sn))\n",
    "print(get_hex_encoding(sn_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def convert_json_examples_to_text(filepath):\n",
    "    example_jsons = list(map(ast.literal_eval, open(filepath))) # Read in the json from the example file\n",
    "    texts = [example_json['text'].decode('utf-8') for example_json in example_jsons] # Decode the byte sequences\n",
    "    text = '\\n\\n'.join(texts)       # Separate different articles by two newlines\n",
    "    text = normalize('NFKC', text)  # Normalize the text\n",
    "\n",
    "    with open('example.txt', 'w') as fw:\n",
    "        fw.write(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginners BBQ Class Taking Place in Missoula!\n",
      "Do you want to get better at making delicious BBQ? You will have the opportunity, put this on your calendar now. Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. He will be teaching a beginner level class for everyone who wants to get better with their culinary skills.\n",
      "He will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information.\n",
      "The cost to be in the class is $35 per person, and for spectators it is free. Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.\n",
      "\n",
      "Discussion in 'Mac OS X Lion (10.7)' started by axboi87, Jan 20, 2012.\n",
      "I've got a 500gb internal drive and a 240gb SSD.\n",
      "When trying to restore using di\n"
     ]
    }
   ],
   "source": [
    "text = convert_json_examples_to_text('data.txt')\n",
    "print(text[:900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter(['\\u2581' + word for word in text.split()])\n",
    "vocab = {' '.join([l for l in word]): freq for word, freq in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_vocab(vocab, end='\\n', limit=20):\n",
    "    shown = 0\n",
    "    for word, freq in vocab.items():\n",
    "        print(f'{word}: {freq}', end=end)\n",
    "        shown +=1\n",
    "        if shown > limit:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁ B e g i n n e r s: 1\n",
      "▁ B B Q: 3\n",
      "▁ C l a s s: 2\n",
      "▁ T a k i n g: 1\n",
      "▁ P l a c e: 1\n",
      "▁ i n: 15\n",
      "▁ M i s s o u l a !: 1\n",
      "▁ D o: 1\n",
      "▁ y o u: 13\n",
      "▁ w a n t: 1\n",
      "▁ t o: 33\n",
      "▁ g e t: 2\n",
      "▁ b e t t e r: 2\n",
      "▁ a t: 1\n",
      "▁ m a k i n g: 2\n",
      "▁ d e l i c i o u s: 1\n",
      "▁ B B Q ?: 1\n",
      "▁ Y o u: 1\n",
      "▁ w i l l: 6\n",
      "▁ h a v e: 4\n",
      "▁ t h e: 31\n"
     ]
    }
   ],
   "source": [
    "show_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 455\n",
      "Number of merges required to reproduce SentencePiece training on the whole corpus: 273\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of unique words: {len(vocab)}')\n",
    "print(f'Number of merges required to reproduce SentencePiece training on the whole corpus: {int(0.60*len(vocab))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_sentence_piece_vocab(vocab, frac_merges=0.60):\n",
    "    sp_vocab = vocab.copy()\n",
    "    num_merges = int(len(sp_vocab)*frac_merges)\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(sp_vocab)\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        sp_vocab = merge_vocab(best, sp_vocab)\n",
    "\n",
    "    return sp_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁B e g in n ers: 1\n",
      "▁BBQ: 3\n",
      "▁Cl ass: 2\n",
      "▁T ak ing: 1\n",
      "▁P la ce: 1\n",
      "▁in: 15\n",
      "▁M is s ou la !: 1\n",
      "▁D o: 1\n",
      "▁you: 13\n",
      "▁w an t: 1\n",
      "▁to: 33\n",
      "▁g et: 2\n",
      "▁be t ter: 2\n",
      "▁a t: 1\n",
      "▁mak ing: 2\n",
      "▁d e l ic i ou s: 1\n",
      "▁BBQ ?: 1\n",
      "▁ Y ou: 1\n",
      "▁will: 6\n",
      "▁have: 4\n",
      "▁the: 31\n"
     ]
    }
   ],
   "source": [
    "sp_vocab = get_sentence_piece_vocab(vocab)\n",
    "show_vocab(sp_vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SentencePiece BPE Tokenizer on Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor(model_file='sentencepiece.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SentencePieceProcessor in module sentencepiece object:\n",
      "\n",
      "class SentencePieceProcessor(builtins.object)\n",
      " |  SentencePieceProcessor(model_file=None, model_proto=None, out_type=<class 'int'>, add_bos=False, add_eos=False, reverse=False, enable_sampling=False, nbest_size=-1, alpha=0.1)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  Decode(self, input)\n",
      " |      Decode processed id or token sequences.\n",
      " |  \n",
      " |  DecodeIds(self, ids)\n",
      " |  \n",
      " |  DecodeIdsAsSerializedProto(self, ids)\n",
      " |  \n",
      " |  DecodePieces(self, pieces)\n",
      " |  \n",
      " |  DecodePiecesAsSerializedProto(self, pieces)\n",
      " |  \n",
      " |  Detokenize = Decode(self, input)\n",
      " |  \n",
      " |  Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, enable_sampling=None, nbest_size=None, alpha=None)\n",
      " |      Encode text input to segmented ids or tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |      input: input string. accepsts list of string.\n",
      " |      out_type: output type. int or str.\n",
      " |      add_bos: Add <s> to the result (Default = false)\n",
      " |      add_eos: Add </s> to the result (Default = false) <s>/</s> is added after\n",
      " |        reversing (if enabled).\n",
      " |      reverse: Reverses the tokenized sequence (Default = false)\n",
      " |      nbest_size: sampling parameters for unigram. Invalid for BPE-Dropout.\n",
      " |                  nbest_size = {0,1}: No sampling is performed.\n",
      " |                  nbest_size > 1: samples from the nbest_size results.\n",
      " |                  nbest_size < 0: assuming that nbest_size is infinite and samples\n",
      " |                    from the all hypothesis (lattice) using\n",
      " |                    forward-filtering-and-backward-sampling algorithm.\n",
      " |      alpha: Soothing parameter for unigram sampling, and merge probability for\n",
      " |        BPE-dropout.\n",
      " |  \n",
      " |  EncodeAsIds(self, input)\n",
      " |  \n",
      " |  EncodeAsPieces(self, input)\n",
      " |  \n",
      " |  EncodeAsSerializedProto(self, input)\n",
      " |  \n",
      " |  GetEncoderVersion(self)\n",
      " |  \n",
      " |  GetPieceSize(self)\n",
      " |  \n",
      " |  GetScore = _batched_func(self, arg)\n",
      " |  \n",
      " |  IdToPiece = _batched_func(self, arg)\n",
      " |  \n",
      " |  Init(self, model_file=None, model_proto=None, out_type=<class 'int'>, add_bos=False, add_eos=False, reverse=False, enable_sampling=False, nbest_size=-1, alpha=0.1)\n",
      " |      Initialzie sentencepieceProcessor.\n",
      " |      \n",
      " |      Args:\n",
      " |        model_file: The sentencepiece model file path.\n",
      " |        model_proto: The sentencepiece model serialized proto.\n",
      " |        out_type: output type. int or str.\n",
      " |        add_bos: Add <s> to the result (Default = false)\n",
      " |        add_eos: Add </s> to the result (Default = false) <s>/</s> is added after\n",
      " |          reversing (if enabled).\n",
      " |        reverse: Reverses the tokenized sequence (Default = false)\n",
      " |        nbest_size: sampling parameters for unigram. Invalid for BPE-Dropout.\n",
      " |                    nbest_size = {0,1}: No sampling is performed.\n",
      " |                    nbest_size > 1: samples from the nbest_size results.\n",
      " |                    nbest_size < 0: assuming that nbest_size is infinite and samples\n",
      " |                      from the all hypothesis (lattice) using\n",
      " |                      forward-filtering-and-backward-sampling algorithm.\n",
      " |        alpha: Soothing parameter for unigram sampling, and merge probability for\n",
      " |          BPE-dropout.\n",
      " |  \n",
      " |  IsByte = _batched_func(self, arg)\n",
      " |  \n",
      " |  IsControl = _batched_func(self, arg)\n",
      " |  \n",
      " |  IsUnknown = _batched_func(self, arg)\n",
      " |  \n",
      " |  IsUnused = _batched_func(self, arg)\n",
      " |  \n",
      " |  Load(self, model_file=None, model_proto=None)\n",
      " |      Overwride SentencePieceProcessor.Load to support both model_file and model_proto.\n",
      " |      \n",
      " |      Args:\n",
      " |        model_file: The sentencepiece model file path.\n",
      " |        model_proto: The sentencepiece model serialized proto. Either `model_file`\n",
      " |          or `model_proto` must be set.\n",
      " |  \n",
      " |  LoadFromFile(self, arg)\n",
      " |  \n",
      " |  LoadFromSerializedProto(self, serialized)\n",
      " |  \n",
      " |  LoadVocabulary(self, filename, threshold)\n",
      " |  \n",
      " |  NBestEncodeAsIds(self, input, nbest_size)\n",
      " |  \n",
      " |  NBestEncodeAsPieces(self, input, nbest_size)\n",
      " |  \n",
      " |  NBestEncodeAsSerializedProto(self, input, nbest_size)\n",
      " |  \n",
      " |  PieceToId = _batched_func(self, arg)\n",
      " |  \n",
      " |  ResetVocabulary(self)\n",
      " |  \n",
      " |  SampleEncodeAsIds(self, input, nbest_size, alpha)\n",
      " |  \n",
      " |  SampleEncodeAsPieces(self, input, nbest_size, alpha)\n",
      " |  \n",
      " |  SampleEncodeAsSerializedProto(self, input, nbest_size, alpha)\n",
      " |  \n",
      " |  SetDecodeExtraOptions(self, extra_option)\n",
      " |  \n",
      " |  SetEncodeExtraOptions(self, extra_option)\n",
      " |  \n",
      " |  SetEncoderVersion(self, encoder_version)\n",
      " |  \n",
      " |  SetVocabulary(self, valid_vocab)\n",
      " |  \n",
      " |  Tokenize = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, enable_sampling=None, nbest_size=None, alpha=None)\n",
      " |  \n",
      " |  __getitem__(self, piece)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__ = Init(self, model_file=None, model_proto=None, out_type=<class 'int'>, add_bos=False, add_eos=False, reverse=False, enable_sampling=False, nbest_size=-1, alpha=0.1)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __repr__ = _swig_repr(self)\n",
      " |  \n",
      " |  __setstate__(self, serialized_model_proto)\n",
      " |  \n",
      " |  bos_id(self)\n",
      " |  \n",
      " |  decode = Decode(self, input)\n",
      " |  \n",
      " |  decode_ids = DecodeIds(self, ids)\n",
      " |  \n",
      " |  decode_ids_as_serialized_proto = DecodeIdsAsSerializedProto(self, ids)\n",
      " |  \n",
      " |  decode_pieces = DecodePieces(self, pieces)\n",
      " |  \n",
      " |  decode_pieces_as_serialized_proto = DecodePiecesAsSerializedProto(self, pieces)\n",
      " |  \n",
      " |  detokenize = Decode(self, input)\n",
      " |  \n",
      " |  encode = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, enable_sampling=None, nbest_size=None, alpha=None)\n",
      " |  \n",
      " |  encode_as_ids = EncodeAsIds(self, input)\n",
      " |  \n",
      " |  encode_as_pieces = EncodeAsPieces(self, input)\n",
      " |  \n",
      " |  encode_as_serialized_proto = EncodeAsSerializedProto(self, input)\n",
      " |  \n",
      " |  eos_id(self)\n",
      " |  \n",
      " |  get_encoder_version = GetEncoderVersion(self)\n",
      " |  \n",
      " |  get_piece_size = GetPieceSize(self)\n",
      " |  \n",
      " |  get_score = _batched_func(self, arg)\n",
      " |  \n",
      " |  id_to_piece = _batched_func(self, arg)\n",
      " |  \n",
      " |  init = Init(self, model_file=None, model_proto=None, out_type=<class 'int'>, add_bos=False, add_eos=False, reverse=False, enable_sampling=False, nbest_size=-1, alpha=0.1)\n",
      " |  \n",
      " |  is_byte = _batched_func(self, arg)\n",
      " |  \n",
      " |  is_control = _batched_func(self, arg)\n",
      " |  \n",
      " |  is_unknown = _batched_func(self, arg)\n",
      " |  \n",
      " |  is_unused = _batched_func(self, arg)\n",
      " |  \n",
      " |  load = Load(self, model_file=None, model_proto=None)\n",
      " |  \n",
      " |  load_from_file = LoadFromFile(self, arg)\n",
      " |  \n",
      " |  load_from_serialized_proto = LoadFromSerializedProto(self, serialized)\n",
      " |  \n",
      " |  load_vocabulary = LoadVocabulary(self, filename, threshold)\n",
      " |  \n",
      " |  nbest_encode_as_ids = NBestEncodeAsIds(self, input, nbest_size)\n",
      " |  \n",
      " |  nbest_encode_as_pieces = NBestEncodeAsPieces(self, input, nbest_size)\n",
      " |  \n",
      " |  nbest_encode_as_serialized_proto = NBestEncodeAsSerializedProto(self, input, nbest_size)\n",
      " |  \n",
      " |  pad_id(self)\n",
      " |  \n",
      " |  piece_size(self)\n",
      " |  \n",
      " |  piece_to_id = _batched_func(self, arg)\n",
      " |  \n",
      " |  reset_vocabulary = ResetVocabulary(self)\n",
      " |  \n",
      " |  sample_encode_as_ids = SampleEncodeAsIds(self, input, nbest_size, alpha)\n",
      " |  \n",
      " |  sample_encode_as_pieces = SampleEncodeAsPieces(self, input, nbest_size, alpha)\n",
      " |  \n",
      " |  sample_encode_as_serialized_proto = SampleEncodeAsSerializedProto(self, input, nbest_size, alpha)\n",
      " |  \n",
      " |  serialized_model_proto(self)\n",
      " |  \n",
      " |  set_decode_extra_options = SetDecodeExtraOptions(self, extra_option)\n",
      " |  \n",
      " |  set_encode_extra_options = SetEncodeExtraOptions(self, extra_option)\n",
      " |  \n",
      " |  set_encoder_version = SetEncoderVersion(self, encoder_version)\n",
      " |  \n",
      " |  set_vocabulary = SetVocabulary(self, valid_vocab)\n",
      " |  \n",
      " |  tokenize = Encode(self, input, out_type=None, add_bos=None, add_eos=None, reverse=None, enable_sampling=None, nbest_size=None, alpha=None)\n",
      " |  \n",
      " |  unk_id(self)\n",
      " |  \n",
      " |  vocab_size(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __swig_destroy__ = delete_SentencePieceProcessor(...)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  thisown\n",
      " |      The membership flag\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = 'Beginners BBQ Class Taking Place in Missoula!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Beginn', 'ers', '▁BBQ', '▁Class', '▁', 'Taking', '▁Place', '▁in', '▁Miss', 'oul', 'a', '!']\n",
      "[12847, 277, 15068, 4501, 3, 12297, 3399, 16, 5964, 7115, 9, 55]\n",
      "Beginners BBQ Class Taking Place in Missoula!\n",
      "Beginners\n"
     ]
    }
   ],
   "source": [
    "# encode: text => id\n",
    "print(sp.encode_as_pieces(s0))\n",
    "print(sp.encode_as_ids(s0))\n",
    "\n",
    "# decode: id => text\n",
    "print(sp.decode_pieces(sp.encode_as_pieces(s0)))\n",
    "print(sp.decode_ids([12847, 277]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece for ID 15068: ▁BBQ\n",
      "ID for Sentence Piece ▁BBQ: 15068\n",
      "ID for unknown text __MUST_BE_UNKNOWN__: 2\n"
     ]
    }
   ],
   "source": [
    "uid = 15068\n",
    "spiece = \"\\u2581BBQ\"\n",
    "unknown = \"__MUST_BE_UNKNOWN__\"\n",
    "\n",
    "# id <=> piece conversion\n",
    "print(f'SentencePiece for ID {uid}: {sp.id_to_piece(uid)}')\n",
    "print(f'ID for Sentence Piece {spiece}: {sp.piece_to_id(spiece)}')\n",
    "\n",
    "# returns 0 for unknown tokens (we can change the id for UNK)\n",
    "print(f'ID for unknown text {unknown}: {sp.piece_to_id(unknown)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of sentence id: -1\n",
      "Pad id: 0\n",
      "End of sentence id: 1\n",
      "Unknown id: 2\n",
      "Vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning of sentence id: {sp.bos_id()}')\n",
    "print(f'Pad id: {sp.pad_id()}')\n",
    "print(f'End of sentence id: {sp.eos_id()}')\n",
    "print(f'Unknown id: {sp.unk_id()}')\n",
    "print(f'Vocab size: {sp.vocab_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Id\tSentP\tControl?\n",
      "------------------------\n",
      "0\t<pad>\tTrue\n",
      "1\t</s>\tTrue\n",
      "2\t<unk>\tFalse\n",
      "3\t▁\tFalse\n",
      "4\tX\tFalse\n",
      "5\t.\tFalse\n",
      "6\t,\tFalse\n",
      "7\ts\tFalse\n",
      "8\t▁the\tFalse\n",
      "9\ta\tFalse\n"
     ]
    }
   ],
   "source": [
    "print('\\nId\\tSentP\\tControl?')\n",
    "print('------------------------')\n",
    "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
    "# <s> and </s> are defined as 'control' symbol.\n",
    "for uid in range(10):\n",
    "    print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')\n",
    "    \n",
    "# for uid in range(sp.vocab_size()-10,sp.vocab_size()):\n",
    "#     print(uid, sp.id_to_piece(uid), sp.is_control(uid), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** BPE ***\n",
      "['▁B', 'e', 'ginn', 'ers', '▁BBQ', '▁Cl', 'ass', '▁T', 'ak', 'ing', '▁P', 'la', 'ce', '▁in', '▁M', 'is', 's', 'ou', 'la', '!']\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=example.txt --model_prefix=example_bpe --vocab_size=450 --model_type=bpe')\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load('example_bpe.model')\n",
    "\n",
    "print('*** BPE ***')\n",
    "print(sp_bpe.encode_as_pieces(s0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁B e g in n ers: 1, ▁BBQ: 3, ▁Cl ass: 2, ▁T ak ing: 1, ▁P la ce: 1, ▁in: 15, ▁M is s ou la !: 1, ▁D o: 1, ▁you: 13, ▁w an t: 1, ▁to: 33, ▁g et: 2, ▁be t ter: 2, ▁a t: 1, ▁mak ing: 2, ▁d e l ic i ou s: 1, ▁BBQ ?: 1, ▁ Y ou: 1, ▁will: 6, ▁have: 4, ▁the: 31, "
     ]
    }
   ],
   "source": [
    "show_vocab(sp_vocab, end = ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heapsort(iterable):\n",
    "    h = []\n",
    "    for value in iterable:\n",
    "        heappush(h, value)\n",
    "    return [heappop(h) for i in range(len(h))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 2, 3, 3, 4, 4]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,4,3,1,3,2,1,4,2]\n",
    "heapsort(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb - sentencepiece repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import string\n",
    "import ast\n",
    "import numpy as np\n",
    "import trax \n",
    "from trax.supervised import decoding\n",
    "import textwrap \n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_jsons = list(map(ast.literal_eval, open('data.txt')))\n",
    "\n",
    "natural_language_texts = [example_json['text'] for example_json in example_jsons]\n",
    "\n",
    "PAD, EOS, UNK = 0, 1, 2\n",
    "\n",
    "def detokenize(np_array):\n",
    "    return trax.data.detokenize(\n",
    "        np_array,\n",
    "        vocab_type='sentencepiece',\n",
    "        vocab_file='sentencepiece.model',\n",
    "        vocab_dir='.')\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return next(trax.data.tokenize(\n",
    "        iter([s]),\n",
    "        vocab_type='sentencepiece',\n",
    "        vocab_file='sentencepiece.model',\n",
    "        vocab_dir='.'))\n",
    " \n",
    "    \n",
    "vocab_size = trax.data.vocab_size(\n",
    "    vocab_type='sentencepiece',\n",
    "    vocab_file='sentencepiece.model',\n",
    "    vocab_dir='.')\n",
    "\n",
    "\n",
    "def get_sentinels(vocab_size, display=False):\n",
    "    sentinels = {}\n",
    "    for i, char in enumerate(reversed(string.ascii_letters), 1):\n",
    "        decoded_text = detokenize([vocab_size - i]) \n",
    "        # Sentinels, ex: <Z> - <a>\n",
    "        sentinels[decoded_text] = f'<{char}>'    \n",
    "        if display:\n",
    "            print(f'The sentinel is <{char}> and the decoded token is:', decoded_text)\n",
    "    return sentinels\n",
    "\n",
    "\n",
    "sentinels = get_sentinels(vocab_size, display=False)    \n",
    "\n",
    "\n",
    "def pretty_decode(encoded_str_list, sentinels=sentinels):\n",
    "    # If already a string, just do the replacements.\n",
    "    if isinstance(encoded_str_list, (str, bytes)):\n",
    "        for token, char in sentinels.items():\n",
    "            encoded_str_list = encoded_str_list.replace(token, char)\n",
    "        return encoded_str_list\n",
    "  \n",
    "    # We need to decode and then prettyfy it.\n",
    "    return pretty_decode(detokenize(encoded_str_list))\n",
    "\n",
    "\n",
    "inputs_targets_pairs = []\n",
    "\n",
    "# here you are reading already computed input/target pairs from a file\n",
    "with open ('inputs_targets_pairs_file.txt', 'rb') as fp:\n",
    "    inputs_targets_pairs = pickle.load(fp)  \n",
    "\n",
    "\n",
    "def display_input_target_pairs(inputs_targets_pairs):\n",
    "    for i, inp_tgt_pair in enumerate(inputs_targets_pairs, 1):\n",
    "        inps, tgts = inp_tgt_pair\n",
    "        inps, tgts = pretty_decode(inps), pretty_decode(tgts)\n",
    "        print(f'[{i}]\\n'\n",
    "              f'inputs:\\n{wrapper.fill(text=inps)}\\n\\n'\n",
    "              f'targets:\\n{wrapper.fill(text=tgts)}\\n\\n\\n\\n')\n",
    "    \n",
    "display_input_target_pairs(inputs_targets_pairs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "model = trax.models.Transformer(\n",
    "    d_ff = 4096,\n",
    "    d_model = 1024,\n",
    "    max_len = 2048,\n",
    "    n_heads = 16,\n",
    "    dropout = 0.1,\n",
    "    input_vocab_size = 32000,\n",
    "    n_encoder_layers = 24,\n",
    "    n_decoder_layers = 24,\n",
    "    mode='predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load in the model\n",
    "# this takes about 1 minute\n",
    "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)  # Needed in predict mode.\n",
    "model.init_from_file('model.pkl.gz',\n",
    "                     weights_only=True, input_signature=(shape11, shape11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the 3rd example\n",
    "c4_input = inputs_targets_pairs[2][0]\n",
    "c4_target = inputs_targets_pairs[2][1]\n",
    "\n",
    "print('pretty_decoded input: \\n\\n', pretty_decode(c4_input))\n",
    "print('\\npretty_decoded target: \\n\\n', pretty_decode(c4_target))\n",
    "print('\\nc4_input:\\n\\n', c4_input)\n",
    "print('\\nc4_target:\\n\\n', c4_target)\n",
    "print(len(c4_target))\n",
    "print(len(pretty_decode(c4_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature is a parameter for sampling.\n",
    "#   # * 0.0: same as argmax, always pick the most probable token\n",
    "#   # * 1.0: sampling from the distribution (can sometimes say random things)\n",
    "#   # * values inbetween can trade off diversity and quality, try it out!\n",
    "output = decoding.autoregressive_sample(model, inputs=np.array(c4_input)[None, :],\n",
    "                                        temperature=0.0, max_length=5) # originally max_length = 50\n",
    "print(wrapper.fill(pretty_decode(output[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 SQUAD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import t5\n",
    "import numpy as np\n",
    "import trax \n",
    "from trax.supervised import decoding\n",
    "import textwrap \n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD, EOS, UNK = 0, 1, 2\n",
    "\n",
    "\n",
    "def detokenize(np_array):\n",
    "    return trax.data.detokenize(\n",
    "        np_array,\n",
    "        vocab_type='sentencepiece',\n",
    "        vocab_file='sentencepiece.model',\n",
    "        vocab_dir='.')\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return next(trax.data.tokenize(\n",
    "        iter([s]),\n",
    "        vocab_type='sentencepiece',\n",
    "        vocab_file='sentencepiece.model',\n",
    "        vocab_dir='.'))\n",
    " \n",
    "    \n",
    "vocab_size = trax.data.vocab_size(\n",
    "    vocab_type='sentencepiece',\n",
    "    vocab_file='sentencepiece.model',\n",
    "    vocab_dir='.')\n",
    "\n",
    "\n",
    "def get_sentinels(vocab_size, display=False):\n",
    "    sentinels = {}\n",
    "    for i, char in enumerate(reversed(string.ascii_letters), 1):\n",
    "        decoded_text = detokenize([vocab_size - i]) \n",
    "        # Sentinels, ex: <Z> - <a>\n",
    "        sentinels[decoded_text] = f'<{char}>'    \n",
    "        if display:\n",
    "            print(f'The sentinel is <{char}> and the decoded token is:', decoded_text)\n",
    "    return sentinels\n",
    "\n",
    "\n",
    "sentinels = get_sentinels(vocab_size, display=False)    \n",
    "\n",
    "\n",
    "def pretty_decode(encoded_str_list, sentinels=sentinels):\n",
    "    # If already a string, just do the replacements.\n",
    "    if isinstance(encoded_str_list, (str, bytes)):\n",
    "        for token, char in sentinels.items():\n",
    "            encoded_str_list = encoded_str_list.replace(token, char)\n",
    "        return encoded_str_list\n",
    "  \n",
    "    # We need to decode and then prettyfy it.\n",
    "    return pretty_decode(detokenize(encoded_str_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Fine-tuning on SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Question, C, A and return \"question: Q context: C\" as input and \"A\" as target.\n",
    "def squad_preprocess_fn(dataset, mode='train'):\n",
    "    return t5.data.preprocessors.squad(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train generator, this takes about 1 minute\n",
    "train_generator_fn, eval_generator_fn = trax.data.tf_inputs.data_streams(\n",
    "  'squad/plain_text:1.0.0',\n",
    "  data_dir='data/',\n",
    "  bare_preprocess_fn=squad_preprocess_fn,\n",
    "  input_name='inputs',\n",
    "  target_name='targets'\n",
    ")\n",
    "\n",
    "train_generator = train_generator_fn()\n",
    "next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print example from train_generator\n",
    "(inp, out) = next(train_generator)\n",
    "print(inp.decode('utf8').split('context:')[0])\n",
    "print()\n",
    "print('context:', inp.decode('utf8').split('context:')[1])\n",
    "print()\n",
    "print('target:', out.decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model \n",
    "model = trax.models.Transformer(\n",
    "    d_ff = 4096,\n",
    "    d_model = 1024,\n",
    "    max_len = 2048,\n",
    "    n_heads = 16,\n",
    "    dropout = 0.1,\n",
    "    input_vocab_size = 32000,\n",
    "    n_encoder_layers = 24,\n",
    "    n_decoder_layers = 24,\n",
    "    mode='predict')  # Change to 'eval' for slow decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the model\n",
    "# this will take a minute\n",
    "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
    "model.init_from_file('model_squad.pkl.gz',\n",
    "                     weights_only=True, input_signature=(shape11, shape11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inputs\n",
    "# a simple example \n",
    "# inputs = 'question: She asked him where is john? context: John was at the game'\n",
    "\n",
    "# an extensive example\n",
    "inputs = 'question: What are some of the colours of a rose? context: A rose is a woody perennial flowering plant of the genus Rosa, in the family Rosaceae, or the flower it bears.There are over three hundred species and tens of thousands of cultivars. They form a group of plants that can be erect shrubs, climbing, or trailing, with stems that are often armed with sharp prickles. Flowers vary in size and shape and are usually large and showy, in colours ranging from white through yellows and reds. Most species are native to Asia, with smaller numbers native to Europe, North America, and northwestern Africa. Species, cultivars and hybrids are all widely grown for their beauty and often are fragrant.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the input so we could feed it for decoding\n",
    "print(tokenize(inputs))\n",
    "test_inputs = tokenize(inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature is a parameter for sampling.\n",
    "#   # * 0.0: same as argmax, always pick the most probable token\n",
    "#   # * 1.0: sampling from the distribution (can sometimes say random things)\n",
    "#   # * values inbetween can trade off diversity and quality, try it out!\n",
    "output = decoding.autoregressive_sample(model, inputs=np.array(test_inputs)[None, :],\n",
    "                                        temperature=0.0, max_length=5) # originally max_length=10\n",
    "print(wrapper.fill(pretty_decode(output[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import string\n",
    "import textwrap\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import trax \n",
    "from trax import layers as tl\n",
    "from trax.supervised import decoding\n",
    "\n",
    "# Will come handy later.\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is part of https://www.tensorflow.org/datasets/catalog/c4 (C4 Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example jsons\n",
    "example_jsons = list(map(ast.literal_eval, open('data_2.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the examples to see how the data looks like\n",
    "for i in range(5):\n",
    "    print(f'example number {i+1}: \\n\\n{example_jsons[i]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(example_jsons[0].get('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab text field from dictionary\n",
    "natural_language_texts = [example_json['text'] for example_json in example_jsons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First text example\n",
    "natural_language_texts[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode to natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "PAD, EOS, UNK = 0, 1, 2\n",
    "\n",
    "def detokenize(np_array):\n",
    "    return trax.data.detokenize(\n",
    "        np_array,\n",
    "        vocab_type='sentencepiece',\n",
    "        vocab_file='sentencepiece.model',\n",
    "        vocab_dir='.')\n",
    "\n",
    "def tokenize(s):\n",
    "  # The trax.data.tokenize function operates on streams,\n",
    "  # that's why we have to create 1-element stream with iter\n",
    "  # and later retrieve the result with next.\n",
    "    return next(trax.data.tokenize(\n",
    "        iter([s]),\n",
    "        vocab_type='sentencepiece',\n",
    "        vocab_file='sentencepiece.model',\n",
    "        vocab_dir='.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the encoding of each word to see how subwords are tokenized\n",
    "tokenized_text = [(tokenize(word).tolist(), word) for word in natural_language_texts[0].split()]\n",
    "print(tokenized_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that detokenize successfully undoes the tokenization\n",
    "print(f\"tokenized: {tokenize('Beginners')}\\ndetokenized: {detokenize(tokenize('Beginners'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = trax.data.vocab_size(\n",
    "    vocab_type='sentencepiece',\n",
    "    vocab_file='sentencepiece.model',\n",
    "    vocab_dir='.')\n",
    "\n",
    "def get_sentinels(vocab_size=vocab_size, display=False):\n",
    "    sentinels = {}\n",
    "    for i, char in enumerate(reversed(string.ascii_letters), 1):\n",
    "        decoded_text = detokenize([vocab_size - i]) \n",
    "        \n",
    "        # Sentinels, ex: <Z> - <a>\n",
    "        sentinels[decoded_text] = f'<{char}>'    \n",
    "    \n",
    "        if display:\n",
    "            print(f'The sentinel is <{char}> and the decoded token is:', decoded_text)\n",
    "\n",
    "    return sentinels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinels = get_sentinels(vocab_size, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_decode(encoded_str_list, sentinels=sentinels):\n",
    "    # If already a string, just do the replacements.\n",
    "    if isinstance(encoded_str_list, (str, bytes)):\n",
    "        for token, char in sentinels.items():\n",
    "            encoded_str_list = encoded_str_list.replace(token, char)\n",
    "        return encoded_str_list\n",
    "  \n",
    "    # We need to decode and then prettyfy it.\n",
    "    return pretty_decode(detokenize(encoded_str_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_decode(\"I want to dress up as an Intellectual this halloween.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing and Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: tokenize_and_mask\n",
    "def tokenize_and_mask(text, vocab_size=vocab_size, noise=0.15, \n",
    "                      randomizer=np.random.uniform, tokenize=tokenize):\n",
    "    \"\"\"Tokenizes and masks a given input.\n",
    "\n",
    "    Args:\n",
    "        text (str or bytes): Text input.\n",
    "        vocab_size (int, optional): Size of the vocabulary. Defaults to vocab_size.\n",
    "        noise (float, optional): Probability of masking a token. Defaults to 0.15.\n",
    "        randomizer (function, optional): Function that generates random values. Defaults to np.random.uniform.\n",
    "        tokenize (function, optional): Tokenizer function. Defaults to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple of lists of integers associated to inputs and targets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # current sentinel number (starts at 0)\n",
    "    cur_sentinel_num = 0\n",
    "    # inputs\n",
    "    inps = []\n",
    "    # targets\n",
    "    targs = []\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "    # prev_no_mask is True if the previous token was NOT masked, False otherwise\n",
    "    # set prev_no_mask to True\n",
    "    prev_no_mask = True\n",
    "    \n",
    "    # loop through tokenized `text`\n",
    "    for token in tokenize(text):\n",
    "        # check if the `noise` is greater than a random value (weighted coin flip)\n",
    "        if randomizer() < noise:\n",
    "            # check to see if the previous token was not masked\n",
    "            if prev_no_mask==True: # add new masked token at end_id\n",
    "                # number of masked tokens increases by 1\n",
    "                cur_sentinel_num += 1\n",
    "                # compute `end_id` by subtracting current sentinel value out of the total vocabulary size\n",
    "                end_id = vocab_size - cur_sentinel_num \n",
    "                # append `end_id` at the end of the targets\n",
    "                targs.append(end_id)\n",
    "                # append `end_id` at the end of the inputs\n",
    "                inps.append(end_id)\n",
    "            # append `token` at the end of the targets\n",
    "            targs.append(token)\n",
    "            # set prev_no_mask accordingly\n",
    "            prev_no_mask = False\n",
    "        \n",
    "        else: # don't have two masked tokens in a row\n",
    "            # append `token ` at the end of the inputs\n",
    "            inps.append(token)\n",
    "            # set prev_no_mask accordingly\n",
    "            prev_no_mask = True\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return inps, targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some logic to mock a np.random value generator\n",
    "# Needs to be in the same cell for it to always generate same output\n",
    "def testing_rnd():\n",
    "    def dummy_generator():\n",
    "        vals = np.linspace(0, 1, 10)\n",
    "        cyclic_vals = itertools.cycle(vals)\n",
    "        for _ in range(100):\n",
    "            yield next(cyclic_vals)\n",
    "\n",
    "    dumr = itertools.cycle(dummy_generator())\n",
    "\n",
    "    def dummy_randomizer():\n",
    "        return next(dumr)\n",
    "    \n",
    "    return dummy_randomizer\n",
    "\n",
    "input_str = natural_language_texts[0]\n",
    "print(f\"input string:\\n\\n{input_str}\\n\")\n",
    "inps, targs = tokenize_and_mask(input_str, randomizer=testing_rnd())\n",
    "print(f\"tokenized inputs:\\n\\n{inps}\\n\")\n",
    "print(f\"targets:\\n\\n{targs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Inputs: \\n\\n', pretty_decode(inps))\n",
    "print('\\nTargets: \\n\\n', pretty_decode(targs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Creating the Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenize_and_mask\n",
    "inputs_targets_pairs = [tokenize_and_mask(text) for text in natural_language_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_input_target_pairs(inputs_targets_pairs):\n",
    "    for i, inp_tgt_pair in enumerate(inputs_targets_pairs, 1):\n",
    "        inps, tgts = inp_tgt_pair\n",
    "        inps, tgts = pretty_decode(inps), pretty_decode(tgts)\n",
    "        print(f'[{i}]\\n\\n'\n",
    "              f'inputs:\\n{wrapper.fill(text=inps)}\\n\\n'\n",
    "              f'targets:\\n{wrapper.fill(text=tgts)}\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_input_target_pairs(inputs_targets_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: FeedForwardBlock\n",
    "def FeedForwardBlock(d_model, d_ff, dropout, dropout_shared_axes, mode, activation):\n",
    "    \"\"\"Returns a list of layers implementing a feed-forward block.\n",
    "    Args:\n",
    "        d_model: int:  depth of embedding\n",
    "        d_ff: int: depth of feed-forward layer\n",
    "        dropout: float: dropout rate (how much to drop out)\n",
    "        dropout_shared_axes: list of integers, axes to share dropout mask\n",
    "        mode: str: 'train' or 'eval'\n",
    "        activation: the non-linearity in feed-forward layer\n",
    "    Returns:\n",
    "        A list of layers which maps vectors to vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    dropout_middle = tl.Dropout(rate=dropout,\n",
    "                                shared_axes=dropout_shared_axes, \n",
    "                                mode=mode)\n",
    "  \n",
    "    dropout_final = tl.Dropout(rate=dropout, \n",
    "                               shared_axes=dropout_shared_axes, \n",
    "                               mode=mode)\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "    ff_block = [ \n",
    "        # trax Layer normalization \n",
    "        tl.LayerNorm(), # @REPLACE None,\n",
    "        # trax Dense layer using `d_ff`\n",
    "        tl.Dense(d_ff), # @REPLACE None,\n",
    "        # activation() layer - you need to call (use parentheses) this func!\n",
    "        activation(), # @REPLACE None,\n",
    "        # dropout middle layer\n",
    "        dropout_middle, # @REPLACE None,\n",
    "        # trax Dense layer using `d_model`\n",
    "        tl.Dense(d_model), # @REPLACE None,\n",
    "        # dropout final layer\n",
    "        dropout_final, # @REPLACE None,\n",
    "    ]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return ff_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the block layout\n",
    "feed_forward_example = FeedForwardBlock(d_model=512, d_ff=2048, dropout=0.8, dropout_shared_axes=0, mode = 'train', activation = tl.Relu)\n",
    "print(feed_forward_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: EncoderBlock\n",
    "def EncoderBlock(d_model, d_ff, n_heads, dropout, dropout_shared_axes,\n",
    "                  mode, ff_activation, FeedForwardBlock=FeedForwardBlock):\n",
    "    \"\"\"\n",
    "    Returns a list of layers that implements a Transformer encoder block.\n",
    "    The input to the layer is a pair, (activations, mask), where the mask was\n",
    "    created from the original source tokens to prevent attending to the padding\n",
    "    part of the input.\n",
    "    \n",
    "    Args:\n",
    "        d_model (int): depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        dropout_shared_axes (int): axes on which to share dropout mask.\n",
    "        mode (str): 'train' or 'eval'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "        FeedForwardBlock (function): A function that returns the feed forward block.\n",
    "    Returns:\n",
    "        list: A list of layers that maps (activations, mask) to (activations, mask).\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "    # Attention block\n",
    "    attention = tl.Attention( \n",
    "        # Use dimension of the model\n",
    "        d_feature=d_model,\n",
    "        # Set it equal to number of attention heads\n",
    "        n_heads=n_heads,\n",
    "        # Set it equal `dropout`\n",
    "        dropout=dropout,\n",
    "        # Set it equal `mode`\n",
    "        mode=mode\n",
    "    )\n",
    "    \n",
    "    # Call the function `FeedForwardBlock` (implemented before) and pass in the parameters\n",
    "    feed_forward = FeedForwardBlock( \n",
    "        d_model,\n",
    "        d_ff,\n",
    "        dropout,\n",
    "        dropout_shared_axes,\n",
    "        mode,\n",
    "        ff_activation \n",
    "    )\n",
    "    \n",
    "    # Dropout block\n",
    "    dropout_ = tl.Dropout( \n",
    "        # set it equal to `dropout`\n",
    "        rate=dropout,\n",
    "        # set it equal to the axes on which to share dropout mask\n",
    "        shared_axes=dropout_shared_axes,\n",
    "        # set it equal to `mode`\n",
    "        mode=mode\n",
    "    )\n",
    "    \n",
    "    encoder_block = [ \n",
    "        # add `Residual` layer\n",
    "        tl.Residual(\n",
    "            # add norm layer\n",
    "            tl.LayerNorm(),\n",
    "            # add attention\n",
    "            attention,\n",
    "            # add dropout\n",
    "            dropout_,\n",
    "        ),\n",
    "        # add another `Residual` layer\n",
    "        tl.Residual(\n",
    "            # add feed forward\n",
    "            feed_forward,\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return encoder_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the block layout\n",
    "encoder_example = EncoderBlock(d_model=512, d_ff=2048, n_heads=6, dropout=0.8, dropout_shared_axes=0, mode = 'train', ff_activation=tl.Relu)\n",
    "print(encoder_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: TransformerEncoder\n",
    "def TransformerEncoder(vocab_size=vocab_size,\n",
    "                       n_classes=10,\n",
    "                       d_model=512,\n",
    "                       d_ff=2048,\n",
    "                       n_layers=6,\n",
    "                       n_heads=8,\n",
    "                       dropout=0.1,\n",
    "                       dropout_shared_axes=None,\n",
    "                       max_len=2048,\n",
    "                       mode='train',\n",
    "                       ff_activation=tl.Relu,\n",
    "                      EncoderBlock=EncoderBlock):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a Transformer encoder model.\n",
    "    The input to the model is a tensor of tokens.\n",
    "  \n",
    "    Args:\n",
    "        vocab_size (int): vocab size. Defaults to vocab_size.\n",
    "        n_classes (int): how many classes on output. Defaults to 10.\n",
    "        d_model (int): depth of embedding. Defaults to 512.\n",
    "        d_ff (int): depth of feed-forward layer. Defaults to 2048.\n",
    "        n_layers (int): number of encoder/decoder layers. Defaults to 6.\n",
    "        n_heads (int): number of attention heads. Defaults to 8.\n",
    "        dropout (float): dropout rate (how much to drop out). Defaults to 0.1.\n",
    "        dropout_shared_axes (int): axes on which to share dropout mask. Defaults to None.\n",
    "        max_len (int): maximum symbol length for positional encoding. Defaults to 2048.\n",
    "        mode (str): 'train' or 'eval'. Defaults to 'train'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer. Defaults to tl.Relu.\n",
    "        EncoderBlock (function): Returns the encoder block. Defaults to EncoderBlock.\n",
    "  \n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A Transformer model as a layer that maps\n",
    "        from a tensor of tokens to activations over a set of output classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    positional_encoder = [\n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        tl.Dropout(rate=dropout, shared_axes=dropout_shared_axes, mode=mode),\n",
    "        tl.PositionalEncoding(max_len=max_len)\n",
    "    ]\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "    # Use the function `EncoderBlock` (implemented above) and pass in the parameters over `n_layers`\n",
    "    encoder_blocks = [EncoderBlock(d_model, d_ff, n_heads, dropout,\n",
    "                        dropout_shared_axes, mode, ff_activation) for _ in range(n_layers)]\n",
    "\n",
    "    # Assemble and return the model.\n",
    "    return tl.Serial(\n",
    "        # Encode\n",
    "        tl.Branch(\n",
    "            # Use `positional_encoder`\n",
    "            positional_encoder,\n",
    "            # Use trax padding mask\n",
    "            tl.PaddingMask(),\n",
    "        ),\n",
    "        # Use `encoder_blocks`\n",
    "        encoder_blocks,\n",
    "        # Use select layer\n",
    "        tl.Select([0], n_in=2),\n",
    "        # Use trax layer normalization\n",
    "        tl.LayerNorm(),\n",
    "        # Map to output categories.\n",
    "        # Use trax mean. set axis to 1\n",
    "        tl.Mean(axis=1),\n",
    "        # Use trax Dense using `n_classes`\n",
    "        tl.Dense(n_classes),\n",
    "        # Use trax log softmax\n",
    "        tl.LogSoftmax(),\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the structure of your model\n",
    "# Only 1 layer is used to keep the output readable\n",
    "TransformerEncoder(n_layers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see this model in action continue to the next 2 ungraded labs - BERT loss and T5 Squad Dataset done above. We strongly recommend you to try the colab versions of them as they will yield a much smoother experience. The links to the colabs can be found within the ungraded labs or if you already know how to open files within colab here are some shortcuts (if not, head to the ungraded labs which contain some extra instructions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
