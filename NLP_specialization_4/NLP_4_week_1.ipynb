{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Semantics in Trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np              # regular ol' numpy\n",
    "from trax import layers as tl   # core building block\n",
    "from trax import shapes         # data signatures: dimensionality and type\n",
    "from trax import fastmath       # uses jax, offers numpy on steroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The tl.Serial Combinator is Stack Oriented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Addition\n",
      "expected inputs : 2\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : [3] \n",
      "\n",
      "y : [4] \n",
      "\n",
      "-- Outputs --\n",
      "z : [7]\n"
     ]
    }
   ],
   "source": [
    "def Addition():\n",
    "    layer_name = \"Addition\"  # don't forget to give your custom layer a name to identify\n",
    "\n",
    "    # Custom function for the custom layer\n",
    "    def func(x, y):\n",
    "        return x + y\n",
    "\n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "\n",
    "# Test it\n",
    "add = Addition()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", add.name)\n",
    "print(\"expected inputs :\", add.n_in)\n",
    "print(\"promised outputs :\", add.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([3])\n",
    "y = np.array([4])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "print(\"y :\", y, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "z = add((x, y))\n",
    "print(\"-- Outputs --\")\n",
    "print(\"z :\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Properties --\n",
      "name : Multiplication\n",
      "expected inputs : 2\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : [7] \n",
      "\n",
      "y : [15] \n",
      "\n",
      "-- Outputs --\n",
      "z : [105]\n"
     ]
    }
   ],
   "source": [
    "def Multiplication():\n",
    "    layer_name = (\n",
    "        \"Multiplication\"  # don't forget to give your custom layer a name to identify\n",
    "    )\n",
    "\n",
    "    # Custom function for the custom layer\n",
    "    def func(x, y):\n",
    "        return x * y\n",
    "\n",
    "    return tl.Fn(layer_name, func)\n",
    "\n",
    "\n",
    "# Test it\n",
    "mul = Multiplication()\n",
    "\n",
    "# Inspect properties\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", mul.name)\n",
    "print(\"expected inputs :\", mul.n_in)\n",
    "print(\"promised outputs :\", mul.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "x = np.array([7])\n",
    "y = np.array([15])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "print(\"y :\", y, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "z = mul((x, y))\n",
    "print(\"-- Outputs --\")\n",
    "print(\"z :\", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Serial Model --\n",
      "Serial_in4[\n",
      "  Addition_in2\n",
      "  Multiplication_in2\n",
      "  Addition_in2\n",
      "] \n",
      "\n",
      "-- Properties --\n",
      "name : Serial\n",
      "sublayers : [Addition_in2, Multiplication_in2, Addition_in2]\n",
      "expected inputs : 4\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : (array([3]), array([4]), array([15]), array([3])) \n",
      "\n",
      "-- Outputs --\n",
      "y : [108]\n"
     ]
    }
   ],
   "source": [
    "# Serial combinator\n",
    "serial = tl.Serial(\n",
    "    Addition(), Multiplication(), Addition()  # add 3 + 4  # multiply result by 15\n",
    ")\n",
    "\n",
    "# Initialization\n",
    "x = (np.array([3]), np.array([4]), np.array([15]), np.array([3]))  # input\n",
    "\n",
    "serial.init(shapes.signature(x))  # initializing serial instance\n",
    "\n",
    "\n",
    "print(\"-- Serial Model --\")\n",
    "print(serial, \"\\n\")\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", serial.name)\n",
    "print(\"sublayers :\", serial.sublayers)\n",
    "print(\"expected inputs :\", serial.n_in)\n",
    "print(\"promised outputs :\", serial.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = serial(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The tl.Select combinator in the context of the Serial combinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Serial Model --\n",
      "Serial_in2[\n",
      "  Select[0,1,0,1]_in2_out4\n",
      "  Addition_in2\n",
      "  Multiplication_in2\n",
      "  Addition_in2\n",
      "] \n",
      "\n",
      "-- Properties --\n",
      "name : Serial\n",
      "sublayers : [Select[0,1,0,1]_in2_out4, Addition_in2, Multiplication_in2, Addition_in2]\n",
      "expected inputs : 2\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : (array([3]), array([4])) \n",
      "\n",
      "-- Outputs --\n",
      "y : [25]\n"
     ]
    }
   ],
   "source": [
    "serial = tl.Serial(tl.Select([0, 1, 0, 1]), Addition(), Multiplication(), Addition())\n",
    "\n",
    "# Initialization\n",
    "x = (np.array([3]), np.array([4]))  # input\n",
    "\n",
    "serial.init(shapes.signature(x))  # initializing serial instance\n",
    "\n",
    "\n",
    "print(\"-- Serial Model --\")\n",
    "print(serial, \"\\n\")\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", serial.name)\n",
    "print(\"sublayers :\", serial.sublayers)\n",
    "print(\"expected inputs :\", serial.n_in)\n",
    "print(\"promised outputs :\", serial.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = serial(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Serial Model --\n",
      "Serial_in2[\n",
      "  Select[0,1,0,1]_in2_out4\n",
      "  Addition_in2\n",
      "  Select[0]_in2\n",
      "  Multiplication_in2\n",
      "] \n",
      "\n",
      "-- Properties --\n",
      "name : Serial\n",
      "sublayers : [Select[0,1,0,1]_in2_out4, Addition_in2, Select[0]_in2, Multiplication_in2]\n",
      "expected inputs : 2\n",
      "promised outputs : 1 \n",
      "\n",
      "-- Inputs --\n",
      "x : (array([3]), array([4])) \n",
      "\n",
      "-- Outputs --\n",
      "y : [28]\n"
     ]
    }
   ],
   "source": [
    "serial = tl.Serial(\n",
    "    tl.Select([0, 1, 0, 1]), Addition(), tl.Select([0], n_in=2), Multiplication()\n",
    ")\n",
    "\n",
    "# Initialization\n",
    "x = (np.array([3]), np.array([4]))  # input\n",
    "\n",
    "serial.init(shapes.signature(x))  # initializing serial instance\n",
    "\n",
    "\n",
    "print(\"-- Serial Model --\")\n",
    "print(serial, \"\\n\")\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", serial.name)\n",
    "print(\"sublayers :\", serial.sublayers)\n",
    "print(\"expected inputs :\", serial.n_in)\n",
    "print(\"promised outputs :\", serial.n_out, \"\\n\")\n",
    "\n",
    "# Inputs\n",
    "print(\"-- Inputs --\")\n",
    "print(\"x :\", x, \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = serial(x)\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The tl.Residual combinator in the context of the Serial combinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Serial Model --\n",
      "Serial_in2_out3[\n",
      "  Select[0,1,0,1]_in2_out4\n",
      "  Serial_in2[\n",
      "    Branch_in2_out2[\n",
      "      None\n",
      "      Addition_in2\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "] \n",
      "\n",
      "-- Properties --\n",
      "name : Serial\n",
      "expected inputs : 2\n",
      "promised outputs : 3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's define a Serial network\n",
    "serial = tl.Serial(\n",
    "    # Practice using Select again by duplicating the first two inputs\n",
    "    tl.Select([0, 1, 0, 1]),\n",
    "    # Place a Residual layer that skips over the Fn: Addition() layer\n",
    "    tl.Residual(Addition())\n",
    ")\n",
    "\n",
    "print(\"-- Serial Model --\")\n",
    "print(serial, \"\\n\")\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", serial.name)\n",
    "print(\"expected inputs :\", serial.n_in)\n",
    "print(\"promised outputs :\", serial.n_out, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Inputs --\n",
      "(x1, x2) : (array([3]), array([4])) \n",
      "\n",
      "-- Outputs --\n",
      "y : (array([10]), array([3]), array([4]))\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "x1 = np.array([3])\n",
    "x2 = np.array([4])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"(x1, x2) :\", (x1, x2), \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = serial((x1, x2))\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Serial Model --\n",
      "Serial_in2_out3[\n",
      "  Select[0,1,0,1]_in2_out4\n",
      "  Serial_in2[\n",
      "    Branch_in2_out2[\n",
      "      None\n",
      "      Multiplication_in2\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "] \n",
      "\n",
      "-- Properties --\n",
      "name : Serial\n",
      "expected inputs : 2\n",
      "promised outputs : 3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model definition\n",
    "serial = tl.Serial(\n",
    "    tl.Select([0, 1, 0, 1]), \n",
    "    tl.Residual(Multiplication())\n",
    ")\n",
    "\n",
    "print(\"-- Serial Model --\")\n",
    "print(serial, \"\\n\")\n",
    "print(\"-- Properties --\")\n",
    "print(\"name :\", serial.name)\n",
    "print(\"expected inputs :\", serial.n_in)\n",
    "print(\"promised outputs :\", serial.n_out, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Inputs --\n",
      "(x1, x2) : (array([3]), array([4])) \n",
      "\n",
      "-- Outputs --\n",
      "y : (array([15]), array([3]), array([4]))\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "x1 = np.array([3])\n",
    "x2 = np.array([4])\n",
    "print(\"-- Inputs --\")\n",
    "print(\"(x1, x2) :\", (x1, x2), \"\\n\")\n",
    "\n",
    "# Outputs\n",
    "y = serial((x1, x2))\n",
    "print(\"-- Outputs --\")\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the Bilingual Evaluation Understudy (BLEU) score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sdeshpande/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Requirement already satisfied: sacrebleu in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (1.4.14)\n",
      "Requirement already satisfied: portalocker in /Users/sdeshpande/opt/anaconda3/lib/python3.8/site-packages (from sacrebleu) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                  # import numpy to make numerical computations.\n",
    "import nltk                         # import NLTK to handle simple NL tasks like tokenization.\n",
    "from nltk.util import ngrams\n",
    "nltk.download('punkt')\n",
    "import math\n",
    "from collections import Counter     # import the Counter module.\n",
    "!pip3 install 'sacrebleu'           # install the sacrebleu package.\n",
    "import sacrebleu                    # import sacrebleu in order compute the BLEU score.\n",
    "import matplotlib.pyplot as plt     # import pyplot in order to make some illustrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_length = np.ones(100)\n",
    "can_length = np.linspace(1.5, 0.5, 100)\n",
    "x = ref_length / can_length\n",
    "y = 1 - x\n",
    "y = np.exp(y)\n",
    "y = np.minimum(np.ones(y.shape), y)\n",
    "\n",
    "# Code for in order to make the plot\n",
    "fig, ax = plt.subplots(1)\n",
    "lines = ax.plot(x, y)\n",
    "ax.set(\n",
    "    xlabel=\"Ratio of the length of the reference to the candidate text\",\n",
    "    ylabel=\"Brevity Penalty\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"1-gram\": 0.8, \"2-gram\": 0.7, \"3-gram\": 0.6, \"4-gram\": 0.5}\n",
    "names = list(data.keys())\n",
    "values = list(data.values())\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "bars = ax.bar(names, values)\n",
    "ax.set(ylabel=\"N-gram precision\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"1-gram\": 0.8, \"2-gram\": 0.77, \"3-gram\": 0.74, \"4-gram\": 0.71}\n",
    "names = list(data.keys())\n",
    "values = list(data.values())\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "bars = ax.bar(names, values)\n",
    "ax.set(ylabel=\"Modified N-gram precision\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Calculations of the BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NASA Opportunity rover is battling a massive dust storm on planet Mars. -> ['the', 'nasa', 'opportunity', 'rover', 'is', 'battling', 'a', 'massive', 'dust', 'storm', 'on', 'planet', 'mars', '.']\n",
      "\n",
      "\n",
      "The Opportunity rover is combating a big sandstorm on planet Mars. -> ['the', 'opportunity', 'rover', 'is', 'combating', 'a', 'big', 'sandstorm', 'on', 'planet', 'mars', '.']\n",
      "\n",
      "\n",
      "A NASA rover is fighting a massive storm on planet Mars. -> ['a', 'nasa', 'rover', 'is', 'fighting', 'a', 'massive', 'storm', 'on', 'planet', 'mars', '.']\n"
     ]
    }
   ],
   "source": [
    "reference = \"The NASA Opportunity rover is battling a massive dust storm on planet Mars.\"\n",
    "candidate_1 = \"The Opportunity rover is combating a big sandstorm on planet Mars.\"\n",
    "candidate_2 = \"A NASA rover is fighting a massive storm on planet Mars.\"\n",
    "\n",
    "tokenized_ref = nltk.word_tokenize(reference.lower())\n",
    "tokenized_cand_1 = nltk.word_tokenize(candidate_1.lower())\n",
    "tokenized_cand_2 = nltk.word_tokenize(candidate_2.lower())\n",
    "\n",
    "print(f\"{reference} -> {tokenized_ref}\")\n",
    "print(\"\\n\")\n",
    "print(f\"{candidate_1} -> {tokenized_cand_1}\")\n",
    "print(\"\\n\")\n",
    "print(f\"{candidate_2} -> {tokenized_cand_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brevity_penalty(candidate, reference):\n",
    "    ref_length = len(reference)\n",
    "    can_length = len(candidate)\n",
    "\n",
    "    # Brevity Penalty\n",
    "    if ref_length < can_length: # if reference length is less than candidate length\n",
    "        BP = 1 # set BP = 1\n",
    "    else:\n",
    "        penalty = 1 - (ref_length / can_length) # else set BP=exp(1-(ref_length/can_length))\n",
    "        BP = np.exp(penalty)\n",
    "\n",
    "    return BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_precision(candidate, reference):\n",
    "    \"\"\"\n",
    "    Clipped precision function given a original and a machine translated sentences\n",
    "    \"\"\"\n",
    "\n",
    "    clipped_precision_score = []\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        ref_n_gram = Counter(ngrams(reference,i))\n",
    "        cand_n_gram = Counter(ngrams(candidate,i))\n",
    "\n",
    "        c = sum(cand_n_gram.values())\n",
    "        \n",
    "        for j in cand_n_gram: # for every n-gram up to 4 in candidate text\n",
    "            if j in ref_n_gram: # check if it is in the reference n-gram\n",
    "                if cand_n_gram[j] > ref_n_gram[j]: # if the count of the candidate n-gram is bigger\n",
    "                                                   # than the corresponding count in the reference n-gram,\n",
    "                    cand_n_gram[j] = ref_n_gram[j] # then set the count of the candidate n-gram to be equal\n",
    "                                                   # to the reference n-gram\n",
    "            else:\n",
    "                cand_n_gram[j] = 0 # else set the candidate n-gram equal to zero\n",
    "\n",
    "        clipped_precision_score.append(sum(cand_n_gram.values())/c)\n",
    "\n",
    "    weights =[0.25]*4\n",
    "\n",
    "    s = (w_i * math.log(p_i) for w_i, p_i in zip(weights, clipped_precision_score))\n",
    "    s = math.exp(math.fsum(s))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(candidate, reference):\n",
    "    BP = brevity_penalty(candidate, reference)\n",
    "    precision = clipped_precision(candidate, reference)\n",
    "    return BP * precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results reference versus candidate 1 our own code BLEU:  27.6\n",
      "Results reference versus candidate 2 our own code BLEU:  35.3\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Results reference versus candidate 1 our own code BLEU: \",\n",
    "    round(bleu_score(tokenized_cand_1, tokenized_ref) * 100, 1),\n",
    ")\n",
    "print(\n",
    "    \"Results reference versus candidate 2 our own code BLEU: \",\n",
    "    round(bleu_score(tokenized_cand_2, tokenized_ref) * 100, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results reference versus candidate 1 sacrebleu library BLEU:  27.6\n",
      "Results reference versus candidate 2 sacrebleu library BLEU:  35.3\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Results reference versus candidate 1 sacrebleu library BLEU: \",\n",
    "    round(sacrebleu.corpus_bleu(candidate_1, reference).score, 1),\n",
    ")\n",
    "print(\n",
    "    \"Results reference versus candidate 2 sacrebleu library BLEU: \",\n",
    "    round(sacrebleu.corpus_bleu(candidate_2, reference).score, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: BLEU computation on a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-63705bf5f17f>:2: DeprecationWarning: 'U' mode is deprecated\n",
      "  wmt19_src = open(\"wmt19_src.txt\", \"rU\")\n",
      "<ipython-input-22-63705bf5f17f>:5: DeprecationWarning: 'U' mode is deprecated\n",
      "  wmt19_ref = open(\"wmt19_ref.txt\", \"rU\")\n",
      "<ipython-input-22-63705bf5f17f>:8: DeprecationWarning: 'U' mode is deprecated\n",
      "  wmt19_can = open(\"wmt19_can.txt\", \"rU\")\n"
     ]
    }
   ],
   "source": [
    "# Loading the raw data\n",
    "wmt19_src = open(\"wmt19_src.txt\", \"rU\")\n",
    "wmt19_src_1 = wmt19_src.read()\n",
    "wmt19_src.close()\n",
    "wmt19_ref = open(\"wmt19_ref.txt\", \"rU\")\n",
    "wmt19_ref_1 = wmt19_ref.read()\n",
    "wmt19_ref.close()\n",
    "wmt19_can = open(\"wmt19_can.txt\", \"rU\")\n",
    "wmt19_can_1 = wmt19_can.read()\n",
    "wmt19_can.close()\n",
    "\n",
    "tokenized_corpus_src = nltk.word_tokenize(wmt19_src_1.lower())\n",
    "tokenized_corpus_ref = nltk.word_tokenize(wmt19_ref_1.lower())\n",
    "tokenized_corpus_cand = nltk.word_tokenize(wmt19_can_1.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English source text:\n",
      "\n",
      "\n",
      "﻿Welsh AMs worried about 'looking like muppets'\n",
      "There is consternation among some AMs at a suggestion their title should change to MWPs (Member of the Welsh Parliament).\n",
      " -> ['\\ufeffwelsh', 'ams', 'worried', 'about', \"'looking\", 'like', \"muppets'\", 'there', 'is', 'consternation', 'among', 'some', 'ams', 'at', 'a', 'suggestion', 'their', 'title', 'should', 'change', 'to', 'mwps', '(', 'member', 'of', 'the', 'welsh', 'parliament', ')', '.']\n",
      "\n",
      "\n",
      "German reference translation:\n",
      "\n",
      "\n",
      "﻿Walisische Ageordnete sorgen sich \"wie Dödel auszusehen\"\n",
      "Es herrscht Bestürzung unter einigen Mitgliedern der Versammlung über einen Vorschlag, der ihren Titel zu MWPs (Mitglied der walisischen Parlament) ändern soll.\n",
      " -> ['\\ufeffwalisische', 'ageordnete', 'sorgen', 'sich', '``', 'wie', 'dödel', 'auszusehen', \"''\", 'es', 'herrscht', 'bestürzung', 'unter', 'einigen', 'mitgliedern', 'der', 'versammlung', 'über', 'einen', 'vorschlag', ',', 'der', 'ihren', 'titel', 'zu', 'mwps', '(', 'mitglied', 'der', 'walisischen', 'parlament', ')', 'ändern', 'soll', '.']\n",
      "\n",
      "\n",
      "German machine translation:\n",
      "\n",
      "\n",
      "Walisische AMs machten sich Sorgen, dass sie wie Muppets aussehen könnten\n",
      "Einige AMs sind bestürzt über den Vorschlag, ihren Titel in MWPs (Mitglied des walisischen Parlaments) zu ändern.\n",
      "Es ist aufg -> ['walisische', 'ams', 'machten', 'sich', 'sorgen', ',', 'dass', 'sie', 'wie', 'muppets', 'aussehen', 'könnten', 'einige', 'ams', 'sind', 'bestürzt', 'über', 'den', 'vorschlag', ',', 'ihren', 'titel', 'in', 'mwps', '(', 'mitglied', 'des', 'walisischen', 'parlaments']\n"
     ]
    }
   ],
   "source": [
    "print(\"English source text:\")\n",
    "print(\"\\n\")\n",
    "print(f\"{wmt19_src_1[0:170]} -> {tokenized_corpus_src[0:30]}\")\n",
    "print(\"\\n\")\n",
    "print(\"German reference translation:\")\n",
    "print(\"\\n\")\n",
    "print(f\"{wmt19_ref_1[0:219]} -> {tokenized_corpus_ref[0:35]}\")\n",
    "print(\"\\n\")\n",
    "print(\"German machine translation:\")\n",
    "print(\"\\n\")\n",
    "print(f\"{wmt19_can_1[0:199]} -> {tokenized_corpus_cand[0:29]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results reference versus candidate 1 our own BLEU implementation:  43.7\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Results reference versus candidate 1 our own BLEU implementation: \",\n",
    "    round(bleu_score(tokenized_corpus_cand, tokenized_corpus_ref) * 100, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results reference versus candidate 1 sacrebleu library BLEU:  43.2\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Results reference versus candidate 1 sacrebleu library BLEU: \",\n",
    "    round(sacrebleu.corpus_bleu(wmt19_can_1, wmt19_ref_1).score, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU Score Interpretation on a Corpus\n",
    "\n",
    "                                     Score   Interpretation\n",
    "    \n",
    "                                     < 10   Almost useless\n",
    "        \n",
    "                                    10 - 19 Hard to get the gist\n",
    "            \n",
    "                                    20 - 29 The gist is clear, but has significant grammatical errors\n",
    "                \n",
    "                                    30 - 40 Understandable to good translations\n",
    "                    \n",
    "                                    40 - 50 High quality translations\n",
    "                        \n",
    "                                    50 - 60 Very high quality, adequate, and fluent translations\n",
    "                            \n",
    "                                     > 60   Quality often better than human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as fastnp\n",
    "from trax.supervised import training\n",
    "\n",
    "!pip list | grep trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as fastnp\n",
    "from trax.supervised import training\n",
    "\n",
    "VOCAB_FILE = 'ende_32k.subword'\n",
    "VOCAB_DIR = 'data/'\n",
    "\n",
    "\n",
    "def jaccard_similarity(candidate, reference):\n",
    "    \"\"\"Returns the Jaccard similarity between two token lists\n",
    "\n",
    "    Args:\n",
    "        candidate (list of int): tokenized version of the candidate translation\n",
    "        reference (list of int): tokenized version of the reference translation\n",
    "\n",
    "    Returns:\n",
    "        float: overlap between the two token lists\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert the lists to a set to get the unique tokens\n",
    "    can_unigram_set, ref_unigram_set = set(candidate), set(reference)  \n",
    "    \n",
    "    # get the set of tokens common to both candidate and reference\n",
    "    joint_elems = can_unigram_set.intersection(ref_unigram_set)\n",
    "    \n",
    "    # get the set of all tokens found in either candidate or reference\n",
    "    all_elems = can_unigram_set.union(ref_unigram_set)\n",
    "    \n",
    "    # divide the number of joint elements by the number of all elements\n",
    "    overlap = len(joint_elems) / len(all_elems)\n",
    "    \n",
    "    return overlap\n",
    "\n",
    "\n",
    "def weighted_avg_overlap(similarity_fn, samples, log_probs):\n",
    "    \"\"\"Returns the weighted mean of each candidate sentence in the samples\n",
    "\n",
    "    Args:\n",
    "        samples (list of lists): tokenized version of the translated sentences\n",
    "        log_probs (list of float): log probability of the translated sentences\n",
    "\n",
    "    Returns:\n",
    "        dict: scores of each sample\n",
    "            key: index of the sample\n",
    "            value: score of the sample\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize dictionary\n",
    "    scores = {}\n",
    "    \n",
    "    # run a for loop for each sample\n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        \n",
    "        # initialize overlap and weighted sum\n",
    "        overlap, weight_sum = 0.0, 0.0\n",
    "        \n",
    "        # run a for loop for each sample\n",
    "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
    "\n",
    "            # skip if the candidate index is the same as the sample index            \n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            # convert log probability to linear scale\n",
    "            sample_p = float(np.exp(logp))\n",
    "\n",
    "            # update the weighted sum\n",
    "            weight_sum += sample_p\n",
    "\n",
    "            # get the unigram overlap between candidate and sample\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            \n",
    "            # update the overlap\n",
    "            overlap += sample_p * sample_overlap\n",
    "            \n",
    "        # get the score for the candidate\n",
    "        score = overlap / weight_sum\n",
    "        \n",
    "        # save the score in the dictionary. use index as the key.\n",
    "        scores[index_candidate] = score\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "# UNIT TEST for UNQ_C1\n",
    "def test_input_encoder_fn(input_encoder_fn):\n",
    "    target = input_encoder_fn\n",
    "    success = 0\n",
    "    fails = 0\n",
    "    \n",
    "    input_vocab_size = 10\n",
    "    d_model = 2\n",
    "    n_encoder_layers = 6\n",
    "    \n",
    "    encoder = target(input_vocab_size, d_model, n_encoder_layers)\n",
    "    \n",
    "    lstms = \"\\n\".join([f'  LSTM_{d_model}'] * n_encoder_layers)\n",
    "\n",
    "    expected = f\"Serial[\\n  Embedding_{input_vocab_size}_{d_model}\\n{lstms}\\n]\"\n",
    "\n",
    "    proposed = str(encoder)\n",
    "    \n",
    "    # Test all layers are in the expected sequence\n",
    "    try:\n",
    "        assert(proposed.replace(\" \", \"\") == expected.replace(\" \", \"\"))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong model. \\nProposed:\\n%s\" %proposed, \"\\nExpected:\\n%s\" %expected)\n",
    "    \n",
    "    # Test the output type\n",
    "    try:\n",
    "        assert(isinstance(encoder, trax.layers.combinators.Serial))\n",
    "        success += 1\n",
    "        # Test the number of layers\n",
    "        try:\n",
    "            # Test \n",
    "            assert len(encoder.sublayers) == (n_encoder_layers + 1)\n",
    "            success += 1\n",
    "        except:\n",
    "            fails += 1\n",
    "            print('The number of sublayers does not match %s <>' %len(encoder.sublayers), \" %s\" %(n_encoder_layers + 1))\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"The enconder is not an object of \", trax.layers.combinators.Serial)\n",
    "    \n",
    "        \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "\n",
    "        \n",
    "# UNIT TEST for UNQ_C2\n",
    "def test_pre_attention_decoder_fn(pre_attention_decoder_fn):\n",
    "    target = pre_attention_decoder_fn\n",
    "    success = 0\n",
    "    fails = 0\n",
    "    \n",
    "    mode = 'train'\n",
    "    target_vocab_size = 10\n",
    "    d_model = 2\n",
    "    \n",
    "    decoder = target(mode, target_vocab_size, d_model)\n",
    "    \n",
    "    expected = f\"Serial[\\n  ShiftRight(1)\\n  Embedding_{target_vocab_size}_{d_model}\\n  LSTM_{d_model}\\n]\"\n",
    "\n",
    "    proposed = str(decoder)\n",
    "    \n",
    "    # Test all layers are in the expected sequence\n",
    "    try:\n",
    "        assert(proposed.replace(\" \", \"\") == expected.replace(\" \", \"\"))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong model. \\nProposed:\\n%s\" %proposed, \"\\nExpected:\\n%s\" %expected)\n",
    "    \n",
    "    # Test the output type\n",
    "    try:\n",
    "        assert(isinstance(decoder, trax.layers.combinators.Serial))\n",
    "        success += 1\n",
    "        # Test the number of layers\n",
    "        try:\n",
    "            # Test \n",
    "            assert len(decoder.sublayers) == 3\n",
    "            success += 1\n",
    "        except:\n",
    "            fails += 1\n",
    "            print('The number of sublayers does not match %s <>' %len(decoder.sublayers), \" %s\" %3)\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"The enconder is not an object of \", trax.layers.combinators.Serial)\n",
    "    \n",
    "        \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "\n",
    "        \n",
    "# UNIT TEST for UNQ_C3\n",
    "def test_prepare_attention_input(prepare_attention_input):\n",
    "    target = prepare_attention_input\n",
    "    success = 0\n",
    "    fails = 0\n",
    "    \n",
    "    #This unit test consider a batch size = 2, number_of_tokens = 3 and embedding_size = 4\n",
    "    \n",
    "    enc_act = fastnp.array([[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]],\n",
    "               [[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 0, 0]]])\n",
    "    dec_act = fastnp.array([[[2, 0, 0, 0], [0, 2, 0, 0], [0, 0, 2, 0]], \n",
    "               [[2, 0, 2, 0], [0, 2, 0, 2], [0, 0, 0, 0]]])\n",
    "    inputs =  fastnp.array([[1, 2, 3], [1, 4, 0]])\n",
    "    \n",
    "    exp_mask = fastnp.array([[[[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]], \n",
    "                             [[[1., 1., 0.], [1., 1., 0.], [1., 1., 0.]]]])\n",
    "    \n",
    "    exp_type = type(enc_act)\n",
    "    \n",
    "    queries, keys, values, mask = target(enc_act, dec_act, inputs)\n",
    "    \n",
    "    try:\n",
    "        assert(fastnp.allclose(queries, dec_act))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Queries does not match the decoder activations\")\n",
    "    try:\n",
    "        assert(fastnp.allclose(keys, enc_act))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Keys does not match the encoder activations\")\n",
    "    try:\n",
    "        assert(fastnp.allclose(values, enc_act))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Values does not match the encoder activations\")\n",
    "    try:\n",
    "        assert(fastnp.allclose(mask, exp_mask))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Mask does not match expected tensor. \\nExpected:\\n%s\" %exp_mask, \"\\nOutput:\\n%s\" %mask)\n",
    "    \n",
    "    # Test the output type\n",
    "    try:\n",
    "        assert(isinstance(queries, exp_type))\n",
    "        assert(isinstance(keys, exp_type))\n",
    "        assert(isinstance(values, exp_type))\n",
    "        assert(isinstance(mask, exp_type))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"One of the output object are not of type \", jax.interpreters.xla.DeviceArray)\n",
    "        \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "\n",
    "        \n",
    "# UNIT TEST for UNQ_C4\n",
    "def test_NMTAttn(NMTAttn):\n",
    "    test_cases = [\n",
    "                {\n",
    "                    \"name\":\"simple_test_check\",\n",
    "                    \"expected\":\"Serial_in2_out2[\\n  Select[0,1,0,1]_in2_out4\\n  Parallel_in2_out2[\\n    Serial[\\n      Embedding_33300_1024\\n      LSTM_1024\\n      LSTM_1024\\n    ]\\n    Serial[\\n      ShiftRight(1)\\n      Embedding_33300_1024\\n      LSTM_1024\\n    ]\\n  ]\\n  PrepareAttentionInput_in3_out4\\n  Serial_in4_out2[\\n    Branch_in4_out3[\\n      None\\n      Serial_in4_out2[\\n        Parallel_in3_out3[\\n          Dense_1024\\n          Dense_1024\\n          Dense_1024\\n        ]\\n        PureAttention_in4_out2\\n        Dense_1024\\n      ]\\n    ]\\n    Add_in2\\n  ]\\n  Select[0,2]_in3_out2\\n  LSTM_1024\\n  LSTM_1024\\n  Dense_33300\\n  LogSoftmax\\n]\",\n",
    "                    \"error\":\"The NMTAttn is not defined properly.\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\":\"layer_len_check\",\n",
    "                    \"expected\":9,\n",
    "                    \"error\":\"We found {} layers in your model. It should be 9.\\nCheck the LSTM stack before the dense layer\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\":\"selection_layer_check\",\n",
    "                    \"expected\":[\"Select[0,1,0,1]_in2_out4\", \"Select[0,2]_in3_out2\"],\n",
    "                    \"error\":\"Look at your selection layers.\"\n",
    "                }\n",
    "            ]\n",
    "    \n",
    "    success = 0\n",
    "    fails = 0\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        try:\n",
    "            if test_case['name'] == \"simple_test_check\":\n",
    "                assert test_case[\"expected\"] == str(NMTAttn())\n",
    "                success += 1\n",
    "            if test_case['name'] == \"layer_len_check\":\n",
    "                if test_case[\"expected\"] == len(NMTAttn().sublayers):\n",
    "                    success += 1\n",
    "                else:\n",
    "                    print(test_case[\"error\"].format(len(NMTAttn().sublayers))) \n",
    "                    fails += 1\n",
    "            if test_case['name'] == \"selection_layer_check\":\n",
    "                model = NMTAttn()\n",
    "                output = [str(model.sublayers[0]),str(model.sublayers[4])]\n",
    "                check_count = 0\n",
    "                for i in range(2):\n",
    "                    if test_case[\"expected\"][i] != output[i]:\n",
    "                        print(test_case[\"error\"])\n",
    "                        fails += 1\n",
    "                        break\n",
    "                    else:\n",
    "                        check_count += 1\n",
    "                if check_count == 2:\n",
    "                    success += 1\n",
    "        except:\n",
    "            print(test_case['error'])\n",
    "            fails += 1\n",
    "            \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "\n",
    "\n",
    "# UNIT TEST for UNQ_C5\n",
    "def test_train_task(train_task):\n",
    "    target = train_task\n",
    "    success = 0\n",
    "    fails = 0\n",
    "     \n",
    "    # Test the labeled data parameter\n",
    "    try:\n",
    "        strlabel = str(target._labeled_data)\n",
    "        assert(strlabel.find(\"generator\") and strlabel.find('add_loss_weights'))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong labeled data parameter\")\n",
    "    \n",
    "    # Test the cross entropy loss data parameter\n",
    "    try:\n",
    "        strlabel = str(target._loss_layer)\n",
    "        assert(strlabel == \"CrossEntropyLoss_in3\")\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong loss functions. CrossEntropyLoss_in3 was expected\")\n",
    "        \n",
    "     # Test the optimizer parameter\n",
    "    try:\n",
    "        assert(isinstance(target.optimizer, trax.optimizers.adam.Adam))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong optimizer\")\n",
    "        \n",
    "    # Test the schedule parameter\n",
    "    try:\n",
    "        assert(isinstance(target._lr_schedule,trax.supervised.lr_schedules._BodyAndTail))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong learning rate schedule type\")\n",
    "    \n",
    "    # Test the _n_steps_per_checkpoint parameter\n",
    "    try:\n",
    "        assert(target._n_steps_per_checkpoint==10)\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Wrong checkpoint step frequency\")\n",
    "        \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "\n",
    "\n",
    "\n",
    "# UNIT TEST for UNQ_C6\n",
    "def test_next_symbol(next_symbol, model):\n",
    "    target = next_symbol\n",
    "    the_model = model\n",
    "    success = 0\n",
    "    fails = 0\n",
    "        \n",
    "    tokens_en = np.array([[17332, 140, 172, 207, 1]])\n",
    "     \n",
    "    # Test the type and size of output\n",
    "    try:\n",
    "        next_de_tokens = target(the_model, tokens_en, [], 0.0) \n",
    "        assert(isinstance(next_de_tokens, tuple))\n",
    "        assert(len(next_de_tokens) == 2)\n",
    "        assert(type(next_de_tokens[0]) == int and type(next_de_tokens[1]) == float)\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Output must be a tuple of size 2 containing a integer and a float number\")\n",
    "    \n",
    "    # Test an output\n",
    "    try:\n",
    "        next_de_tokens = target(the_model, tokens_en, [18477], 0.0)\n",
    "        assert(np.allclose([next_de_tokens[0], next_de_tokens[1]], [140, -0.000217437744]))\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Expected output: \", [140, -0.000217437744])\n",
    "    \n",
    "        \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "\n",
    "\n",
    "# UNIT TEST for UNQ_C7\n",
    "def test_sampling_decode(sampling_decode, model):\n",
    "    target = sampling_decode\n",
    "    the_model = model\n",
    "    success = 0\n",
    "    fails = 0\n",
    "    \n",
    "    try:\n",
    "        output = target(\"I eat soup.\", model, temperature=0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
    "        expected = ([161, 15103, 5, 25132, 35, 3, 1], -0.0003108978271484375, 'Ich iss Suppe.')\n",
    "        assert(output[2] == expected[2])\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Test 1 fails\")\n",
    "        \n",
    "    try:\n",
    "        output = target(\"I like your shoes.\", model, temperature=0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
    "        expected = ([161, 15103, 5, 25132, 35, 3, 1], -0.0003108978271484375, 'Ich mag Ihre Schuhe.')\n",
    "        assert(output[2] == expected[2])\n",
    "        success += 1\n",
    "    except:\n",
    "        fails += 1\n",
    "        print(\"Test 2 fails\")\n",
    "            \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "        \n",
    "\n",
    "# UNIT TEST for UNQ_C8\n",
    "def test_rouge1_similarity(rouge1_similarity):\n",
    "    target = rouge1_similarity\n",
    "    success = 0\n",
    "    fails = 0\n",
    "    n_samples = 10\n",
    "    \n",
    "    test_cases = [\n",
    "        \n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\": [[1, 2, 3], [1, 2, 3, 4]],\n",
    "            \"expected\":0.8571428571428571,\n",
    "            \"error\":\"Expected similarity: 0.8571428571428571\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\":[[2, 1], [3, 1]],\n",
    "            \"expected\":0.5,\n",
    "            \"error\":\"Expected similarity: 0.5\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\":[[2], [3]],\n",
    "            \"expected\":0,\n",
    "            \"error\":\"Expected similarity: 0\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\":[[0] * 100 + [2] * 100, [0] * 100 + [1] * 100],\n",
    "            \"expected\":0.5,\n",
    "            \"error\":\"Expected similarity: 0.5\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for test_case in test_cases:\n",
    "        \n",
    "        try:\n",
    "            if test_case['name'] == \"simple_test_check\":\n",
    "                assert abs(test_case[\"expected\"] -target(*test_case['input'])) < 1e-6\n",
    "                success += 1\n",
    "        except:\n",
    "            print(test_case['error'])\n",
    "            fails += 1\n",
    "            \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "        \n",
    "\n",
    "# UNIT TEST for UNQ_C9\n",
    "def test_average_overlap(average_overlap):\n",
    "    target = average_overlap\n",
    "    success = 0\n",
    "    fails = 0\n",
    "    \n",
    "    test_cases = [\n",
    "        \n",
    "        {\n",
    "            \"name\":\"dict_test_check\",\n",
    "            \"input\": [jaccard_similarity, [[1, 2], [3, 4], [1, 2], [3, 5]]],\n",
    "            \"expected\":{0: 0.3333333333333333,\n",
    "                        1: 0.1111111111111111,\n",
    "                        2: 0.3333333333333333,\n",
    "                        3: 0.1111111111111111},\n",
    "            \"error\":\"Expected output does not match\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"dict_test_check\",\n",
    "            \"input\":[jaccard_similarity, [[1, 2], [3, 4], [1, 2, 5], [3, 5], [3, 4, 1]]],\n",
    "            \"expected\":{0: 0.22916666666666666,\n",
    "                        1: 0.25,\n",
    "                        2: 0.2791666666666667,\n",
    "                        3: 0.20833333333333331,\n",
    "                        4: 0.3416666666666667},\n",
    "            \"error\":\"Expected output does not match\"\n",
    "        }\n",
    "    ]\n",
    "    for test_case in test_cases:\n",
    "        try:\n",
    "            if test_case['name'] == \"dict_test_check\":\n",
    "                output = target(*test_case['input'])\n",
    "                for x in output:\n",
    "                    assert (abs(output[x] - test_case['expected'][x]) < 1e-5)\n",
    "                    success += 1\n",
    "        except:\n",
    "            print(test_case['error'])\n",
    "            fails += 1\n",
    "            \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")\n",
    "\n",
    "\n",
    "# UNIT TEST for UNQ_C10\n",
    "def test_mbr_decode(mbr_decode, model):\n",
    "    target = mbr_decode\n",
    "    success = 0\n",
    "    fails = 0\n",
    "    \n",
    "    TEMPERATURE = 0.0\n",
    "    \n",
    "    test_cases = [\n",
    "        \n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\": \"I am hungry\",\n",
    "            \"expected\":\"Ich bin hungrig.\",\n",
    "            \"error\":\"Expected output does not match\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\":'Congratulations!',\n",
    "            \"expected\":'Herzlichen Glückwunsch!',\n",
    "            \"error\":\"Expected output does not match\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\":'You have completed the assignment!',\n",
    "            \"expected\":'Sie haben die Abtretung abgeschlossen!',\n",
    "            \"error\":\"Expected output does not match\"\n",
    "        }\n",
    "    ]\n",
    "    for test_case in test_cases:\n",
    "        try:\n",
    "            result = target(test_case['input'], 4, weighted_avg_overlap, jaccard_similarity, \n",
    "                                model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)   \n",
    "\n",
    "            output = result[0]\n",
    "            if test_case['name'] == \"simple_test_check\":\n",
    "                assert(output == test_case['expected'])\n",
    "                success += 1\n",
    "        except:\n",
    "            print(test_case['error'])\n",
    "            fails += 1\n",
    "            \n",
    "    # Test that function return the most likely translation\n",
    "    TEMPERATURE = 0.5\n",
    "    test_case =  test_cases[0]\n",
    "    try:\n",
    "        result = target(test_case['input'], 4, weighted_avg_overlap, jaccard_similarity, \n",
    "                                model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)   \n",
    "\n",
    "        assert  max(result[2], key=result[2].get) == result[1]\n",
    "        success += 1\n",
    "    except:\n",
    "        print('Use max function to select max_index')\n",
    "        fails += 1\n",
    "    \n",
    "    if fails == 0:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print('\\033[92m', success,\" Tests passed\")\n",
    "        print('\\033[91m', fails, \" Tests failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import the dataset we will use to train the model. To meet the storage constraints in this lab environment, we will just use a small dataset from Opus(http://opus.nlpl.eu/), a growing collection of translated texts from the web. Particularly, we will get an English to German translation subset specified as opus/medical which has medical related texts. If storage is not an issue, you can opt to get a larger corpus such as the English to German translation dataset from ParaCrawl(https://paracrawl.eu/), a large multi-lingual translation dataset created by the European Union. Both of these datasets are available via Tensorflow Datasets (TFDS) and you can browse through the other available datasets here(https://www.tensorflow.org/datasets/catalog/overview). We have downloaded the data for you in the data/ directory of your workspace. As you'll see below, you can easily access this dataset from TFDS with trax.data.TFDS. The result is a python generator function yielding tuples. Use the keys argument to select what appears at which position in the tuple. For example, keys=('en', 'de') below will return pairs as (English sentence, German sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get generator function for the training set\n",
    "# This will download the train dataset if no data_dir is specified.\n",
    "train_stream_fn = trax.data.TFDS('opus/medical',\n",
    "                                 data_dir='./data/',\n",
    "                                 keys=('en', 'de'),\n",
    "                                 eval_holdout_size=0.01, # 1% for eval\n",
    "                                 train=True)\n",
    "\n",
    "# Get generator function for the eval set\n",
    "eval_stream_fn = trax.data.TFDS('opus/medical',\n",
    "                                data_dir='./data/',\n",
    "                                keys=('en', 'de'),\n",
    "                                eval_holdout_size=0.01, # 1% for eval\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stream = train_stream_fn()\n",
    "print(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n",
    "print()\n",
    "\n",
    "eval_stream = eval_stream_fn()\n",
    "print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Tokenization and Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables that state the filename and directory of the vocabulary file\n",
    "VOCAB_FILE = 'ende_32k.subword'\n",
    "VOCAB_DIR = 'data/'\n",
    "\n",
    "# Tokenize the dataset.\n",
    "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
    "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append EOS at the end of each sentence.\n",
    "\n",
    "# Integer assigned as end-of-sentence (EOS)\n",
    "EOS = 1\n",
    "\n",
    "# generator helper function to append EOS to each sentence\n",
    "def append_eos(stream):\n",
    "    for (inputs, targets) in stream:\n",
    "        inputs_with_eos = list(inputs) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
    "\n",
    "# append EOS to the train data\n",
    "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
    "\n",
    "# append EOS to the eval data\n",
    "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter too long sentences to not run out of memory.\n",
    "# length_keys=[0, 1] means we filter both English and German sentences, so\n",
    "# both much be not longer that 256 tokens for training / 512 for eval.\n",
    "filtered_train_stream = trax.data.FilterByLength(\n",
    "    max_length=256, length_keys=[0, 1])(tokenized_train_stream)\n",
    "filtered_eval_stream = trax.data.FilterByLength(\n",
    "    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)\n",
    "\n",
    "# print a sample input-target pair of tokenized sentences\n",
    "train_input, train_target = next(filtered_train_stream)\n",
    "print(colored(f'Single tokenized example input:', 'red' ), train_input)\n",
    "print(colored(f'Single tokenized example target:', 'red'), train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 tokenize & detokenize helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup helper functions for tokenizing and detokenizing sentences\n",
    "\n",
    "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Encodes a string to an array of integers\n",
    "\n",
    "    Args:\n",
    "        input_str (str): human-readable string to encode\n",
    "        vocab_file (str): filename of the vocabulary text file\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "  \n",
    "    Returns:\n",
    "        numpy.ndarray: tokenized version of the input string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the encoding of the \"end of sentence\" as 1\n",
    "    EOS = 1\n",
    "    \n",
    "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
    "    # we get around it by making a 1-element stream with `iter`.\n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
    "    \n",
    "    # Mark the end of the sentence with EOS\n",
    "    inputs = list(inputs) + [EOS]\n",
    "    \n",
    "    # Adding the batch dimension to the front of the shape\n",
    "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
    "    \n",
    "    return batch_inputs\n",
    "\n",
    "\n",
    "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Decodes an array of integers to a human readable string\n",
    "\n",
    "    Args:\n",
    "        integers (numpy.ndarray): array of integers to decode\n",
    "        vocab_file (str): filename of the vocabulary text file\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "  \n",
    "    Returns:\n",
    "        str: the decoded sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove the dimensions of size 1\n",
    "    integers = list(np.squeeze(integers))\n",
    "    \n",
    "    # Set the encoding of the \"end of sentence\" as 1\n",
    "    EOS = 1\n",
    "    \n",
    "    # Remove the EOS to decode only the original tokens\n",
    "    if EOS in integers:\n",
    "        integers = integers[:integers.index(EOS)] \n",
    "    \n",
    "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As declared earlier:\n",
    "# VOCAB_FILE = 'ende_32k.subword'\n",
    "# VOCAB_DIR = 'data/'\n",
    "\n",
    "# Detokenize an input-target pair of tokenized sentences\n",
    "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print()\n",
    "\n",
    "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n",
    "# See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.\n",
    "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketing to create streams of batches.\n",
    "\n",
    "# Buckets are defined in terms of boundaries and batch sizes.\n",
    "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
    "# So below, we'll take a batch of 256 sentences of length < 8, 128 if length is\n",
    "# between 8 and 16, and so on -- and only 2 if length is over 512.\n",
    "boundaries =  [8,   16,  32, 64, 128, 256, 512]\n",
    "batch_sizes = [256, 128, 64, 32, 16,    8,   4,  2]\n",
    "\n",
    "# Create the generators.\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes,\n",
    "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
    ")(filtered_train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes,\n",
    "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
    ")(filtered_eval_stream)\n",
    "\n",
    "# Add masking for the padding (0s).\n",
    "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
    "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
    "\n",
    "# let's see the data type of a batch\n",
    "print(\"input_batch data type: \", type(input_batch))\n",
    "print(\"target_batch data type: \", type(target_batch))\n",
    "\n",
    "# let's see the shape of this particular batch (batch length, sentence length)\n",
    "print(\"input_batch shape: \", input_batch.shape)\n",
    "print(\"target_batch shape: \", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random index less than the batch size.\n",
    "index = random.randrange(len(input_batch))\n",
    "\n",
    "# use the index to grab an entry from the input and target batch\n",
    "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
    "print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION\n",
    "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n",
    "    \"\"\" Input encoder runs on the input sentence and creates\n",
    "    activations that will be the keys and values for attention.\n",
    "    \n",
    "    Args:\n",
    "        input_vocab_size: int: vocab size of the input\n",
    "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
    "        n_encoder_layers: int: number of LSTM layers in the encoder\n",
    "    Returns:\n",
    "        tl.Serial: The input encoder\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a serial network\n",
    "    input_encoder = tl.Serial( \n",
    "        \n",
    "        ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
    "        # create an embedding layer to convert tokens to vectors\n",
    "        tl.Embedding(input_vocab_size, d_model),\n",
    "        \n",
    "        # feed the embeddings to the LSTM layers. It is a stack of n_encoder_layers LSTM layers\n",
    "        [tl.LSTM(d_model) for _ in range(n_encoder_layers)],\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    )\n",
    "\n",
    "    return input_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "import w1_unittest\n",
    "\n",
    "w1_unittest.test_input_encoder_fn(input_encoder_fn)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION\n",
    "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
    "    \"\"\" Pre-attention decoder runs on the targets and creates\n",
    "    activations that are used as queries in attention.\n",
    "    \n",
    "    Args:\n",
    "        mode: str: 'train' or 'eval'\n",
    "        target_vocab_size: int: vocab size of the target\n",
    "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
    "    Returns:\n",
    "        tl.Serial: The pre-attention decoder\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a serial network\n",
    "    pre_attention_decoder = tl.Serial(\n",
    "        \n",
    "        ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
    "        # shift right to insert start-of-sentence token and implement\n",
    "        # teacher forcing during training\n",
    "        tl.ShiftRight(mode=mode),\n",
    "\n",
    "        # run an embedding layer to convert tokens to vectors\n",
    "        tl.Embedding(target_vocab_size, d_model),\n",
    "\n",
    "        # feed to an LSTM layer\n",
    "        tl.LSTM(d_model)\n",
    "        ### END CODE HERE ###\n",
    "    )\n",
    "    \n",
    "    return pre_attention_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "\n",
    "w1_unittest.test_pre_attention_decoder_fn(pre_attention_decoder_fn)\n",
    "\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION\n",
    "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
    "    \"\"\"Prepare queries, keys, values and mask for attention.\n",
    "    \n",
    "    Args:\n",
    "        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n",
    "        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n",
    "        inputs fastnp.array(batch_size, padded_input_length): padded input tokens\n",
    "    \n",
    "    Returns:\n",
    "        queries, keys, values and mask for attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
    "    \n",
    "    # set the keys and values to the encoder activations\n",
    "    keys = encoder_activations\n",
    "    values = encoder_activations\n",
    "\n",
    "    \n",
    "    # set the queries to the decoder activations\n",
    "    queries = decoder_activations\n",
    "    \n",
    "    # generate the mask to distinguish real tokens from padding\n",
    "    # hint: inputs is 1 for real tokens and 0 where they are padding\n",
    "    mask = (inputs != 0)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # add axes to the mask for attention heads and decoder length.\n",
    "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
    "    \n",
    "    # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].\n",
    "    # note: for this assignment, attention heads is set to 1.\n",
    "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
    "        \n",
    "    \n",
    "    return queries, keys, values, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_prepare_attention_input(prepare_attention_input)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Implementation Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AttentionQKV(d_feature, n_heads=1, dropout=0.0, mode='train'):\n",
    "  \"\"\"Returns a layer that maps (q, k, v, mask) to (activations, mask).\n",
    "\n",
    "  See `Attention` above for further context/details.\n",
    "\n",
    "  Args:\n",
    "    d_feature: Depth/dimensionality of feature embedding.\n",
    "    n_heads: Number of attention heads.\n",
    "    dropout: Probababilistic rate for internal dropout applied to attention\n",
    "        activations (based on query-key pairs) before dotting them with values.\n",
    "    mode: Either 'train' or 'eval'.\n",
    "  \"\"\"\n",
    "  return cb.Serial(\n",
    "      cb.Parallel(\n",
    "          core.Dense(d_feature),\n",
    "          core.Dense(d_feature),\n",
    "          core.Dense(d_feature),\n",
    "      ),\n",
    "      PureAttention(  # pylint: disable=no-value-for-parameter\n",
    "          n_heads=n_heads, dropout=dropout, mode=mode),\n",
    "      core.Dense(d_feature),\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION\n",
    "def NMTAttn(input_vocab_size=33300,\n",
    "            target_vocab_size=33300,\n",
    "            d_model=1024,\n",
    "            n_encoder_layers=2,\n",
    "            n_decoder_layers=2,\n",
    "            n_attention_heads=4,\n",
    "            attention_dropout=0.0,\n",
    "            mode='train'):\n",
    "    \"\"\"Returns an LSTM sequence-to-sequence model with attention.\n",
    "\n",
    "    The input to the model is a pair (input tokens, target tokens), e.g.,\n",
    "    an English sentence (tokenized) and its translation into German (tokenized).\n",
    "\n",
    "    Args:\n",
    "    input_vocab_size: int: vocab size of the input\n",
    "    target_vocab_size: int: vocab size of the target\n",
    "    d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
    "    n_encoder_layers: int: number of LSTM layers in the encoder\n",
    "    n_decoder_layers: int: number of LSTM layers in the decoder after attention\n",
    "    n_attention_heads: int: number of attention heads\n",
    "    attention_dropout: float, dropout for the attention layer\n",
    "    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\n",
    "\n",
    "    Returns:\n",
    "    A LSTM sequence-to-sequence model with attention.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
    "    \n",
    "    # Step 0: call the helper function to create layers for the input encoder\n",
    "    input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)\n",
    "\n",
    "    # Step 0: call the helper function to create layers for the pre-attention decoder\n",
    "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n",
    "\n",
    "    # Step 1: create a serial network\n",
    "    model = tl.Serial( \n",
    "        \n",
    "      # Step 2: copy input tokens and target tokens as they will be needed later.\n",
    "      tl.Select([0, 1, 0, 1]),\n",
    "        \n",
    "      # Step 3: run input encoder on the input and pre-attention decoder the target.\n",
    "      tl.Parallel(input_encoder, pre_attention_decoder),\n",
    "        \n",
    "      # Step 4: prepare queries, keys, values and mask for attention.\n",
    "      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
    "        \n",
    "      # Step 5: run the AttentionQKV layer\n",
    "      # nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)\n",
    "      tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
    "      \n",
    "      # Step 6: drop attention mask (i.e. index = None\n",
    "      tl.Select([0, 2]),\n",
    "        \n",
    "      # Step 7: run the rest of the RNN decoder\n",
    "      [tl.LSTM(d_model) for _ in range(n_decoder_layers)],\n",
    "        \n",
    "      # Step 8: prepare output by making it the right size\n",
    "      tl.Dense(target_vocab_size),\n",
    "        \n",
    "      # Step 9: Log-softmax for output\n",
    "      tl.LogSoftmax()\n",
    "    )\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_NMTAttn(NMTAttn)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print your model\n",
    "model = NMTAttn()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED \n",
    "train_task = training.TrainTask(\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
    "    \n",
    "    # use the train batch stream as labeled data\n",
    "    labeled_data= train_batch_stream,\n",
    "    \n",
    "    # use the cross entropy loss\n",
    "    loss_layer= tl.CrossEntropyLoss(),\n",
    "    \n",
    "    # use the Adam optimizer with learning rate of 0.01\n",
    "    optimizer= trax.optimizers.Adam(.01),\n",
    "    \n",
    "    # use the `trax.lr.warmup_and_rsqrt_decay` as the learning rate schedule\n",
    "    # have 1000 warmup steps with a max value of 0.01\n",
    "    lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000, .01),\n",
    "    \n",
    "    # have a checkpoint every 10 steps\n",
    "    n_steps_per_checkpoint= 10,\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_train_task(train_task)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 EvalTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_task = training.EvalTask(\n",
    "    \n",
    "    ## use the eval batch stream as labeled data\n",
    "    labeled_data=eval_batch_stream,\n",
    "    \n",
    "    ## use the cross entropy loss and accuracy as metrics\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the output directory\n",
    "output_dir = 'output_dir/'\n",
    "\n",
    "# remove old model if it exists. restarts training.\n",
    "!rm -f ~/output_dir/model.pkl.gz  \n",
    "\n",
    "# define the training loop\n",
    "training_loop = training.Loop(NMTAttn(mode='train'),\n",
    "                              train_task,\n",
    "                              eval_tasks=[eval_task],\n",
    "                              output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Execute the training loop. This will take around 8 minutes to complete.\n",
    "training_loop.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model we built in eval mode\n",
    "model = NMTAttn(mode='eval')\n",
    "\n",
    "# initialize weights from a pre-trained model\n",
    "model.init_from_file(\"model.pkl.gz\", weights_only=True)\n",
    "model = tl.Accelerate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsoftmax_sample(log_probs, temperature=1.0):  # pylint: disable=invalid-name\n",
    "    \"\"\"Returns a sample from a log-softmax output, with temperature.\n",
    "\n",
    "    Args:\n",
    "    log_probs: Logarithms of probabilities (often coming from LogSofmax)\n",
    "    temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)\n",
    "    \"\"\"\n",
    "    # This is equivalent to sampling from a softmax with temperature.\n",
    "    u = np.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
    "    g = -np.log(-np.log(u))\n",
    "    return np.argmax(log_probs + g * temperature, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6\n",
    "# GRADED FUNCTION\n",
    "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
    "    \"\"\"Returns the index of the next token.\n",
    "\n",
    "    Args:\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence\n",
    "        cur_output_tokens (list): tokenized representation of previously translated words\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "\n",
    "    Returns:\n",
    "        int: index of the next token in the translated sentence\n",
    "        float: log probability of the next symbol\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
    "\n",
    "    # set the length of the current output tokens\n",
    "    token_length = len(cur_output_tokens)\n",
    "\n",
    "    # calculate next power of 2 for padding length \n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length + 1))) \n",
    "\n",
    "    # pad cur_output_tokens up to the padded_length\n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    \n",
    "    # model expects the output to have an axis for the batch size in front so\n",
    "    # convert `padded` list to a numpy array with shape (None, <padded_length>) where\n",
    "    # None is a placeholder for the batch size\n",
    "    padded_with_batch = np.expand_dims(padded, axis=0)\n",
    "    \n",
    "    # get the model prediction (remember to use the `NMAttn` argument defined above)\n",
    "    output, _ = NMTAttn((input_tokens, padded_with_batch))\n",
    "\n",
    "    # get log probabilities from the last token output\n",
    "    log_probs = output[0, token_length, :]\n",
    "    \n",
    "    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)\n",
    "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return symbol, float(log_probs[symbol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_next_symbol(next_symbol, model)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C7\n",
    "# GRADED FUNCTION\n",
    "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Returns the translated sentence.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (str): sentence to translate.\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling fr0om the distribution (can sometimes say random things)\n",
    "        vocab_file (str): filename of the vocabulary\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list, str, float)\n",
    "            list of int: tokenized version of the translated sentence\n",
    "            float: log probability of the translated sentence\n",
    "            str: the translated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
    "    \n",
    "    # encode the input sentence\n",
    "    input_tokens = tokenize(input_sentence,vocab_file,vocab_dir)\n",
    "    \n",
    "    # initialize the list of output tokens\n",
    "    cur_output_tokens = []\n",
    "    \n",
    "    # initialize an integer that represents the current output index\n",
    "    cur_output = 0\n",
    "    \n",
    "    # Set the encoding of the \"end of sentence\" as 1\n",
    "    EOS = 1\n",
    "    \n",
    "    # check that the current output is not the end of sentence token\n",
    "    while cur_output != EOS:\n",
    "        \n",
    "        # update the current output token by getting the index of the next word (hint: use next_symbol)\n",
    "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n",
    "        \n",
    "        # append the current output token to the list of output tokens\n",
    "        cur_output_tokens.append(cur_output)\n",
    "    \n",
    "    # detokenize the output tokens\n",
    "    sentence = detokenize(cur_output_tokens, vocab_file, vocab_dir)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cur_output_tokens, log_prob, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function above. Try varying the temperature setting with values from 0 to 1.\n",
    "# Run it several times with each setting and see how often the output changes.\n",
    "sampling_decode(\"I love languages.\", model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_sampling_decode(sampling_decode, model)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_test(sentence, NMTAttn=None, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Prints the input and output of our NMTAttn model using greedy decode\n",
    "\n",
    "    Args:\n",
    "        sentence (str): a custom string.\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        vocab_file (str): filename of the vocabulary\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "\n",
    "    Returns:\n",
    "        str: the translated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    _,_, translated_sentence = sampling_decode(sentence, NMTAttn, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
    "    \n",
    "    print(\"English: \", sentence)\n",
    "    print(\"German: \", translated_sentence)\n",
    "    \n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put a custom string here\n",
    "your_sentence = 'I love languages.'\n",
    "\n",
    "greedy_decode_test(your_sentence, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_decode_test('You are almost done with the assignment!', model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Minimum Bayes-Risk Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(sentence, n_samples, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Generates samples using sampling_decode()\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to translate.\n",
    "        n_samples (int): number of samples to generate\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "        vocab_file (str): filename of the vocabulary\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (list, list)\n",
    "            list of lists: token list per sample\n",
    "            list of floats: log probability per sample\n",
    "    \"\"\"\n",
    "    # define lists to contain samples and probabilities\n",
    "    samples, log_probs = [], []\n",
    "\n",
    "    # run a for loop to generate n samples\n",
    "    for _ in range(n_samples):\n",
    "        \n",
    "        # get a sample using the sampling_decode() function\n",
    "        sample, logp, _ = sampling_decode(sentence, NMTAttn, temperature, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
    "        \n",
    "        # append the token list to the samples list\n",
    "        samples.append(sample)\n",
    "        \n",
    "        # append the log probability to the log_probs list\n",
    "        log_probs.append(logp)\n",
    "                \n",
    "    return samples, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 4 samples with the default temperature (0.6)\n",
    "generate_samples('I love languages.', 4, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(candidate, reference):\n",
    "    \"\"\"Returns the Jaccard similarity between two token lists\n",
    "\n",
    "    Args:\n",
    "        candidate (list of int): tokenized version of the candidate translation\n",
    "        reference (list of int): tokenized version of the reference translation\n",
    "\n",
    "    Returns:\n",
    "        float: overlap between the two token lists\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert the lists to a set to get the unique tokens\n",
    "    can_unigram_set, ref_unigram_set = set(candidate), set(reference)  \n",
    "    \n",
    "    # get the set of tokens common to both candidate and reference\n",
    "    joint_elems = can_unigram_set.intersection(ref_unigram_set)\n",
    "    \n",
    "    # get the set of all tokens found in either candidate or reference\n",
    "    all_elems = can_unigram_set.union(ref_unigram_set)\n",
    "    \n",
    "    # divide the number of joint elements by the number of all elements\n",
    "    overlap = len(joint_elems) / len(all_elems)\n",
    "    \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try using the function. remember the result here and compare with the next function below.\n",
    "jaccard_similarity([1, 2, 3], [1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8\n",
    "# GRADED FUNCTION\n",
    "\n",
    "# for making a frequency table easily\n",
    "from collections import Counter\n",
    "\n",
    "def rouge1_similarity(system, reference):\n",
    "    \"\"\"Returns the ROUGE-1 score between two token lists\n",
    "\n",
    "    Args:\n",
    "        system (list of int): tokenized version of the system translation\n",
    "        reference (list of int): tokenized version of the reference translation\n",
    "\n",
    "    Returns:\n",
    "        float: overlap between the two token lists\n",
    "    \"\"\"    \n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
    "    \n",
    "    # make a frequency table of the system tokens (hint: use the Counter class)\n",
    "    sys_counter = Counter(system)\n",
    "\n",
    "    # make a frequency table of the reference tokens (hint: use the Counter class)\n",
    "    ref_counter = Counter(reference)\n",
    "    \n",
    "    # initialize overlap to 0\n",
    "    overlap = 0\n",
    "    \n",
    "    # run a for loop over the sys_counter object (can be treated as a dictionary)\n",
    "    for token in sys_counter:\n",
    "        \n",
    "        # lookup the value of the token in the sys_counter dictionary (hint: use the get() method)\n",
    "        token_count_sys = sys_counter.get(token,0)\n",
    "        \n",
    "        # lookup the value of the token in the ref_counter dictionary (hint: use the get() method)\n",
    "        token_count_ref = ref_counter.get(token,0)\n",
    "        \n",
    "        # update the overlap by getting the smaller number between the two token counts above\n",
    "        overlap += min(token_count_sys, token_count_ref)\n",
    "    \n",
    "    # get the precision (i.e. number of overlapping tokens / number of system tokens)\n",
    "    \n",
    "    \n",
    "    precision = overlap / sum(sys_counter.values())\n",
    "    \n",
    "    # get the recall (i.e. number of overlapping tokens / number of reference tokens)\n",
    "    recall = overlap / sum(ref_counter.values())\n",
    "    \n",
    "    if precision + recall != 0:\n",
    "        # compute the f1-score\n",
    "        rouge1_score = 2 * ((precision * recall)/(precision + recall))\n",
    "    else:\n",
    "        rouge1_score = 0 \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return rouge1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that this produces a different value from the jaccard similarity earlier\n",
    "rouge1_similarity([1, 2, 3], [1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_rouge1_similarity(rouge1_similarity)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C9\n",
    "# GRADED FUNCTION\n",
    "def average_overlap(similarity_fn, samples, *ignore_params):\n",
    "    \"\"\"Returns the arithmetic mean of each candidate sentence in the samples\n",
    "\n",
    "    Args:\n",
    "        similarity_fn (function): similarity function used to compute the overlap\n",
    "        samples (list of lists): tokenized version of the translated sentences\n",
    "        *ignore_params: additional parameters will be ignored\n",
    "\n",
    "    Returns:\n",
    "        dict: scores of each sample\n",
    "            key: index of the sample\n",
    "            value: score of the sample\n",
    "    \"\"\"  \n",
    "    \n",
    "    # initialize dictionary\n",
    "    scores = {}\n",
    "    \n",
    "    # run a for loop for each sample\n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        \n",
    "        ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
    "        \n",
    "        # initialize overlap to 0.0\n",
    "        overlap = 0.0\n",
    "        \n",
    "        # run a for loop for each sample\n",
    "        for index_sample, sample in enumerate(samples): \n",
    "\n",
    "            # skip if the candidate index is the same as the sample index\n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            # get the overlap between candidate and sample using the similarity function\n",
    "            sample_overlap = similarity_fn(candidate,sample)\n",
    "\n",
    "            # add the sample overlap to the total overlap\n",
    "            overlap += sample_overlap\n",
    "\n",
    "        # get the score for the candidate by computing the average\n",
    "        score = overlap/index_sample\n",
    "        \n",
    "        # save the score in the dictionary. use index as the key.\n",
    "        scores[index_candidate] = score\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_average_overlap(average_overlap)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_overlap(similarity_fn, samples, log_probs):\n",
    "    \"\"\"Returns the weighted mean of each candidate sentence in the samples\n",
    "\n",
    "    Args:\n",
    "        samples (list of lists): tokenized version of the translated sentences\n",
    "        log_probs (list of float): log probability of the translated sentences\n",
    "\n",
    "    Returns:\n",
    "        dict: scores of each sample\n",
    "            key: index of the sample\n",
    "            value: score of the sample\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize dictionary\n",
    "    scores = {}\n",
    "    \n",
    "    # run a for loop for each sample\n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        \n",
    "        # initialize overlap and weighted sum\n",
    "        overlap, weight_sum = 0.0, 0.0\n",
    "        \n",
    "        # run a for loop for each sample\n",
    "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
    "\n",
    "            # skip if the candidate index is the same as the sample index            \n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            # convert log probability to linear scale\n",
    "            sample_p = float(np.exp(logp))\n",
    "\n",
    "            # update the weighted sum\n",
    "            weight_sum += sample_p\n",
    "\n",
    "            # get the unigram overlap between candidate and sample\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            \n",
    "            # update the overlap\n",
    "            overlap += sample_p * sample_overlap\n",
    "            \n",
    "        # get the score for the candidate\n",
    "        score = overlap / weight_sum\n",
    "        \n",
    "        # save the score in the dictionary. use index as the key.\n",
    "        scores[index_candidate] = score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_avg_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C10\n",
    "# GRADED FUNCTION\n",
    "def mbr_decode(sentence, n_samples, score_fn, similarity_fn, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Returns the translated sentence using Minimum Bayes Risk decoding\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to translate.\n",
    "        n_samples (int): number of samples to generate\n",
    "        score_fn (function): function that generates the score for each sample\n",
    "        similarity_fn (function): function used to compute the overlap between a pair of samples\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "        vocab_file (str): filename of the vocabulary\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "\n",
    "    Returns:\n",
    "        str: the translated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
    "    # generate samples\n",
    "    samples, log_probs = generate_samples(sentence, n_samples, NMTAttn, temperature, vocab_file, vocab_dir)\n",
    "    \n",
    "    # use the scoring function to get a dictionary of scores\n",
    "    # pass in the relevant parameters as shown in the function definition of \n",
    "    # the mean methods you developed earlier\n",
    "    scores = score_fn(similarity_fn, samples, log_probs )\n",
    "\n",
    "    # find the key with the highest score\n",
    "    max_index = max(scores, key=scores.get)\n",
    "    \n",
    "    # detokenize the token list associated with the max_index\n",
    "    translated_sentence = detokenize(samples[max_index], vocab_file, vocab_dir)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return (translated_sentence, max_index, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 1.0\n",
    "\n",
    "# put a custom string here\n",
    "your_sentence = 'She speaks English and German.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbr_decode(your_sentence, 4, weighted_avg_overlap, jaccard_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbr_decode('Congratulations!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbr_decode('You have completed the assignment!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_mbr_decode(mbr_decode, model)\n",
    "# END UNIT TEST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
